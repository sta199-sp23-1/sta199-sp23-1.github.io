[
  {
    "objectID": "ae/ae-15.html",
    "href": "ae/ae-15.html",
    "title": "Model selection",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nhomework 03 due next Wednesday\nfinal project instructions"
  },
  {
    "objectID": "ae/ae-15.html#getting-started",
    "href": "ae/ae-15.html#getting-started",
    "title": "Model selection",
    "section": "Getting started",
    "text": "Getting started\nClone your ae15-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-15.html#today",
    "href": "ae/ae-15.html#today",
    "title": "Model selection",
    "section": "Today",
    "text": "Today\nBy the end of today you will‚Ä¶\n\nselect between linear models with different numbers of predictors"
  },
  {
    "objectID": "ae/ae-15.html#load-packages-and-data",
    "href": "ae/ae-15.html#load-packages-and-data",
    "title": "Model selection",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-15.html#notes",
    "href": "ae/ae-15.html#notes",
    "title": "Model selection",
    "section": "Notes",
    "text": "Notes\n\nThe problem with \\(R^2\\)\n\\(R^2\\) tell us the proportion of variability in the data our model explains. If we add predictors to our model, we will always improve \\(R^2\\) (regardless of whether the predictor is good or not).\nTo see this‚Ä¶\n\noffline example\ntake away: a line can go through any two points, a plane can go through any three points, etc. In general an \\(n\\) dimensional object can go through \\(n\\) points.\n\nFor this reason, \\(R^2\\) is not a good way to select between two models that have a different number of predictors. Instead, we prefer to use Akaike Information Criterion (AIC).\n\n\nAIC\n\\[\n\\text{AIC} = 2k - 2 \\log (\\text{likelihood})\n\\]\nwhere \\(k\\) is the number of estimated parameters (\\(\\beta\\)s) in the model. Notice this will be 1 + the number of predictors. and \\(\\hat{L}\\) is ‚Äúlikelihood‚Äù of the data given the fitted model.\nThe likelihood is a measure of how well a given model fits the data. Specifically, higher likelihoods imply better fits. Since the AIC score has a negative in front of the log likelihood, lower scores are better fits. However, \\(k\\) penalizes adding new predictors to the model.\nTake-away: lower AIC is better fit.\nYou can find AIC using glance(fitted-model). (Assuming you named your fitted model fitted-model)"
  },
  {
    "objectID": "ae/ae-15.html#building-a-model",
    "href": "ae/ae-15.html#building-a-model",
    "title": "Model selection",
    "section": "Building a model",
    "text": "Building a model\nScenario: you have an outcome \\(y\\) you want to predict. You have several variables you‚Äôve measured that you could use as predictors in your linear model. Each predictor is expensive to collect future measurements of. You want your model to only include the most useful predictors.\n\nBackward elimination\nBackward elimination starts with the full model (the model that includes all potential predictor variables). Variables are eliminated one-at-a-time from the model until we cannot improve the model any further.1\nProcedure:\n\nStart with a model that has all predictors under study and compute the AIC.\nNext fit every possible model with 1 less predictor.\nCompare AIC scores to select the best model with 1 less predictor.\nRepeat steps 2 and 3 until you can no longer improve the model.\n\n\n\nForward selection\nForward selection is the reverse of the backward elimination technique. Instead, of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model any further.\nProcedure:\n\nStart with a model that has no predictors.\nNext fit every possible model with 1 additional predictor and score each model.\nCompare AIC scores to select the best model with 1 additional predictor.\nRepeat steps 2 and 3 until you can no longer improve the model."
  },
  {
    "objectID": "ae/ae-15.html#example",
    "href": "ae/ae-15.html#example",
    "title": "Model selection",
    "section": "Example",
    "text": "Example\n\nExercise\n\nWill forward selection and backward elimination always yield the same model? Type your answer below before running any code.\nNext, see if you are right using the data set below.\n\nSolution below\n\ntest_df = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/test_df.csv\")\n\nIn the following two examples, we will use stepwise selection to build a main effects model.\nPerform 1 step of forward selection. What variable will be in the final forward selection model?\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n\n[1] 93.08637\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x2, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n\n[1] 131.8917\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n\n[1] 152.9043\n\n\nNext, perform 1 step of backward elimination. Which variable will not be in the final backward elimination model?\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x2 + x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n\n[1] 35.25949\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1 + x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n\n[1] 94.2536\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1 + x2, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n\n[1] 87.1686"
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW 3 - Do you even lift?",
    "section": "",
    "text": "Important\n\n\n\nThis homework is due Wednesday, March 8 at 5:00pm."
  },
  {
    "objectID": "hw/hw-3.html#getting-started",
    "href": "hw/hw-3.html#getting-started",
    "title": "HW 3 - Do you even lift?",
    "section": "Getting Started",
    "text": "Getting Started\n\nGo to the Github Organization page and open your hw3-username repo\nClone the repository, open a new project in RStudio. It contains the starter documents you need to complete the homework assignment."
  },
  {
    "objectID": "hw/hw-3.html#exercises",
    "href": "hw/hw-3.html#exercises",
    "title": "HW 3 - Do you even lift?",
    "section": "Exercises",
    "text": "Exercises\nFor all of the following exercises, you should include units on axes labels, e.g.¬†‚ÄúBench press (lbs)‚Äù or ‚ÄúBench press (kg)‚Äù. ‚ÄúAge (years)‚Äù etc. This is good practice.\n\nLet‚Äôs begin by taking a look at the squat powerlifting records. To begin, remove any observations that are negative for squat. Next, create a new column called best3_squat_lbs that converts the record from kg to lbs (you may have to google the conversion). Save your data frame as ipf_squat.\n\n\nUsing ipf_squat, create a scatter plot to investigate the relationship between squat (in lbs) and age. Age should be on the x-axis. Add a linear trend-line. Remove the standard error. Be sure to label all axes and give the plot a title. Comment on what you observe.\n\n\nWrite down the full linear model to predict lift squat lbs from age in \\(x\\), \\(y\\), \\(\\beta\\) notation. What is \\(x\\)? What is \\(y\\)? Next, fit the linear model. Use the ipf_squat data frame. Re-write your previous equation replacing \\(\\beta\\) with the numeric estimates. This is called the ‚Äúfitted‚Äù linear model. Interpret each estimate of \\(\\beta\\). Are the interpretations reasonable?\nBuilding on your ipf_squat data frame, create a new column called age2 that takes the age of each lifter and squares it. Save your data frame with an appropriate name. Next, plot squat in lbs vs age2 and add a linear best fit line. Does this model look like it fits the data better? Is this still a linear model?\nOne metric to assess the fit of a model is the correlation squared, also known as \\(R^2\\). Fit the age\\(^2\\) model and save the object as age2Fit. Subsequently report the \\(R^2\\). Compare \\(R^2\\) of the age\\(^2\\) model to the model from exercise 2. Which model do you prefer?\n\n\nIf you were to add body weight as a second predictor to the age\\(^2\\) model, would \\(R^2\\) increase or decrease? Explain.\n\n\nStarting with the original ipf dataframe, filter and mutate the data as we did in exercise 1, but this time filtering for best3bench_kg \\(>0\\) and creating a best3_bench_lbs variable, a bodyweight_lbs variable, and a sex variable that is a factor rather than a character. Fit an interaction effects model with bodywieght (in lbs) and sex as predictors of best bench press (in lbs). Write down the fitted model equation only, replacing \\(\\hat{\\beta}\\) with the fitted estimates. Interpret the \\(\\hat{\\beta}\\).\nVisualize the interaction effects model we built in exercise 5. Hint: there should be two lines with different slopes. Bodyweight should be on the x-axis. Add a linear trend-line. Be sure to label all axes and give the plot a title. Comment on what you observe.\nDo lifters who fail a drug test perform better or worse at bench press than other lifters? Does this vary across sexes? We‚Äôll answer this question in two parts. First, remove all observations from the data frame that have NA listed under bench press. Next, create a new column called doping_status that takes value doping if the lifter failed a drug test and not doping otherwise. Save this data frame as ipf_dope.\n\n\n\n\n\n\n\nHint\n\n\n\nCheck the data dictionary at the top to figure out what variables will help you build the doping_status column.\n\n\n\nUsing ipf_dope from the previous exercise, compute the 5%, 50%, 95% quantiles for bench press across both sex and doping_status. You can use either bench press in kg or lbs here. With this information, answer the question ‚ÄúDo lifters who fail a drug test perform better or worse at bench press than other lifters? Is this consistent across sex and quantiles?‚Äù"
  },
  {
    "objectID": "hw/hw-3.html#reminder",
    "href": "hw/hw-3.html#reminder",
    "title": "HW 3 - Do you even lift?",
    "section": "Reminder:",
    "text": "Reminder:\n\nAll plots should follow the best visualization practices: include an informed title, label axes, and carefully consider aesthetic choices.\nAll code should follow the tidyverse style guidelines, including not exceeding the 80 character limit."
  },
  {
    "objectID": "hw/hw-3.html#submission",
    "href": "hw/hw-3.html#submission",
    "title": "HW 3 - Do you even lift?",
    "section": "Submission",
    "text": "Submission\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials Duke Net ID and log in using your Net ID credentials.\nClick on your STA 199 course.\nClick on the assignment, and you‚Äôll be prompted to submit it.\nMark all the pages associated with exercise. All the pages of your homework should be associated with at least one question (i.e., should be ‚Äúchecked‚Äù). If you do not do this, you will be subject to lose points on the assignment.\nSelect all pages of your PDF submission to be associated with the ‚ÄúWorkflow & formatting‚Äù question."
  },
  {
    "objectID": "hw/hw-3.html#rubric",
    "href": "hw/hw-3.html#rubric",
    "title": "HW 3 - Do you even lift?",
    "section": "Rubric",
    "text": "Rubric\n\nEx 1: 6 pts.\nEx 2: 7 pts.\nEx 3: 6 pts.\nEx 4: 5 pts.\nEx 5: 8 pts.\nEx 6: 4 pts.\nEx 7: 4 pts\nEx 8: 5 pts\nWorkflow and formatting - 5 pts\n\n\n\n\n\n\n\nNote\n\n\n\nThe ‚ÄúWorkflow & formatting‚Äù grade is to assess the reproducible workflow. This includes:\n\nlinking all pages appropriately on Gradescope\nputting your name in the YAML at the top of the document\ncommitting the submitted version of your .qmd to GitHub\nAre you under the 80 character code limit? (You shouldn‚Äôt have to scroll to see all your code).\nPipes %>%, |> and ggplot layers + should be followed by a new line\nYou should be consistent with stylistic choices, e.g.¬†only use 1 of = vs <- and %>% vs |>\nAll binary operators should be surrounded by space. For example x + y is appropriate. x+y is not."
  },
  {
    "objectID": "slides/lab-project-proposal.html#project-proposal-steps",
    "href": "slides/lab-project-proposal.html#project-proposal-steps",
    "title": "Project proposal",
    "section": "Project proposal steps",
    "text": "Project proposal steps\n\n\n\n\n\nFind a data set that satisfies the guidelines.\nWrite about:\n\nthe source of data\nwhen and how it was originally collected (by the curator, not necessarily how you found the data)\na brief description of the observations\n\nChoose 1-2 research questions and write a hypothesis for each.\nput your data in the data folder and push. glimpse the data"
  },
  {
    "objectID": "slides/lab-project-proposal.html#demo-data-in-data-folder",
    "href": "slides/lab-project-proposal.html#demo-data-in-data-folder",
    "title": "Project proposal",
    "section": "Demo: data in data folder",
    "text": "Demo: data in data folder\n\nload data from your computer\n\nupload button in R files pane\n\nread and write a csv to file from a URL\n\ndinosaur_dataset = read_csv(\"website-url-here\")\nwrite_csv(dataset, \"data/dinosaur_dataset.csv\")\nNote read_csv and write_csv are from the readr package and loaded automatically with library(tidyverse)\n\nread in an excel spreadsheet\n\nreadxl::read_xlsx()\nreadxl::read_xls()"
  },
  {
    "objectID": "slides/lab-project-proposal.html#ex-introduction-and-data",
    "href": "slides/lab-project-proposal.html#ex-introduction-and-data",
    "title": "Project proposal",
    "section": "Ex: Introduction and Data",
    "text": "Ex: Introduction and Data\n\nData set #1: NC Courage Homefield Advantage Our first data set comes from the National Women‚Äôs Soccer League (NSWL) Github and was sourced from nwslsoccer.com.\nThe dataset contains 78 observations (soccer games) played by the NC courage spanning three seasons: 2017, 2018, 2019. There are 10 variables in this dataset. Some of the variables we care about are home_team, away_team, and result (of the game)."
  },
  {
    "objectID": "slides/lab-project-proposal.html#ex-research-questions",
    "href": "slides/lab-project-proposal.html#ex-research-questions",
    "title": "Project proposal",
    "section": "Ex: Research question(s):",
    "text": "Ex: Research question(s):\n\nDoes NC Courage have a home-field advantage? We hypothesize that NC Courage is more likely to win on their home field than another team‚Äôs field.\n\nTo answer this question we will use information about the home_team, and the result of the game.\n\nDoes winning propagate winning? When NC Courage win a game, does it increase the probability of winning the very next game?\n\nTo answer this question we will use information about the result of the game and the game_number."
  },
  {
    "objectID": "slides/lab-project-proposal.html#ex-glimpse",
    "href": "slides/lab-project-proposal.html#ex-glimpse",
    "title": "Project proposal",
    "section": "Ex: Glimpse",
    "text": "Ex: Glimpse\n\nglimpse(courage)\n\nRows: 78\nColumns: 10\n$ game_id     <chr> \"washington-spirit-vs-north-carolina-courage-2017-04-15\", ‚Ä¶\n$ game_date   <chr> \"4/15/2017\", \"4/22/2017\", \"4/29/2017\", \"5/7/2017\", \"5/14/2‚Ä¶\n$ game_number <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,‚Ä¶\n$ home_team   <chr> \"WAS\", \"NC\", \"NC\", \"BOS\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NC\", ‚Ä¶\n$ away_team   <chr> \"NC\", \"POR\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NJ\", \"NC\", \"KC\", \"‚Ä¶\n$ opponent    <chr> \"WAS\", \"POR\", \"ORL\", \"BOS\", \"ORL\", \"CHI\", \"NJ\", \"CHI\", \"KC‚Ä¶\n$ home_pts    <dbl> 0, 1, 3, 0, 3, 1, 2, 3, 2, 3, 0, 0, 2, 1, 1, 0, 1, 2, 2, 2‚Ä¶\n$ away_pts    <dbl> 1, 0, 1, 1, 1, 3, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 2, 0, 3, 1‚Ä¶\n$ result      <chr> \"win\", \"win\", \"win\", \"win\", \"loss\", \"loss\", \"win\", \"loss\",‚Ä¶\n$ season      <dbl> 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017‚Ä¶"
  },
  {
    "objectID": "slides/lab-project-proposal.html#remember",
    "href": "slides/lab-project-proposal.html#remember",
    "title": "Project proposal",
    "section": "Remember",
    "text": "Remember\n\n\nlist data sets in your proposal in your order of preference\nall work for the proposal should go into proposal.qmd within your project repo.\nrender to pdf and submit to Gradescope, selecting all for the exercise ‚Äúproposal‚Äù\nlink your teammates in Gradescope when you submit\n\n\n\n\nüîó sta199-sp23-1.github.io"
  },
  {
    "objectID": "ae/ae-16.html",
    "href": "ae/ae-16.html",
    "title": "Logistic regression",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nhomework 03 due today\nfinal project proposal due Friday\nstatistics experience homework\n\nFriday is last day to register for datafest."
  },
  {
    "objectID": "ae/ae-16.html#getting-started",
    "href": "ae/ae-16.html#getting-started",
    "title": "Logistic regression",
    "section": "Getting started",
    "text": "Getting started\nClone your ae16-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-16.html#load-packages-and-data",
    "href": "ae/ae-16.html#load-packages-and-data",
    "title": "Logistic regression",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(boot) # contains inv.logit() function\n\nTo illustrate logistic regression, we will build a spam filter from email data. Today‚Äôs data consists of 4601 emails that are classified as spam or non-spam. The data was collected at Hewlett-Packard labs and contains 58 variables. The first 48 variables are specific keywords and each observation is the percentage of appearance (frequency) of that word in the message. Click here to read more.\n\ntype \\(= 1\\) is spam\ntype \\(= 0\\) is non-spam\n\n\nspam = read_csv(\"https://sta101.github.io/static/appex/data/spam.csv\")\nglimpse(spam)\n\nRows: 4,601\nColumns: 58\n$ make              <dbl> 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15‚Ä¶\n$ address           <dbl> 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ all               <dbl> 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.46‚Ä¶\n$ num3d             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ our               <dbl> 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1.88, 0.61‚Ä¶\n$ over              <dbl> 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ remove            <dbl> 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0.00, 0.30‚Ä¶\n$ internet          <dbl> 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1.88, 0.00‚Ä¶\n$ order             <dbl> 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0.00, 0.92‚Ä¶\n$ mail              <dbl> 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0.00, 0.76‚Ä¶\n$ receive           <dbl> 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0.00, 0.76‚Ä¶\n$ will              <dbl> 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0.00, 0.92‚Ä¶\n$ people            <dbl> 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ report            <dbl> 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ addresses         <dbl> 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ free              <dbl> 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0.00, 0.00‚Ä¶\n$ business          <dbl> 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ email             <dbl> 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0.00, 0.15‚Ä¶\n$ you               <dbl> 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0.00, 1.23‚Ä¶\n$ credit            <dbl> 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0.00, 3.53‚Ä¶\n$ your              <dbl> 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0.00, 2.00‚Ä¶\n$ font              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ num000            <dbl> 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ money             <dbl> 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15‚Ä¶\n$ hp                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ hpl               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ george            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ num650            <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ lab               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ labs              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ telnet            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ num857            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ data              <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15‚Ä¶\n$ num415            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ num85             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ technology        <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ num1999           <dbl> 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ parts             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ pm                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ direct            <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ cs                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ meeting           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ original          <dbl> 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30‚Ä¶\n$ project           <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ re                <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ edu               <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ table             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ conference        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ charSemicolon     <dbl> 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0‚Ä¶\n$ charRoundbracket  <dbl> 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0.054, 0.2‚Ä¶\n$ charSquarebracket <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0‚Ä¶\n$ charExclamation   <dbl> 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0.164, 0.0‚Ä¶\n$ charDollar        <dbl> 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0.054, 0.0‚Ä¶\n$ charHash          <dbl> 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0‚Ä¶\n$ capitalAve        <dbl> 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1.671, 2.4‚Ä¶\n$ capitalLong       <dbl> 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6, 11, 61,‚Ä¶\n$ capitalTotal      <dbl> 278, 1028, 2259, 191, 191, 54, 112, 49, 1257, 749, 2‚Ä¶\n$ type              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n\n\nThe basic logic of our model is that the frequency of certain words can help us determine whether or not an email is spam.\nFor example, these emails came from George‚Äôs inbox. If the word ‚Äúgeorge‚Äù is not present in the message and the dollar symbol (charDollar) is, you might expect the email to be spam.\nUsing this data, we want to build a model that predicts whether a new email is spam or not. How do we build a model that can do this?\n\nExercise 1\nStart by examining 1 predictor.\n\nVisualize a linear model where the outcome is type (spam or not) and george is the predictor.\nDiscuss your visualization with your neighbor. Is this a good model? Why or why not?\n\n\n# code here"
  },
  {
    "objectID": "ae/ae-16.html#example",
    "href": "ae/ae-16.html#example",
    "title": "Logistic regression",
    "section": "Example",
    "text": "Example\nLet‚Äôs build a model centered around just two predictor variables.\nThe first will be the word you and the second will be capitalTotal (the total number of capital letters in the message).\n\nExercise 2\nCreate a visualization with you on the x-axis and capitalTotal on the y-axis. Color data points by whether or not they are spam.\n\n# code here\n\nLet‚Äôs fit the model!\n\nfit_1 = logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(as.factor(type) ~ you + capitalTotal, data = spam, family = \"binomial\")\n  \nfit_1 %>%\n  tidy()\n\n# A tibble: 3 √ó 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.50     0.0554       -27.1 2.97e-162\n2 you           0.361    0.0198        18.3 1.84e- 74\n3 capitalTotal  0.00173  0.000104      16.6 5.66e- 62\n\n\n\n\nExercise 3\n\nWhat is different in the code above from previous linear models we fit?\n\n\n\nExercise 4\n\nWhat is the probability the email is spam if the frequency of you is 5% in the email and there are 2500 capital letters. Use the model equation above.\nWhat is the log-odds? (Recall from the prep that log-odds \\(= \\log \\frac{p}{1-p}\\)). Use the code below to check your work.\n\n\nnewdata = data.frame(you = 5, capitalTotal = 2500)\n\n# code here\n\n# check work\ncheckLogOdds = predict(fit_1$fit, newdata)\ncheckLogOdds\n\n       1 \n4.633134 \n\ncheckP = inv.logit(checkLogOdds)\ncheckP\n\n        1 \n0.9903694"
  },
  {
    "objectID": "ae/ae-16.html#visualize-logistic-regression",
    "href": "ae/ae-16.html#visualize-logistic-regression",
    "title": "Logistic regression",
    "section": "Visualize logistic regression",
    "text": "Visualize logistic regression\n\nbeta = fit_1$fit$coefficients\nhyperplane = function(x){\n    decisionBoundary = 0.5\n    c = logit(decisionBoundary)\n    const = c - beta[1]\n    return((-beta[2]*x + const) / beta[3])\n}\n\nspam %>%\n  ggplot(aes(x = you, y = capitalTotal, color = as.factor(type))) + \n  geom_point(alpha = 0.3) +\n  geom_function(fun = hyperplane) +\n  scale_colour_manual(values = c(\"orange\", \"steelblue\")) +\n  theme_minimal()\n\nWarning: Multiple drawing groups in `geom_function()`. Did you use the correct\n`group`, `colour`, or `fill` aesthetics?\n\n\n\n\n\n\nJust because there‚Äôs greater than 50% probability an email is spam doesn‚Äôt mean we have to label it as such. We can adjust our threshold or critical probability, a.k.a. decision boundary to be more or less sensitive to spam emails.\n\nIn other words we get to select a number \\(p^*\\) such that\nif \\(p > p^*\\), then label the email as spam.\n\nExercise 5\n\nWhat would you set your decision boundary to and why?\nChange decisionBoundary in the code above to 0.01 and 0.999999. Do the results surprise you? Why or why not?\nlower boundary means that we label more emails as spam, high boundary means fewer emails as spam. We can adjust the boundary depending on how much we value receiving important emails vs how much we dislike spam.\n0 means all emails are spam, 1 means no emails are spam. Note you cannot set decision boundary to 0 or 1 because of logit function (would evaluate to inf or negative inf)"
  },
  {
    "objectID": "ae/ae-16.html#classify-a-new-email",
    "href": "ae/ae-16.html#classify-a-new-email",
    "title": "Logistic regression",
    "section": "Classify a new email",
    "text": "Classify a new email\n\nemail = readLines(\"https://sta101.github.io/static/appex/data/test-email.txt\")\nemail\n\n[1] \"You Have Been Selected To Win A Free Trip To Disney World! \"\n[2] \"\"                                                           \n[3] \"YOU HAVE 30 SECONDS TO CLICK HERE TO CLAIM YOUR REWARD!\"    \n[4] \"\"                                                           \n[5] \"WHAT ARE YOU WAITING FOR? ACT NOW!\"                         \n[6] \"\"                                                           \n[7] \"SINCERELY,\"                                                 \n[8] \"\"                                                           \n[9] \"WALT DISNEY\"                                                \n\ntotalWord = sum(str_count(email, \" \"))\ntotalYou = sum(str_count(tolower(email), \"you\"))\ncapitalTotal = sum(str_count(email, \"[A-Z]\"))\n\nyouFreq = 100 * totalYou / totalWord\nnewemail = data.frame(you = youFreq, capitalTotal = capitalTotal)\n\nlogOdds = predict(fit_1$fit, newemail)\nlogOdds\n\n       1 \n3.648776 \n\ninv.logit(logOdds)\n\n        1 \n0.9746371 \n\n\n\nExercise 6\n\nDoes the code above count the correct number of ‚Äúyou‚Äù? Why or why not?\nDo you believe the predicted odds of the email being spam? Why or why not?\nWhat is the probability the test email is spam?"
  },
  {
    "objectID": "ae/ae-16.html#assessing-predictive-ability",
    "href": "ae/ae-16.html#assessing-predictive-ability",
    "title": "Logistic regression",
    "section": "Assessing predictive ability",
    "text": "Assessing predictive ability\nWe will divide the data into a training set and testing set.\n\nset.seed(6)\nsampleIndices = sample.int(n = nrow(spam), size = 2000, replace = F)\ntrain = spam[sampleIndices, ]\ntest  = spam[-sampleIndices, ] %>%\n  slice_sample(n = 2000)\n\n\nExercise 7\nNext, let‚Äôs train your model on the training set. Build a predictive model using any combination of predictors from spam. Save your fitted model as myModel\n\n# code here\n\n#example (delete this):\nmyModel = logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(as.factor(type) ~ you + address, data = train, family = \"binomial\")\n\nand test it on the testing set,\n\nprediction = test %>%\n  mutate(myModelPrediction = predict(myModel, test)$.pred_class) \n\nprediction\n\n# A tibble: 2,000 √ó 59\n    make address   all num3d   our  over remove internet order  mail receive\n   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl> <dbl> <dbl>   <dbl>\n 1  0       0     0.7      0  0     0.14   0        0     0.28  0       0   \n 2  0       0     0        0  0     0      0        0     0     0       0   \n 3  0       0     2.5      0  0     0      0        0     0     0       0   \n 4  0       2.08  0        0  3.12  0      1.04     0     0     0       0   \n 5  0       0     1.04     0  1.04  0      0        1.39  0.34  0       0   \n 6  0       0.35  0.7      0  0.35  0      0        0     0     0       0   \n 7  0.39    0     0        0  1.17  0      0        0     0     0.39    0   \n 8  0       0.25  0.75     0  1     0.25   0        0     0     0       0.25\n 9  0       0     0        0  0.87  0      0        0     0     0       1.31\n10  0       0     0        0  1.11  0      0        0.55  0     3.91    0   \n# ‚Ä¶ with 1,990 more rows, and 48 more variables: will <dbl>, people <dbl>,\n#   report <dbl>, addresses <dbl>, free <dbl>, business <dbl>, email <dbl>,\n#   you <dbl>, credit <dbl>, your <dbl>, font <dbl>, num000 <dbl>, money <dbl>,\n#   hp <dbl>, hpl <dbl>, george <dbl>, num650 <dbl>, lab <dbl>, labs <dbl>,\n#   telnet <dbl>, num857 <dbl>, data <dbl>, num415 <dbl>, num85 <dbl>,\n#   technology <dbl>, num1999 <dbl>, parts <dbl>, pm <dbl>, direct <dbl>,\n#   cs <dbl>, meeting <dbl>, original <dbl>, project <dbl>, re <dbl>, ‚Ä¶\n\n\n\n\nExercise 8\nWhat is the proportion of false positives (i.e.¬†classified as spam but was not)? False negatives?\n\n# code here"
  },
  {
    "objectID": "teaching-resources/GitHubClassroomManagement.html",
    "href": "teaching-resources/GitHubClassroomManagement.html",
    "title": "GitHub Classroom Management",
    "section": "",
    "text": "# $ git clone ghclass\n# $ git checkout artifacts\n\n#devtools::install(\"/path/to/ghclass\")\nEach code chunk below is designed to be self-contained (no dependency between chunks)."
  },
  {
    "objectID": "teaching-resources/GitHubClassroomManagement.html#to-make-new-repos",
    "href": "teaching-resources/GitHubClassroomManagement.html#to-make-new-repos",
    "title": "GitHub Classroom Management",
    "section": "To make new repos",
    "text": "To make new repos\n\n\n\n\nCreate a new repo in the organization called ‚Äúlab1‚Äù with a README.md\nPush to ‚Äúlab1‚Äù: lab1.qmd, data/any_relevant_data.csv and optionally lab1.Rproj.\nSelect the repository, click ‚ÄúSettings‚Äù and check ‚ÄúTemplate repository‚Äù.\nFollow the code below.\n\n\n# initial assignment creation\norg = \"sta199-sp23-1\"\nusernames = ghclass::org_members(org)\n\n## edit this:\nassignment_template_repo = \"lab0_template\"\n\nghclass::org_create_assignment(\n  org = org,\n  repo = paste0(assignment_template_repo, \"-\", usernames),\n  user = usernames,\n  source_repo = paste0(org, \"/\", assignment_template_repo)\n)\n\n\n\n\nFeel free to test lab creation on my dummy account ‚Äúfishswish‚Äù or on your own account with the code below\n\norg = \"sta199-sp23-1\"\nusernames = \"fishswish\"\nassignment_template_repo = \"lab0_template\"\nghclass::org_create_assignment(\n  org = org,\n  repo = paste0(assignment_template_repo, \"-\", usernames),\n  user = usernames,\n  source_repo = paste0(org, \"/\", assignment_template_repo)\n)\n\nIt‚Äôs possible some people are not in the organization and will need to be have additional repos created manually following the steps below:\n\ngo to template repo and click the green ‚ÄúUse this template‚Äù -> ‚ÄúCreate a new repository‚Äù -> name it ‚Äúlab-x-their_github_username‚Äù -> leave ‚ÄúPrivate‚Äù selected and click ‚ÄúCreate repository from template‚Äù\nnext in the new repo go to ‚ÄúSettings‚Äù -> ‚ÄúCollaborators and Teams‚Äù -> click the green ‚ÄúAdd people‚Äù button and type in their GitHub username.\nnext make sure to go to the ‚ÄúPeople‚Äù section of the organization and select ‚ÄúInvite member‚Äù so they join the organization too. Owning a repo in the org just makes them an outside collaborator. They still have to actually join the org"
  },
  {
    "objectID": "teaching-resources/GitHubClassroomManagement.html#oops.-file-management-after-you-create-repos",
    "href": "teaching-resources/GitHubClassroomManagement.html#oops.-file-management-after-you-create-repos",
    "title": "GitHub Classroom Management",
    "section": "Oops. File management after you create repos",
    "text": "Oops. File management after you create repos\nThe magic happens in repo_add_file(). Be sure to edit/check each item below, specifically:\n\norg as appropriate\nrepos_to_modify object\narguments to repo_add_file()\n\n\norg = \"sta199-sp23-1\"\nusernames = ghclass::org_members(org)\n\n# get all existing labX repos\nrepos_to_modify = org_repos(org, filter = \"practice1-\") # edit this \"filter\"\n\nghclass::repo_add_file(repo = repos_to_modify,\n                       branch = \"main\",\n                       repo_folder = \"\", # edit this to be path to folder in repo\n                       message = \"update README\", # update to be a meaningful commit message\n                       file = \"~/Desktop/README.md\", # update local path to the file you want to add\n                       overwrite = TRUE) # if you want to overwrite or not"
  },
  {
    "objectID": "teaching-resources/GitHubClassroomManagement.html#creating-team-assignments",
    "href": "teaching-resources/GitHubClassroomManagement.html#creating-team-assignments",
    "title": "GitHub Classroom Management",
    "section": "Creating team assignments",
    "text": "Creating team assignments\n\n# example data frame for demo purposes\n# you will need to format your data frame to look like this\nroster = data.frame(\n  github = c(\"fishswish\", \"athos00\"),\n  lab1 = rep(\"lab1-team1\", 2)\n)\n\n# edit each item below\norg_create_assignment(\n  org = \"sta199-sp23-1\",\n  user = roster$github,\n  repo = roster$lab1,\n  team = roster$lab1,\n  source_repo = \"sta199-sp23-1/lab1_template\",\n  private = TRUE\n)\n\n\nroster = data.frame(\n  github = c(\"devinjohnson7\", \"athos00\"),\n  lab1 = rep(\"lab1-team-awesome\", 2)\n)\n\n# edit each item below\norg_create_assignment(\n  org = \"sta323-sp23\",\n  user = roster$github,\n  repo = roster$lab1,\n  team = roster$lab1,\n  source_repo = \"sta323-sp23/lab-1\", # template file\n  private = TRUE\n)\n\n\norg = \"sta323-sp23\"\nrepos = ghclass::org_repos(org, filter = \"lab-2\")\n\n\n# repos = \"sta323-sp23/lab-1\"\nghclass::action_artifact_delete(repos, ids=action_artifacts(repos, which=\"all\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 199: Introduction to Data Science",
    "section": "",
    "text": "week\n      dow\n      date\n      what\n      topic\n      prepare\n      slides\n      ae\n      hw\n      lab\n      exam\n      project\n      notes\n    \n  \n  \n    0\nW\nJan 11\nLab 0\nHello R!\n\n\n\n\n\n\n\n\n    \nF\nJan 13\nLec 1\nWelcome to STA 199\n\n\n\n\n\n\n\n\n    1\nM\nJan 16\n\nNo class -- Martin Luther King Jr. Day\n\n\n\n\n\n\n\n\n    \nW\nJan 18\nLec 2\nIntro to statistics and plotting\n\n\n\n\n\n\n\n\n    \nF\nJan 20\nLec 3\nVisualizing various types of data\n\n\n\n\n\n\n\n\n    2\nM\nJan 23\nLab 1\nData visualization\n\n\n\n\n\n\n\n\n    \nW\nJan 25\nLec 4\nGrammar of data wrangling\n\n\n\n\n\n\n\nHomework 1 released\n    \nF\nJan 27\nLec 5\nWorking with multiple data frames\n\n\n\n\n\n\n\n\n    3\nM\nJan 30\nLab 2\nData wrangling\n\n\n\n\n\n\n\n\n    \nW\nFeb 1\nLec 6\nTidy data\n\n\n\n\n\n\n\n\n    \nF\nFeb 3\nLec 7\nData types and classes\n\n\n\n\n\n\n\nHomework 1 due\n    4\nM\nFeb 6\nLab 3\nData tidying\n\n\n\n\n\n\n\n\n    \nW\nFeb 8\nLec 8\nSpatial data\n\n\n\n\n\n\n\n\n    \nF\nFeb 10\nLec 9\nTroubleshooting / Review\n\n\n\n\n\n\n\nExam 1 released! Due Feb 14 5:00pm\n    5\nM\nFeb 13\nLab\nNo lab: exam 1\n\n\n\n\n\n\n\n\n    \nW\nFeb 15\nLec 10\nIntro to probability\n\n\n\n\n\n\n\n\n    \nF\nFeb 17\nLec 11\nConditional probability\n\n\n\n\n\n\n\nHomework 2 released\n    6\nM\nFeb 20\nLab 4\nMerge conflicts\n\n\n\n\n\n\n\n\n    \nW\nFeb 22\nLec 12\nSimple regression\n\n\n\n\n\n\n\n\n    \nF\nFeb 24\nLec 13\nMultiple regression I\n\n\n\n\n\n\n\nHomework 2 due\n    7\nM\nFeb 27\nLab 5\nPredicting a numerical outcome\n\n\n\n\n\n\n\n\n    \nW\nMar 1\nLec 14\nMultiple regression II\n\n\n\n\n\n\n\nHomework 3 released; project announced\n    \nF\nMar 3\nLec 15\nModel selection\n\n\n\n\n\n\n\n\n    8\nM\nMar 6\nLab\nWork on project proposal\n\n\n\n\n\n\n\n\n    \nW\nMar 8\nLec 16\nLogistic Regression\n\n\n\n\n\n\n\nHomework 3 due; statistics experience released\n    \nF\nMar 10\nLec 17\nPrediction\n\n\n\n\n\n\n\nProject proposal due\n    9\nM\nMar 13\n\nNo class -- Spring Break\n\n\n\n\n\n\n\n\n    \nW\nMar 15\n\nNo class -- Spring Break\n\n\n\n\n\n\n\n\n    \nF\nMar 17\n\nNo class -- Spring Break\n\n\n\n\n\n\n\n\n    10\nM\nMar 20\nLab\n\n\n\n\n\n\n\n\n\n    \nW\nMar 22\nLec 18\nBootstrap\n\n\n\n\n\n\n\n\n    \nF\nMar 24\nLec 19\nCentral limit theorem I\n\n\n\n\n\n\n\n\n    11\nM\nMar 27\nLab\n\n\n\n\n\n\n\n\n\n    \nW\nMar 29\nLec 20\nCentral limit theorem II\n\n\n\n\n\n\n\n\n    \nF\nMar 31\nLec 21\nHypothesis testing I\n\n\n\n\n\n\n\n\n    12\nM\nApr 3\nLab\nProject work day\n\n\n\n\n\n\n\n\n    \nW\nApr 5\nLec 22\nHypothesis testing II\n\n\n\n\n\n\n\n\n    \nF\nApr 7\nLec 23\nHypothesis testing III\n\n\n\n\n\n\n\n\n    13\nM\nApr 10\nLab\nProject peer review\n\n\n\n\n\n\n\n\n    \nW\nApr 12\nLec 24\nProject tips\n\n\n\n\n\n\n\n\n    \nF\nApr 14\nLec 25\nEthics\n\n\n\n\n\n\n\nExam 2 released! Due Tue April 18 5:00pm\n    14\nM\nApr 17\nLab\nNo lab: exam 2\n\n\n\n\n\n\n\n\n    \nW\nApr 19\nLec 26\nSpecial topic\n\n\n\n\n\n\n\n\n    \nF\nApr 21\nLec 27\nSpecial topic\n\n\n\n\n\n\n\n\n    15\nM\nApr 24\nLab\nProject presentations\n\n\n\n\n\n\n\n\n    \nW\nApr 26\nLec 28\nSpecial topic"
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Proposal due Friday, March 10th\nDraft due Friday April 7th\nPeer review in lab Monday April 10th\nPresentation + slides + report and final GitHub repo due Monday April 24th\nIn addition to the above, a component of the grade will be comprised of evaluating group members via survey by exam period.\n\n\nFind a data set, develop a question you can answer with the data, and do it."
  },
  {
    "objectID": "project-description.html#where-to-find-data",
    "href": "project-description.html#where-to-find-data",
    "title": "Project description",
    "section": "Where to find data?",
    "text": "Where to find data?\n\nAwesome public datasets\nBikeshare data portal\nCDC\nData.gov\nData is Plural\nDurham Open Data Portal\nEdinburgh Open Data\nElection Studies\nEuropean Statistics\nCORGIS: The Collection of Really Great, Interesting, Situated Datasets\nGeneral Social Survey\nGoogle Dataset Search\nHarvard Dataverse\nInternational Monetary Fund\nIPUMS survey data from around the world\nLos Angeles Open Data\nNHS Scotland Open Data\nNYC OpenData\nOpen access to Scotland‚Äôs official statistics\nPew Research\nPRISM Data Archive Project\nStatistics Canada\nThe National Bureau of Economic Research\nTidyTuesday\nUCI Machine Learning Repository\nUK Government Data\nUNICEF Data\nUnited Nations Data\nUnited Nations Statistics Division\nUS Census Data\nUS Government Data\nWorld Bank Data\nYouth Risk Behavior Surveillance System (YRBSS)"
  },
  {
    "objectID": "project-description.html#presentation-30pts",
    "href": "project-description.html#presentation-30pts",
    "title": "Project description",
    "section": "Presentation 30pts",
    "text": "Presentation 30pts\nPresentations will take place in class during the last lab of the semester.\nThe presentation must be no longer than 5 minutes. This will be strictly enforced."
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo.\nThe repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#teamwork",
    "href": "project-description.html#teamwork",
    "title": "Project description",
    "section": "Teamwork",
    "text": "Teamwork\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member.\nFilling out the survey is a prerequisite for getting credit on the teamwork portion of the grade.\nThe teamwork survey together with GitHub commits will be used to measure individual contribution to the assignment. All group members are expected to participate equally. In the event of team concerns and low effort commits, individual grades may differ from the rest of the group.\nIf you have concerns with the teamwork and/or contribution from any team members, please reach out to me or the head TA as early as possible."
  },
  {
    "objectID": "hw/statistics-experience.html",
    "href": "hw/statistics-experience.html",
    "title": "Statistics Experience Homework",
    "section": "",
    "text": "Important\n\n\n\n\nThis homework is due Wednesday, April 26 at 5:00pm.\nNo late work for this assignment will be accepted.\nThere is no GitHub repo for this assignment. Read all instructions.\nThe world of statistics and data science is vast and continually growing! The goal of the statistics experience assignment is to help you engage with statistics and data science communities outside of the classroom.\nThere will be one statistics experience assignment that will count as a homework grade. The submission for the statistics experience is due on Wednesday, April 26 at 5:00pm on Gradescope. No late work will be accepted for the statistics experience. You may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1Ô∏è‚É£ Have a statistics experience\n2Ô∏è‚É£ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/statistics-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/statistics-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "Statistics Experience Homework",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do each one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. You can email Professor Fisher if there is an activity you‚Äôd like to do but you‚Äôre not sure if it qualifies for the statistics experience.\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::global 2021 talks\nrstudio::conf 2020 talks\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. Information for an upcoming data challenge is linked below.\n\nDataFest 2023 at Duke (signups close Friday March 10).\n\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn‚Äôt on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nThe Signal and the Noise: Why so many predictions fail - but some don‚Äôt by Nate Silver\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nList of books about data science ethics\n\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons. You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n\nCreate a GitHub repo for your TidyTuesday submission.\nYour repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nIn the README file:\n\nInclude an image of your final visualization\nWrite a short summary (~ 1 paragraph) about your visualization\n\n\nThe visualization should include features or customization that are beyond what we‚Äôve done in class .\nInclude the link to your GitHub repo in the slide summarizing your experience."
  },
  {
    "objectID": "hw/statistics-experience.html#part-2-summarize-your-experience",
    "href": "hw/statistics-experience.html#part-2-summarize-your-experience",
    "title": "Statistics Experience Homework",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we‚Äôve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e.¬†use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/statistics-experience.html#submission",
    "href": "hw/statistics-experience.html#submission",
    "title": "Statistics Experience Homework",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the Statistics Experience assignment on Gradescope by Wednesday, April 26 at 5:00pm. It must be submitted by the deadline on Gradescope to be considered for grading."
  },
  {
    "objectID": "ae/ae17.html",
    "href": "ae/ae17.html",
    "title": "Prediction",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nfinal project proposal due today at 5:00pm\nstatistics experience homework released\n\ntoday is last day to register for datafest."
  },
  {
    "objectID": "ae/ae17.html#getting-started",
    "href": "ae/ae17.html#getting-started",
    "title": "Prediction",
    "section": "Getting started",
    "text": "Getting started\nClone your ae17-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae17.html#today",
    "href": "ae/ae17.html#today",
    "title": "Prediction",
    "section": "Today",
    "text": "Today\nBy the end of today you will‚Ä¶\n\nbe able to make new predictions from your fitted linear models\nvisualize the fit of your model"
  },
  {
    "objectID": "ae/ae17.html#load-packages-and-data",
    "href": "ae/ae17.html#load-packages-and-data",
    "title": "Prediction",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae17.html#notes",
    "href": "ae/ae17.html#notes",
    "title": "Prediction",
    "section": "Notes",
    "text": "Notes\n\nPrediction\npredict() is a powerful function that takes two arguments:\n\nyour model fit\nnew data you want to make predictions from\n\nThere are several ways you can use the predict() function.\nFor standard linear regression,\npredict(model_fit, test_data) # returns predicted outcome\nFor logistic regression you can use the code above to obtain the predicted outcome (0 or 1) or alternatively use one of the formulations below to quickly grab the log-odds or the probability of the outcome ‚Äú1‚Äù.\npredict(model_fit$fit, test_data) # returns log-odds\npredict(model_fit, test_data, type = \"prob\") # returns probability of a 1."
  },
  {
    "objectID": "ae/ae17.html#practice",
    "href": "ae/ae17.html#practice",
    "title": "Prediction",
    "section": "Practice",
    "text": "Practice\nLoad data:\n\nparkinsons_train = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/parkinsons_train.csv\")\nparkinsons_test = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/parkinsons_test.csv\")\n\nThis dataset comes from Little et al.¬†(2008). The data includes various measurements of dysphonia (disorders of the voice) from 32 people, 24 with Parkinson‚Äôs disease (PD). Multiple measurements were taken per individual. The measurements we examine in this subset of the data include:\n\nname: patient ID\njitter: a measure of relative variation in fundamental frequency\nshimmer: a measure of variation in amplitude (dB)\nPPE: pitch period entropy\nHNR: a ratio of total components vs.¬†noise in the voice recording\nstatus: health status (1 for PD, 0 for healthy)\n\n\nExercise 1\nWrite down a main effects model to predict Parkinson‚Äôs status from HNR, shimmer, jitter and PPE.\n\n\nExercise 2\nFit your model from the previous exercise using the parkinsons_train data set.\n\n# code here\n\n\n\nExercise 3\nUse your model to predict PD status in the parkinsons_test data set with a decision boundary of p = 0.5. How many false positives do you observe? How many false negatives?\nNext change the decision boundary to 0.25. How many false positives and false negatives do you observe?\nWhich decision boundary do you prefer?\n\n# code here\n\n\n\nVisualizing model fits\nThere are many ways you can visualize a fitted model. Plotting the hyperplane is limited to simple two-variable (predictor, outcome) and three-variable (predictor, predictor, outcome) plots. Here we explore some useful visualizations for high-dimensional multivariate models.\n\nExample: logistic regression\n\nCreate a stacked bar plot with status on the x-axis and fill by whether or not the predicted status is correct or incorrect.\n\n\n# code here\n\n\n\nExample: ordinary least squares regression:\nScenario: we are trying to predict vocal amplitude variation (shimmer) from jitter and pitch period entropy (PPE).\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\nwhere\n\\(y\\): shimmer \\(x_1\\): jitter \\(x_2\\): PPE\n\nmyPredictiveModel = linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(shimmer ~ jitter + PPE, data = parkinsons_train)\n\nEven if we don‚Äôt have a test data set, we could still create a new column of predictions like before:\n\n# predict based on new data\npredict_train = parkinsons_train %>%\n  mutate(myPrediction = predict(myPredictiveModel, parkinsons_train)$.pred)\n\nFrom here we can plot \\(\\hat{y}\\) vs \\(y\\):\n\npredict_train %>%\n  ggplot(aes(x = shimmer, y = myPrediction)) +\n  geom_point() +\n  labs(x = \"True Shimmer (dB)\", y = \"Predicted shimmer (dB)\", title = \"Predicted vs true shimmer values\") +\n  geom_abline(slope = 1, intercept = 0, color = \"steelblue\")\n\n\n\n\nAlternatively, we could create a residual plot. Residual plots can be used to assess whether a linear model is appropriate.\nA common assumption of linear regression models is that the error term, \\(\\epsilon\\), has constant variance everywhere.\n\nIf the linear model is appropriate, a residual plot should show this.\nPatterned or nonconstant residual spread may sometimes be indicative a model is missing predictors or missing interactions.\n\n\n\nExercise 4\nCreate a new column residuals in predict_train and save your data frame as predict_train_2\n\n# code here\n\n\npredict_train_2 %>%\n  ggplot(aes(x = myPrediction, y = residuals)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(x = \"Predicted shimmer (dB)\", y = \"Residual\")"
  },
  {
    "objectID": "ae/ae-17.html",
    "href": "ae/ae-17.html",
    "title": "Prediction",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nfinal project proposal due today at 5:00pm\nstatistics experience homework released\n\ntoday is last day to register for datafest."
  },
  {
    "objectID": "ae/ae-17.html#getting-started",
    "href": "ae/ae-17.html#getting-started",
    "title": "Prediction",
    "section": "Getting started",
    "text": "Getting started\nClone your ae17-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-17.html#today",
    "href": "ae/ae-17.html#today",
    "title": "Prediction",
    "section": "Today",
    "text": "Today\nBy the end of today you will‚Ä¶\n\nbe able to make new predictions from your fitted linear models\nvisualize the fit of your model"
  },
  {
    "objectID": "ae/ae-17.html#load-packages-and-data",
    "href": "ae/ae-17.html#load-packages-and-data",
    "title": "Prediction",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-17.html#notes",
    "href": "ae/ae-17.html#notes",
    "title": "Prediction",
    "section": "Notes",
    "text": "Notes\n\nPrediction\npredict() is a powerful function that takes two arguments:\n\nyour model fit\nnew data you want to make predictions from\n\nThere are several ways you can use the predict() function.\nFor standard linear regression,\npredict(model_fit, test_data) # returns predicted outcome\nFor logistic regression you can use the code above to obtain the predicted outcome (0 or 1) or alternatively use one of the formulations below to quickly grab the log-odds or the probability of the outcome ‚Äú1‚Äù.\npredict(model_fit$fit, test_data) # returns log-odds\npredict(model_fit, test_data, type = \"prob\") # returns probability of a 1."
  },
  {
    "objectID": "ae/ae-17.html#practice",
    "href": "ae/ae-17.html#practice",
    "title": "Prediction",
    "section": "Practice",
    "text": "Practice\nLoad data:\n\nparkinsons_train = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/parkinsons_train.csv\")\nparkinsons_test = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/parkinsons_test.csv\")\n\nThis dataset comes from Little et al.¬†(2008). The data includes various measurements of dysphonia (disorders of the voice) from 32 people, 24 with Parkinson‚Äôs disease (PD). Multiple measurements were taken per individual. The measurements we examine in this subset of the data include:\n\nname: patient ID\njitter: a measure of relative variation in fundamental frequency\nshimmer: a measure of variation in amplitude (dB)\nPPE: pitch period entropy\nHNR: a ratio of total components vs.¬†noise in the voice recording\nstatus: health status (1 for PD, 0 for healthy)\n\n\nExercise 1\nWrite down a main effects model to predict Parkinson‚Äôs status from HNR, shimmer, jitter and PPE.\n\n\nExercise 2\nFit your model from the previous exercise using the parkinsons_train data set.\n\n# code here\n\n\n\nExercise 3\nUse your model to predict PD status in the parkinsons_test data set with a decision boundary of p = 0.5. How many false positives do you observe? How many false negatives?\nNext change the decision boundary to 0.25. How many false positives and false negatives do you observe?\nWhich decision boundary do you prefer?\n\n# code here\n\n\n\nVisualizing model fits\nThere are many ways you can visualize a fitted model. Plotting the hyperplane is limited to simple two-variable (predictor, outcome) and three-variable (predictor, predictor, outcome) plots. Here we explore some useful visualizations for high-dimensional multivariate models.\n\nExample: logistic regression\n\nCreate a stacked bar plot with status on the x-axis and fill by whether or not the predicted status is correct or incorrect.\n\n\n# code here\n\n\n\nExample: ordinary least squares regression:\nScenario: we are trying to predict vocal amplitude variation (shimmer) from jitter and pitch period entropy (PPE).\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\nwhere\n\\(y\\): shimmer \\(x_1\\): jitter \\(x_2\\): PPE\n\nmyPredictiveModel = linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(shimmer ~ jitter + PPE, data = parkinsons_train)\n\nEven if we don‚Äôt have a test data set, we could still create a new column of predictions like before:\n\n# predict based on new data\npredict_train = parkinsons_train %>%\n  mutate(myPrediction = predict(myPredictiveModel, parkinsons_train)$.pred)\n\nFrom here we can plot \\(\\hat{y}\\) vs \\(y\\):\n\npredict_train %>%\n  ggplot(aes(x = shimmer, y = myPrediction)) +\n  geom_point() +\n  labs(x = \"True Shimmer (dB)\", y = \"Predicted shimmer (dB)\", title = \"Predicted vs true shimmer values\") +\n  geom_abline(slope = 1, intercept = 0, color = \"steelblue\")\n\n\n\n\nAlternatively, we could create a residual plot. Residual plots can be used to assess whether a linear model is appropriate.\nA common assumption of linear regression models is that the error term, \\(\\epsilon\\), has constant variance everywhere.\n\nIf the linear model is appropriate, a residual plot should show this.\nPatterned or nonconstant residual spread may sometimes be indicative a model is missing predictors or missing interactions.\n\n\n\nExercise 4\nCreate a new column residuals in predict_train and save your data frame as predict_train_2\n\n# code here\n\n\npredict_train_2 %>%\n  ggplot(aes(x = myPrediction, y = residuals)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(x = \"Predicted shimmer (dB)\", y = \"Residual\")"
  }
]
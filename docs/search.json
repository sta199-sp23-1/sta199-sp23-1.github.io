[
  {
    "objectID": "labs/lab-7.html",
    "href": "labs/lab-7.html",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "",
    "text": "Important\n\n\n\nThis lab is due Monday, April 10th at 5:00pm.\nToday we will continue learning about hypothesis testing using the chickwts that comes built in with R. From the help file: “An experiment was conducted to measure and compare the effectiveness of various feed supplements on the growth rate of chickens… Newly hatched chicks were randomly allocated into six groups, and each group was given a different feed supplement. Their weights in grams after six weeks are given along with feed types.” We will evaluate the strength of statistical evidence that the various feed groups had significantly different expected weights."
  },
  {
    "objectID": "labs/lab-7.html#packages",
    "href": "labs/lab-7.html#packages",
    "title": "Lab 7 - Logistic regression",
    "section": "Packages",
    "text": "Packages\nYou’ll need the following packages for today’s lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox)"
  },
  {
    "objectID": "labs/lab-7.html#data",
    "href": "labs/lab-7.html#data",
    "title": "Lab 7 - Logistic regression",
    "section": "Data",
    "text": "Data\nThe data can be found in the dsbox package, and it’s called gss16. Since the dataset is distributed with the package, we don’t need to load it separately; it becomes available to us when we load the package.\nIf you would like to explicitly load the data into your environment so you can view it, you can do so by running this code.\n\ngss16 <- gss16\n\nYou can find out more about the dataset by inspecting its documentation, which you can access by running ?gss16 in the Console or using the Help menu in RStudio to search for gss16. You can also find this information here."
  },
  {
    "objectID": "labs/lab-7.html#exercise-1---data-wrangling",
    "href": "labs/lab-7.html#exercise-1---data-wrangling",
    "title": "Lab 7 - Logistic regression",
    "section": "Exercise 1 - Data wrangling",
    "text": "Exercise 1 - Data wrangling\n\n\n\n\n\n\nImportant\n\n\n\nRemember: For each exercise, you should choose one person to type. All others should contribute to the discussion, but only one person should type up the answer, render the document, commit, and push to GitHub. All others should not touch the document.\n\n\n\nCreate a new data frame called gss16_advfront that includes the variables advfront, educ, polviews, and wrkstat. Then, use the drop_na() function to remove rows that contain NAs from this new data frame.\nTransform the advfront variable such that it has two levels: \"Strongly agree\" and \"Agree\" should both be mapped to \"Agree\" and the remaining levels should all be relabeled \"Not agree\". Make sure the resulting levels are in the following order: \"Agree\" and \"Not agree\".\n\nHint: use the factor() function inside a mutate() statement to relabel the original levels. Be sure to list the levels in order so that they are correctly ordered after relabeling.\n\nSimilarly to part b, combine the levels of the polviews variable such that levels that have the word “liberal” in them are lumped into a level called \"Liberal\" and those that have the word “conservative” in them are lumped into a level called \"Conservative\". Make sure the levels are in the following order: \"Conservative\" , \"Moderate\", and \"Liberal\". Finally, count() how many times each new level appears in the polviews variable.\n\nHint: be careful if you manually type out the levels in the original polviews variable to note that there are typos in two of the original levels “slightly conservative” and “extremely conservative” are both misspelled, and so you will need to match those misspellings in your call to factor().\n\n\n\n\n\n\nImportant\n\n\n\nAfter the team member working on Exercise 1 renders, commits, and pushes, all other team members should pull. Then, choose a new team member to write the answer to Exercise 2. (And so on for the remaining exercises.)"
  },
  {
    "objectID": "labs/lab-7.html#exercise-2---train-and-test-sets",
    "href": "labs/lab-7.html#exercise-2---train-and-test-sets",
    "title": "Lab 7 - Logistic regression",
    "section": "Exercise 2 - Train and test sets",
    "text": "Exercise 2 - Train and test sets\nNow, let’s split the data into training and test sets so that we can evaluate the models we’re going to fit by how well they predict outcomes on data that wasn’t used to fit the models.\nSpecify a random seed of 1234 (i.e., include set.seed(1234) at the beginning of your code chunk), and then split gss16_advfront randomly into a training set train_data and a test set test_data. Do this so that the training set contains 80% of the rows of the original data."
  },
  {
    "objectID": "labs/lab-7.html#exercise-3---logistic-regression",
    "href": "labs/lab-7.html#exercise-3---logistic-regression",
    "title": "Lab 7 - Logistic regression",
    "section": "Exercise 3 - Logistic Regression",
    "text": "Exercise 3 - Logistic Regression\n\nUsing the training data, fit a logistic regression model that predicts advfront using educ. In particular, the model should predict the probability that advfront has value \"Not agree\". Name this model model1. Report the tidy model output.\nWrite out the fitted model equation in proper notation. State the meaning of any variables in the context of the data.\nUsing your fitted model, report the estimated probability of agreeing with the following statement: Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government (Agree in advfront) if you have an education of 7 years."
  },
  {
    "objectID": "labs/lab-7.html#exercise-4---another-model",
    "href": "labs/lab-7.html#exercise-4---another-model",
    "title": "Lab 7 - Logistic regression",
    "section": "Exercise 4 - Another model",
    "text": "Exercise 4 - Another model\n\nAgain using the training data, fit a new logistic regression model that adds the additional explanatory variable of polviews. Name this model model2. Report the tidy output.\nNow, report the estimated probability of agreeing with the following statement: Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government (Agree in advfront) if you have an education of 7 years and are Conservative."
  },
  {
    "objectID": "labs/lab-7.html#exercise-5---evaluating-models-with-aic",
    "href": "labs/lab-7.html#exercise-5---evaluating-models-with-aic",
    "title": "Lab 7 - Logistic regression",
    "section": "Exercise 5 - Evaluating models with AIC",
    "text": "Exercise 5 - Evaluating models with AIC\n\nReport the AIC values for each of model1 and model2.\nBased on your results in part a, does it appear that including political views in addition to years of education is useful for modeling whether employees agree with the statement “Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government”? Explain."
  },
  {
    "objectID": "labs/lab-7.html#exercise-6---evaluating-models-using-test-data",
    "href": "labs/lab-7.html#exercise-6---evaluating-models-using-test-data",
    "title": "Lab 7 - Logistic regression",
    "section": "Exercise 6 - Evaluating models using test data",
    "text": "Exercise 6 - Evaluating models using test data\n\nFor each of model1 and model2, report the number of false positive and false negatives when making predictions on the test_data with a decision boundary of 0.5.\nDo these results provide much information about which model you would prefer for a prediction task? If so, which model would you choose?\nDo you think a decision boundary of 0.5 makes sense here or would you adjust it?\n\nThe ROC curve provides a way to compare predictive performance of binary classifiers across the full range of decision boundaries. Notes about the ROC curve can be found here https://sta199-s23-2.github.io/ae-sa/ae-16-A.html."
  },
  {
    "objectID": "prepare/prep21.html",
    "href": "prepare/prep21.html",
    "title": "Prepare",
    "section": "",
    "text": "Watch Central limit theorem\nRead (optional): 13.1, 13.2, 13.3: inference with mathematical models"
  },
  {
    "objectID": "prepare/prep20.html",
    "href": "prepare/prep20.html",
    "title": "Prepare",
    "section": "",
    "text": "Watch Bootstrapping\nRead (optional): section 12"
  },
  {
    "objectID": "prepare/prep22.html",
    "href": "prepare/prep22.html",
    "title": "Prepare",
    "section": "",
    "text": "Read 13.6 Case Study (interval): Stents\nRead 19.2 Mathematical model for a mean"
  },
  {
    "objectID": "prepare/prep23.html",
    "href": "prepare/prep23.html",
    "title": "Prepare",
    "section": "",
    "text": "Watch Hypothesis testing\nRead (optional): chapter 11: hypothesis testing with randomization"
  },
  {
    "objectID": "teaching-resources/GitHubClassroomManagement.html",
    "href": "teaching-resources/GitHubClassroomManagement.html",
    "title": "GitHub Classroom Management",
    "section": "",
    "text": "# $ git clone ghclass\n# $ git checkout artifacts\n\n#devtools::install(\"/path/to/ghclass\")\nEach code chunk below is designed to be self-contained (no dependency between chunks)."
  },
  {
    "objectID": "teaching-resources/GitHubClassroomManagement.html#to-make-new-repos",
    "href": "teaching-resources/GitHubClassroomManagement.html#to-make-new-repos",
    "title": "GitHub Classroom Management",
    "section": "To make new repos",
    "text": "To make new repos\n\n\n\n\nCreate a new repo in the organization called “lab1” with a README.md\nPush to “lab1”: lab1.qmd, data/any_relevant_data.csv and optionally lab1.Rproj.\nSelect the repository, click “Settings” and check “Template repository”.\nFollow the code below.\n\n\n# initial assignment creation\norg = \"sta199-sp23-1\"\nusernames = ghclass::org_members(org)\n\n## edit this:\nassignment_template_repo = \"lab0_template\"\n\nghclass::org_create_assignment(\n  org = org,\n  repo = paste0(assignment_template_repo, \"-\", usernames),\n  user = usernames,\n  source_repo = paste0(org, \"/\", assignment_template_repo)\n)\n\n\n\n\nFeel free to test lab creation on my dummy account “fishswish” or on your own account with the code below\n\norg = \"sta199-sp23-1\"\nusernames = \"fishswish\"\nassignment_template_repo = \"lab0_template\"\nghclass::org_create_assignment(\n  org = org,\n  repo = paste0(assignment_template_repo, \"-\", usernames),\n  user = usernames,\n  source_repo = paste0(org, \"/\", assignment_template_repo)\n)\n\nIt’s possible some people are not in the organization and will need to be have additional repos created manually following the steps below:\n\ngo to template repo and click the green “Use this template” -> “Create a new repository” -> name it “lab-x-their_github_username” -> leave “Private” selected and click “Create repository from template”\nnext in the new repo go to “Settings” -> “Collaborators and Teams” -> click the green “Add people” button and type in their GitHub username.\nnext make sure to go to the “People” section of the organization and select “Invite member” so they join the organization too. Owning a repo in the org just makes them an outside collaborator. They still have to actually join the org"
  },
  {
    "objectID": "teaching-resources/GitHubClassroomManagement.html#oops.-file-management-after-you-create-repos",
    "href": "teaching-resources/GitHubClassroomManagement.html#oops.-file-management-after-you-create-repos",
    "title": "GitHub Classroom Management",
    "section": "Oops. File management after you create repos",
    "text": "Oops. File management after you create repos\nThe magic happens in repo_add_file(). Be sure to edit/check each item below, specifically:\n\norg as appropriate\nrepos_to_modify object\narguments to repo_add_file()\n\n\norg = \"sta199-sp23-1\"\nusernames = ghclass::org_members(org)\n\n# get all existing labX repos\nrepos_to_modify = org_repos(org, filter = \"practice1-\") # edit this \"filter\"\n\nghclass::repo_add_file(repo = repos_to_modify,\n                       branch = \"main\",\n                       repo_folder = \"\", # edit this to be path to folder in repo\n                       message = \"update README\", # update to be a meaningful commit message\n                       file = \"~/Desktop/README.md\", # update local path to the file you want to add\n                       overwrite = TRUE) # if you want to overwrite or not"
  },
  {
    "objectID": "teaching-resources/GitHubClassroomManagement.html#creating-team-assignments",
    "href": "teaching-resources/GitHubClassroomManagement.html#creating-team-assignments",
    "title": "GitHub Classroom Management",
    "section": "Creating team assignments",
    "text": "Creating team assignments\n\n# example data frame for demo purposes\n# you will need to format your data frame to look like this\nroster = data.frame(\n  github = c(\"fishswish\", \"athos00\"),\n  lab1 = rep(\"lab1-team1\", 2)\n)\n\n# edit each item below\norg_create_assignment(\n  org = \"sta199-sp23-1\",\n  user = roster$github,\n  repo = roster$lab1,\n  team = roster$lab1,\n  source_repo = \"sta199-sp23-1/lab1_template\",\n  private = TRUE\n)\n\n\nroster = data.frame(\n  github = c(\"devinjohnson7\", \"athos00\"),\n  lab1 = rep(\"lab1-team-awesome\", 2)\n)\n\n# edit each item below\norg_create_assignment(\n  org = \"sta323-sp23\",\n  user = roster$github,\n  repo = roster$lab1,\n  team = roster$lab1,\n  source_repo = \"sta323-sp23/lab-1\", # template file\n  private = TRUE\n)\n\n\norg = \"sta323-sp23\"\nrepos = ghclass::org_repos(org, filter = \"lab-2\")\n\n\n# repos = \"sta323-sp23/lab-1\"\nghclass::action_artifact_delete(repos, ids=action_artifacts(repos, which=\"all\"))"
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Lab 6 - Logistic regression",
    "section": "",
    "text": "Important\n\n\n\nThis lab is due Monday, March 27 at 5:00pm."
  },
  {
    "objectID": "labs/lab-6.html#packages",
    "href": "labs/lab-6.html#packages",
    "title": "Lab 6 - Logistic regression",
    "section": "Packages",
    "text": "Packages\nYou’ll need the following packages for today’s lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox)"
  },
  {
    "objectID": "labs/lab-6.html#data",
    "href": "labs/lab-6.html#data",
    "title": "Lab 6 - Logistic regression",
    "section": "Data",
    "text": "Data\nThe data can be found in the dsbox package, and it’s called gss16. Since the data set is distributed with the package, we don’t need to load it separately; it becomes available to us when we load the package.\nIf you would like to explicitly load the data into your environment so you can view it, you can do so by running this code.\n\ngss16 <- gss16\n\nYou can find out more about the data set by inspecting its documentation, which you can access by running ?gss16 in the Console or using the Help menu in RStudio to search for gss16. You can also find this information here."
  },
  {
    "objectID": "labs/lab-6.html#exercise-1---data-wrangling",
    "href": "labs/lab-6.html#exercise-1---data-wrangling",
    "title": "Lab 6 - Logistic regression",
    "section": "Exercise 1 - Data wrangling",
    "text": "Exercise 1 - Data wrangling\n\n\n\n\n\n\nImportant\n\n\n\nRemember: For each exercise, you should choose one person to type. All others should contribute to the discussion, but only one person should type up the answer, render the document, commit, and push to GitHub. All others should not touch the document.\n\n\n\nCreate a new data frame called gss16_advfront that includes the variables advfront, educ, polviews, and wrkstat. Then, use the drop_na() function to remove rows that contain NAs from this new data frame.\nTransform the advfront variable such that it has two levels: \"Strongly agree\" and \"Agree\" should both be mapped to \"Agree\" and the remaining levels should all be relabeled \"Not agree\". Make sure the resulting levels are in the following order: \"Agree\" and \"Not agree\".\n\nHint: use the factor() function inside a mutate() statement to relabel the original levels. Be sure to list the levels in order so that they are correctly ordered after relabeling.\n\nSimilarly to part b, combine the levels of the polviews variable such that levels that have the word “liberal” in them are lumped into a level called \"Liberal\" and those that have the word “conservative” in them are lumped into a level called \"Conservative\". Make sure the levels are in the following order: \"Conservative\" , \"Moderate\", and \"Liberal\". Finally, count() how many times each new level appears in the polviews variable.\n\nHint: be careful if you manually type out the levels in the original polviews variable to note that there are typos in two of the original levels “slightly conservative” and “extremely conservative” are both misspelled, and so you will need to match those misspellings in your call to factor().\n\n\n\n\n\n\nImportant\n\n\n\nAfter the team member working on Exercise 1 renders, commits, and pushes, all other team members should pull. Then, choose a new team member to write the answer to Exercise 2. (And so on for the remaining exercises.)"
  },
  {
    "objectID": "labs/lab-6.html#exercise-2---train-and-test-sets",
    "href": "labs/lab-6.html#exercise-2---train-and-test-sets",
    "title": "Lab 6 - Logistic regression",
    "section": "Exercise 2 - Train and test sets",
    "text": "Exercise 2 - Train and test sets\nNow, let’s split the data into training and test sets so that we can evaluate the models we’re going to fit by how well they predict outcomes on data that wasn’t used to fit the models.\nSpecify a random seed of 1234 (i.e., include set.seed(1234) at the beginning of your code chunk), and then split gss16_advfront randomly into a training set train_data and a test set test_data. Do this so that the training set contains 80% of the rows of the original data."
  },
  {
    "objectID": "labs/lab-6.html#exercise-3---logistic-regression",
    "href": "labs/lab-6.html#exercise-3---logistic-regression",
    "title": "Lab 6 - Logistic regression",
    "section": "Exercise 3 - Logistic Regression",
    "text": "Exercise 3 - Logistic Regression\n\nUsing the training data, fit a logistic regression model that predicts advfront using educ. In particular, the model should predict the probability that advfront has value \"Not agree\". Name this model model1. Report the tidy model output.\nWrite out the fitted model equation in proper notation. State the meaning of any variables in the context of the data.\nUsing your fitted model, report the estimated probability of agreeing with the following statement: Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government (Agree in advfront) if you have an education of 7 years."
  },
  {
    "objectID": "labs/lab-6.html#exercise-4---another-model",
    "href": "labs/lab-6.html#exercise-4---another-model",
    "title": "Lab 6 - Logistic regression",
    "section": "Exercise 4 - Another model",
    "text": "Exercise 4 - Another model\n\nAgain using the training data, fit a new logistic regression model that adds the additional explanatory variable of polviews. Name this model model2. Report the tidy output.\nNow, report the estimated probability of agreeing with the following statement: Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government (Agree in advfront) if you have an education of 7 years and are Conservative."
  },
  {
    "objectID": "labs/lab-6.html#exercise-5---evaluating-models-with-aic",
    "href": "labs/lab-6.html#exercise-5---evaluating-models-with-aic",
    "title": "Lab 6 - Logistic regression",
    "section": "Exercise 5 - Evaluating models with AIC",
    "text": "Exercise 5 - Evaluating models with AIC\n\nReport the AIC values for each of model1 and model2.\nBased on your results in part a, does it appear that including political views in addition to years of education is useful for modeling whether employees agree with the statement “Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government”? Explain."
  },
  {
    "objectID": "labs/lab-6.html#exercise-6---evaluating-models-using-test-data",
    "href": "labs/lab-6.html#exercise-6---evaluating-models-using-test-data",
    "title": "Lab 6 - Logistic regression",
    "section": "Exercise 6 - Evaluating models using test data",
    "text": "Exercise 6 - Evaluating models using test data\n\nFor each of model1 and model2, report the number of false positive and false negatives when making predictions on the test_data with a decision boundary of 0.5.\nDo these results provide much information about which model you would prefer for a prediction task? If so, which model would you choose?\nDo you think a decision boundary of 0.5 makes sense here or would you adjust it?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 199: Introduction to Data Science",
    "section": "",
    "text": "week\n      dow\n      date\n      what\n      topic\n      prepare\n      slides\n      ae\n      hw\n      lab\n      exam\n      project\n      notes\n    \n  \n  \n    0\nW\nJan 11\nLab 0\nHello R!\n\n\n\n\n\n\n\n\n    \nF\nJan 13\nLec 1\nWelcome to STA 199\n\n\n\n\n\n\n\n\n    1\nM\nJan 16\n\nNo class -- Martin Luther King Jr. Day\n\n\n\n\n\n\n\n\n    \nW\nJan 18\nLec 2\nIntro to statistics and plotting\n\n\n\n\n\n\n\n\n    \nF\nJan 20\nLec 3\nVisualizing various types of data\n\n\n\n\n\n\n\n\n    2\nM\nJan 23\nLab 1\nData visualization\n\n\n\n\n\n\n\n\n    \nW\nJan 25\nLec 4\nGrammar of data wrangling\n\n\n\n\n\n\n\nHomework 1 released\n    \nF\nJan 27\nLec 5\nWorking with multiple data frames\n\n\n\n\n\n\n\n\n    3\nM\nJan 30\nLab 2\nData wrangling\n\n\n\n\n\n\n\n\n    \nW\nFeb 1\nLec 6\nTidy data\n\n\n\n\n\n\n\n\n    \nF\nFeb 3\nLec 7\nData types and classes\n\n\n\n\n\n\n\nHomework 1 due\n    4\nM\nFeb 6\nLab 3\nData tidying\n\n\n\n\n\n\n\n\n    \nW\nFeb 8\nLec 8\nSpatial data\n\n\n\n\n\n\n\n\n    \nF\nFeb 10\nLec 9\nTroubleshooting / Review\n\n\n\n\n\n\n\nExam 1 released! Due Feb 14 5:00pm\n    5\nM\nFeb 13\nLab\nNo lab: exam 1\n\n\n\n\n\n\n\n\n    \nW\nFeb 15\nLec 10\nIntro to probability\n\n\n\n\n\n\n\n\n    \nF\nFeb 17\nLec 11\nConditional probability\n\n\n\n\n\n\n\nHomework 2 released\n    6\nM\nFeb 20\nLab 4\nMerge conflicts\n\n\n\n\n\n\n\n\n    \nW\nFeb 22\nLec 12\nSimple regression\n\n\n\n\n\n\n\n\n    \nF\nFeb 24\nLec 13\nMultiple regression I\n\n\n\n\n\n\n\nHomework 2 due\n    7\nM\nFeb 27\nLab 5\nPredicting a numerical outcome\n\n\n\n\n\n\n\n\n    \nW\nMar 1\nLec 14\nMultiple regression II\n\n\n\n\n\n\n\nHomework 3 released; project announced\n    \nF\nMar 3\nLec 15\nModel selection\n\n\n\n\n\n\n\n\n    8\nM\nMar 6\nLab\nWork on project proposal\n\n\n\n\n\n\n\n\n    \nW\nMar 8\nLec 16\nLogistic regression\n\n\n\n\n\n\n\nHomework 3 due; statistics experience released\n    \nF\nMar 10\nLec 17\nPrediction\n\n\n\n\n\n\n\nProject proposal due\n    9\nM\nMar 13\n\nNo class -- Spring Break\n\n\n\n\n\n\n\n\n    \nW\nMar 15\n\nNo class -- Spring Break\n\n\n\n\n\n\n\n\n    \nF\nMar 17\n\nNo class -- Spring Break\n\n\n\n\n\n\n\n\n    10\nM\nMar 20\nLab\nLogistic regression (lab)\n\n\n\n\n\n\n\n\n    \nW\nMar 22\nLec 18\nBootstrap\n\n\n\n\n\n\n\nHomework 4 released\n    \nF\nMar 24\nLec 19\nCentral limit theorem I\n\n\n\n\n\n\n\n\n    11\nM\nMar 27\nLab\nProject work day\n\n\n\n\n\n\n\n\n    \nW\nMar 29\nLec 20\nCentral limit theorem II\n\n\n\n\n\n\n\n\n    \nF\nMar 31\nLec 21\nHypothesis testing I\n\n\n\n\n\n\n\nHomework 4 due\n    12\nM\nApr 3\nLab\nHypothesis testing lab\n\n\n\n\n\n\n\n\n    \nW\nApr 5\nLec 22\nHypothesis testing II\n\n\n\n\n\n\n\nHomework 5 released\n    \nF\nApr 7\nLec 23\nHypothesis testing III\n\n\n\n\n\n\n\n\n    13\nM\nApr 10\nLab\nProject peer review\n\n\n\n\n\n\n\n\n    \nW\nApr 12\nLec 24\n\n\n\n\n\n\n\n\nHomework 5 due\n    \nF\nApr 14\nLec 25\nProject tips\n\n\n\n\n\n\n\nExam 2 released! Due Tue April 18 5:00pm\n    14\nM\nApr 17\nLab\nNo lab: exam 2\n\n\n\n\n\n\n\n\n    \nW\nApr 19\nLec 26\nEthics\n\n\n\n\n\n\n\n\n    \nF\nApr 21\nLec 27\nSpecial topic: cryptanalysis\n\n\n\n\n\n\n\n\n    15\nM\nApr 24\nLab\nProject presentations\n\n\n\n\n\n\n\n\n    \nW\nApr 26\nLec 28\nSpecial topic: forensic genetic analysis\n\n\n\n\n\n\n\nReport due; statistics experience due"
  },
  {
    "objectID": "ae/ae-18.html",
    "href": "ae/ae-18.html",
    "title": "Bootstrap",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nproject proposal feedback\nhomework 4 released"
  },
  {
    "objectID": "ae/ae-18.html#getting-started",
    "href": "ae/ae-18.html#getting-started",
    "title": "Bootstrap",
    "section": "Getting started",
    "text": "Getting started\nClone your ae18-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-18.html#today",
    "href": "ae/ae-18.html#today",
    "title": "Bootstrap",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to draw a bootstrap sample and calculate a bootstrap statistic\nuse infer to obtain a bootstrap distribution\ncalculate a confidence interval from the bootstrap distribution\ninterpret a confidence interval in context of the data"
  },
  {
    "objectID": "ae/ae-18.html#load-packages",
    "href": "ae/ae-18.html#load-packages",
    "title": "Bootstrap",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-18.html#load-data",
    "href": "ae/ae-18.html#load-data",
    "title": "Bootstrap",
    "section": "Load data",
    "text": "Load data\n\nmanhattan = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/manhattan.csv\"\n  )"
  },
  {
    "objectID": "ae/ae-18.html#notes-for-reference",
    "href": "ae/ae-18.html#notes-for-reference",
    "title": "Bootstrap",
    "section": "Notes (for reference)",
    "text": "Notes (for reference)\nBootstrapping is a re-sampling technique. The key idea is you have already collected a sample of size \\(N\\) from the population. To create a bootstrap sample, you sample with replacement from your original sample \\(N\\) times.\nLet’s say you measure the height of five Duke students in meters:\n\nheights = c(1.51, 1.62, 1.89, 2.01, 1.78)\n\nstudents = data.frame(heights)\n\nThere are many ways to create a bootstrap sample in R. We will focus on the tidy way below. which uses the infer package that loads with tidymodels.\n\nExample\n\nset.seed(2)\nstudents %>%\n  specify(response = heights) %>%\n  generate(reps = 1, type = \"bootstrap\")\n\nResponse: heights (numeric)\n# A tibble: 5 × 2\n# Groups:   replicate [1]\n  replicate heights\n      <int>   <dbl>\n1         1    1.78\n2         1    1.51\n3         1    1.78\n4         1    1.51\n5         1    2.01\n\n\n\n\n\n\n\n\nNote\n\n\n\nSampling is random. Notice the seed above ensures we get the same bootstrap sample.\n\n\nFrom here, we can compute a bootstrap statistic. E.g.\n\nset.seed(2)\nstudents %>%\n  specify(response = heights) %>%\n  generate(reps = 1, type = \"bootstrap\") %>%\n  calculate(stat = \"median\")\n\nResponse: heights (numeric)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  1.78"
  },
  {
    "objectID": "ae/ae-18.html#example-rent-in-manhattan",
    "href": "ae/ae-18.html#example-rent-in-manhattan",
    "title": "Bootstrap",
    "section": "Example: rent in Manhattan",
    "text": "Example: rent in Manhattan\nOn a given day in 2018, twenty one-bedroom apartments were randomly selected on Craigslist Manhattan from apartments listed as “by owner”. The data are in the manhattan data frame. We will use this sample to conduct inference on the typical rent of 1 bedroom apartments in Manhattan.\n\nPart 1: Drawing a bootstrap sample\nLet’s start by using bootstrapping to estimate the mean rent of one-bedroom apartments in Manhattan.\n\nExercise 1\nWhat is a point estimate (i.e. single number summary) of the typical rent?\n\n\nExercise 2\nLet’s bootstrap!\n\nTo bootstrap we will sample with replacement by drawing a value from the box.\nHow many draws do we need for our bootstrap sample?\n\nFill in the values from the bootstrap sample conducted in class. Once the values are filled in, un-comment the code.\n\n# class_bootstrap = c()\n\n\n\nExercise 3\n\nAbout what value do you expect the bootstrap statistic to take?\nCalculate the statistic from the bootstrap sample.\n\n\n# add code\n\n\n\n\nPart 2: Bootstrap confidence interval\nWe will calculate a 95% confidence interval for the mean rent of one-bedroom apartments in Manhattan.\nWe start by setting a seed to ensure our analysis is reproducible.\n\nGenerating the bootstrap distribution\nWe can use R to take many bootstrap samples, compute a statistic and then view the bootstrap distribution of that statistic.\nUn-comment the lines and fill in the blanks to create the bootstrap distribution of sample means and save the results in the data frame boot_dist.\nUse 1000 reps for the in-class activity. (You will use about 10,000 reps for assignments outside of class.)\n\nset.seed(7182022)\n\nboot_dist = manhattan #%>%\n  #specify(______) %>%\n  #generate(______) %>%\n  #calculate(______)\n\n\nHow many rows are in boot_dist?\nWhat does each row represent?\nWhat are the variables in boot_dist? What do they mean?\n\n\n\nVisualize the bootstrap distribution\nA sample statistic is a random variable, we can look at its distribution.\nVisualize the bootstrap distribution using a histogram. Describe the shape and center of the distribution.\n\n# add code\n\n\n\nCalculate the confidence interval\nUncomment the lines and fill in the blanks to construct the 95% bootstrap confidence interval for the mean rent of one-bedroom apartments in Manhattan.\n\n#___ %>%\n#  summarize(lower = quantile(______),\n  #          upper = quantile(______))\n\n\n\nInterpret the interval\nWrite the interpretation for the interval calculated above.\n\nQuestion: Does a confidence interval have to be symmetric?\nWhat is one advantage to using a 90% confidence interval instead of a 95% confidence interval to estimate a parameter? - What is one advantage to using a 99% confidence interval instead of a 95% confidence interval to estimate a parameter?"
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "HW 4 - Ultra Trail Running",
    "section": "",
    "text": "Important\n\n\n\nThis homework is due Friday, March 31st at 5:00pm."
  },
  {
    "objectID": "hw/hw-4.html#getting-started",
    "href": "hw/hw-4.html#getting-started",
    "title": "HW 4 - Ultra Trail Running",
    "section": "Getting Started",
    "text": "Getting Started\n\nGo to the Github Organization page and open your hw4-username repo\nClone the repository, open a new project in RStudio. It contains the starter documents you need to complete the homework assignment."
  },
  {
    "objectID": "hw/hw-4.html#exercises",
    "href": "hw/hw-4.html#exercises",
    "title": "HW 4 - Ultra Trail Running",
    "section": "Exercises",
    "text": "Exercises\n\nTo begin, join the data frames. Save your result as ultra. Next, drop all rows without an observed race time_in_seconds. Your final data frame should have 60924 rows and 20 columns. Print the number of rows of ultra to the screen.\nYour friend computes the mean time in seconds it takes participants of 170+ km races to finish. Your friend also constructs a 90% confidence interval and states, “There is a .9 probability that the sample mean race time for races over 170 km is between 130000 and 160000 seconds.” Without running any code, what is wrong with your friend’s statement? Correct the statement as well (without running any code).\nReport the mean race time for races 170 km or longer and construct a 99% bootstrap confidence interval for the mean race time using set.seed(6) and 5000 reps.\n\nNext, check that central limit theorem holds, do you need to make any assumptions? Use CLT to construct a 99% confidence interval. Compare your result to your bootstrap interval. Interpret the interval.\n\nNow, calculate a 95% bootstrap confidence interval for mean race time for races 170 km or longer (same bootstrap distribution as the previous exercise). Which of the following characteristics change compared to the interval created in question 3? (Please answer “yes” or “no” to each of the following. If you answer “yes”, explain the change observed.)\n\n\nThe center of the confidence interval\nThe width of the confidence interval\n\n\nDoes the standard error of the bootstrap distribution change when you change confidence levels? Use appropriate code to justify your answer.\n\n\n\nIn your own words, describe the process of how to create a bootstrap confidence interval. Be specific.\nIdentify at least two anomalies in the data set. For example, if the same runner ran a race in two different countries on the same day, that would be an anomaly. While this example is false, there are indeed real anomalies in the data. For full credit, display your anomaly either via code (printing to screen) or via a visualization. Discuss your findings.\nGiven the race was in Argentina, what is the median age of runners? Can you use Central Limit Theorem (CLT) to construct a confidence interval about this estimate? Explain. If not, construct a 90% bootstrap confidence interval that bounds the median from below. Use set.seed(8) and 10000 reps. Interpret your interval in context.\n\n– Note, this is not a symmetric confidence interval!\n\nA fellow colleague wants your next investigation to be about mean race times in France. They are adamant that we must use 80% confidence intervals when reporting results. In 1-2 sentences, discuss both any potential benefits and concerns you may have with creating a confidence interval with this low of level."
  },
  {
    "objectID": "hw/hw-4.html#rubric",
    "href": "hw/hw-4.html#rubric",
    "title": "HW 4 - Ultra Trail Running",
    "section": "Rubric",
    "text": "Rubric\n\nEx 1: 5 pts.\nEx 2: 4 pts.\nEx 3: 5 pts.\nEx 4: 10 pts.\nEx 5: 6 pts.\nEx 6: 6 pts.\nEx 7: 5 pts\nEx 8: 4 pts\nWorkflow and formatting - 5 pts"
  },
  {
    "objectID": "hw/hw-4.html#submission",
    "href": "hw/hw-4.html#submission",
    "title": "HW 4 - Ultra Trail Running",
    "section": "Submission",
    "text": "Submission\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials Duke Net ID and log in using your Net ID credentials.\nClick on your STA 199 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark all the pages associated with exercise. All the pages of your homework should be associated with at least one question (i.e., should be “checked”). If you do not do this, you will be subject to lose points on the assignment.\nSelect all pages of your PDF submission to be associated with the “Workflow & formatting” question.\n\n\n\n\n\n\n\nNote\n\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes:\n\nlinking all pages appropriately on Gradescope\nputting your name in the YAML at the top of the document\ncommitting the submitted version of your .qmd to GitHub\nAre you under the 80 character code limit? (You shouldn’t have to scroll to see all your code).\nPipes %>%, |> and ggplot layers + should be followed by a new line\nYou should be consistent with stylistic choices, e.g. only use 1 of = vs <- and %>% vs |>\nAll binary operators should be surrounded by space. For example x + y is appropriate. x+y is not."
  },
  {
    "objectID": "ae/ae-19.html",
    "href": "ae/ae-19.html",
    "title": "Central limit theorem",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nproject proposal feedback\nhomework 4 due date updated"
  },
  {
    "objectID": "ae/ae-19.html#getting-started",
    "href": "ae/ae-19.html#getting-started",
    "title": "Central limit theorem",
    "section": "Getting started",
    "text": "Getting started\nClone your ae19-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-19.html#today",
    "href": "ae/ae-19.html#today",
    "title": "Central limit theorem",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nUse Central Limit Theorem to define distribution of sample means\nCalculate probabilities from the normal distribution\nUse Central Limit Theorem (CLT) to conduct inference on a population mean"
  },
  {
    "objectID": "ae/ae-19.html#load-packages",
    "href": "ae/ae-19.html#load-packages",
    "title": "Central limit theorem",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-19.html#load-data",
    "href": "ae/ae-19.html#load-data",
    "title": "Central limit theorem",
    "section": "Load data",
    "text": "Load data\n\nmanhattan = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/manhattan.csv\"\n  )"
  },
  {
    "objectID": "ae/ae-19.html#notes-for-reference",
    "href": "ae/ae-19.html#notes-for-reference",
    "title": "Bootstrap",
    "section": "Notes (for reference)",
    "text": "Notes (for reference)\nBootstrapping is a re-sampling technique. The key idea is you have already collected a sample of size \\(N\\) from the population. To create a bootstrap sample, you sample with replacement from your original sample \\(N\\) times.\nLet’s say you measure the height of five Duke students in meters:\n\nheights = c(1.51, 1.62, 1.89, 2.01, 1.78)\n\nstudents = data.frame(heights)\n\nThere are many ways to create a bootstrap sample in R. We will focus on the tidy way below. which uses the infer package that loads with tidymodels.\n\nExample\n\nset.seed(2)\nstudents %>%\n  specify(response = heights) %>%\n  generate(reps = 1, type = \"bootstrap\")\n\nResponse: heights (numeric)\n# A tibble: 5 × 2\n# Groups:   replicate [1]\n  replicate heights\n      <int>   <dbl>\n1         1    1.78\n2         1    1.51\n3         1    1.78\n4         1    1.51\n5         1    2.01\n\n\n\n\n\n\n\n\nNote\n\n\n\nSampling is random. Notice the seed above ensures we get the same bootstrap sample.\n\n\nFrom here, we can compute a bootstrap statistic. E.g.\n\nset.seed(2)\nstudents %>%\n  specify(response = heights) %>%\n  generate(reps = 1, type = \"bootstrap\") %>%\n  calculate(stat = \"median\")\n\nResponse: heights (numeric)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  1.78"
  },
  {
    "objectID": "ae/ae-19.html#example-rent-in-manhattan",
    "href": "ae/ae-19.html#example-rent-in-manhattan",
    "title": "Bootstrap",
    "section": "Example: rent in Manhattan",
    "text": "Example: rent in Manhattan\nOn a given day in 2018, twenty one-bedroom apartments were randomly selected on Craigslist Manhattan from apartments listed as “by owner”. The data are in the manhattan data frame. We will use this sample to conduct inference on the typical rent of 1 bedroom apartments in Manhattan.\n\nPart 1: Drawing a bootstrap sample\nLet’s start by using bootstrapping to estimate the mean rent of one-bedroom apartments in Manhattan.\n\nExercise 1\nWhat is a point estimate (i.e. single number summary) of the typical rent?\n\n\nExercise 2\nLet’s bootstrap!\n\nTo bootstrap we will sample with replacement by drawing a value from the box.\nHow many draws do we need for our bootstrap sample?\n\nFill in the values from the bootstrap sample conducted in class. Once the values are filled in, un-comment the code.\n\n# class_bootstrap = c()\n\n\n\nExercise 3\n\nAbout what value do you expect the bootstrap statistic to take?\nCalculate the statistic from the bootstrap sample.\n\n\n# add code\n\n\n\n\nPart 2: Bootstrap confidence interval\nWe will calculate a 95% confidence interval for the mean rent of one-bedroom apartments in Manhattan.\nWe start by setting a seed to ensure our analysis is reproducible.\n\nGenerating the bootstrap distribution\nWe can use R to take many bootstrap samples, compute a statistic and then view the bootstrap distribution of that statistic.\nUn-comment the lines and fill in the blanks to create the bootstrap distribution of sample means and save the results in the data frame boot_dist.\nUse 1000 reps for the in-class activity. (You will use about 10,000 reps for assignments outside of class.)\n\nset.seed(7182022)\n\nboot_dist = manhattan #%>%\n  #specify(______) %>%\n  #generate(______) %>%\n  #calculate(______)\n\n\nHow many rows are in boot_dist?\nWhat does each row represent?\nWhat are the variables in boot_dist? What do they mean?\n\n\n\nVisualize the bootstrap distribution\nA sample statistic is a random variable, we can look at its distribution.\nVisualize the bootstrap distribution using a histogram. Describe the shape and center of the distribution.\n\n# add code\n\n\n\nCalculate the confidence interval\nUncomment the lines and fill in the blanks to construct the 95% bootstrap confidence interval for the mean rent of one-bedroom apartments in Manhattan.\n\n#___ %>%\n#  summarize(lower = quantile(______),\n  #          upper = quantile(______))\n\n\n\nInterpret the interval\nWrite the interpretation for the interval calculated above.\n\nQuestion: Does a confidence interval have to be symmetric?\nWhat is one advantage to using a 90% confidence interval instead of a 95% confidence interval to estimate a parameter? - What is one advantage to using a 99% confidence interval instead of a 95% confidence interval to estimate a parameter?"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus.\nOur course coordinator, Edric Tam, will handle all inquiries about late work, extensions and absences in accordance with the late policy described in the syllabus below. Please send emails to sta199@duke.edu."
  },
  {
    "objectID": "course-syllabus.html#class-time-and-location",
    "href": "course-syllabus.html#class-time-and-location",
    "title": "Syllabus",
    "section": "Class time and location",
    "text": "Class time and location\n\n\n\nLecture\nW/F: 10:15-11:30 AM\nLSRC B101\nDr. Alexander Fisher\n\n\nLab 1\nM: 10:15-11:30 AM\nPerkins LINK 087 (Classroom 3)\nDavid Buch, Isabella Swigart\n\n\nLab 2\nM: 12:00-1:15 PM\nPerkins LINK 087 (Classroom 3)\nAlonso M Guerrero Castañeda, Kelly Huang\n\n\nLab 3\nM: 1:45-3:00 PM\nPerkins LINK 087 (Classroom 3)\nShuo Wang, Eva Noel\n\n\nLab 4\nM: 3:30-4:35 PM\nPerkins LINK 087 (Classroom 3)\nNathan Varberg, Ben Wallace\n\n\nLab 5\nM: 5:15-6:30 PM\nPerkins LINK 087 (Classroom 3)\nNaomie Gao, Miles Eng\n\n\nLab 11\nM: 5:15 - 6:30 PM\nPerkins LINK 071 (Classroom 5)\nOne Chowdhury, Konnie Huang"
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office hours",
    "text": "Office hours\nClick here for the instructor and TA office hours locations and Zoom links. You are welcome to attend the office hours of any member of the teaching team, regardless of section."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nAll books are freely available online.\n\n\n\nR for Data Science, 2e\nGrolemund, Wickham\nO’Reilly, 2nd edition, 2022\nHard copy only available of 1st edition\n\n\nIntroduction to Modern Statistics\nÇetinkaya-Rundel, Hardin\nOpenIntro Inc., 1st Edition, 2021\nHard copy available on Amazon"
  },
  {
    "objectID": "course-syllabus.html#course-learning-objectives",
    "href": "course-syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will…\n\nlearn to explore, visualize, and analyze data in a reproducible manner\ngain experience in data wrangling and munging, exploratory data analysis, predictive modeling, and data visualization\nwork on problems and case studies inspired by and based on real-world questions and data\nlearn to effectively communicate results through written assignments and project presentation\ndevelop your own question about a data set of your choosing and use techniques from this class to answer the question"
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe activities and assessments in this course are designed to help you successfully achieve the course learning objectives. They are designed to follow the Prepare, Practice, Perform format.\n\nPrepare: Includes short videos, reading assignments, and lectures to introduce new concepts and ensure a basic comprehension of the material. The goal is to help you prepare for the in-class activities during lecture.\nPractice: In-class application exercises where you will begin to master the concepts and methods introduced in the prepare assignment. The activities will graded for completion, as they are designed for you to gain experience with the statistical and computing techniques before working on graded assignments.\nPerform: Includes labs, homework, exams, and the project. These assignments build upon the prepare and practice assignments and are the opportunity for you to demonstrate your understanding of the course material and how it is applied to analyze real-world data.\n\n\nTeam work policy\nThe final project and several labs will be completed in teams. GitHub commits will be used to measure individual contribution to the assignment. All group members are expected to participate equally. Commit history may be used to give individual team members different grades. Your grade may differ from the rest of your group.\n\n\nApplication exercises (practice)\nEach lecture, we will work through application exercise (AEs). These serve as notes you will fill in during class mixed with practice exercises. Exercises which give you an opportunity to practice statistical concepts and code introduced in the prepare assignment.\nBecause these AEs are for practice, they will be graded based on completion, i.e., a good-faith effort has been made in attempting all parts. Successful on-time completion of at least 80% of AEs will result in full credit for AEs in the final course grade. To submit an AE, you simply need to push your completed AE to the designated repo in GitHub.\n\n\nLabs (perform)\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios, with a focus on the computation. Some lab assignments will be completed in teams, and all team members are expected to contribute equally to the completion of each assignment. You are expected to use the team’s git repository on the course’s GitHub page as the central platform for collaboration. Commits to this repository will be used as a metric of each team member’s relative contribution for each lab, see team work policy above. Lab assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and be submitted for grading in Gradescope.\nThe lowest lab grade will be dropped at the end of the semester.\n\n\nHomework (perform)\nIn homework, you will apply what you’ve learned during lecture and lab to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and GitHub and submitted as a PDF in Gradescope.\nHomework assignments are due at 5:00 PM ET on the indicated due date.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you’re learning in the course connects with society more broadly.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams (perform)\nThere will be two, take-home, open-note exams. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. Each exam will include small analysis and computational tasks related to the content in the prepare, practice, and perform assignments. More details about the content and structure of the exams will be discussed during the semester.\n\n\nProject (perform)\nThe purpose of the project is to apply what you’ve learned throughout the semester to analyze an interesting data-driven research question. The project will be completed with your lab teams, and each team will present their work in video and in writing during the final exam period. More information about the project will be provided during the semester."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n30%\n\n\nLabs\n15%\n\n\nProject\n15%\n\n\nExam 01\n18%\n\n\nExam 02\n18%\n\n\nApplication Exercises\n4%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60"
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\nInclusive community: It is my intent that students from all backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nAcademic honesty: You should be familiar with Duke’s community standard: https://studentaffairs.duke.edu/conduct/about-us/duke-community-standard\nBy enrolling in this course, you commit to upholding Duke’s community standard reproduced as follows:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations in academic honesty standards as outlined in the Duke Community Standard and those specific to this course will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action.\nPlease abide by the following as you work on assignments in this course: - You may discuss lab assignments with other students; however, you may not directly share (or copy) code or write up with other students. For team assignments, you may collaborate freely within your team. You may discuss the assignment with other teams; however, you may not directly share (or copy) code or write up with another team. Unauthorized sharing (or copying) of the code or write up will be considered a violation for all students involved. - You may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date. - Reusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. Stack- Overflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nLate policy:\nHomeworks and labs can be turned in within 72 hours of the deadline for grade penalty (5% off per day, stacks to 15% by the third day). Exams cannot be turned in late and can only be excused under exceptional circumstances. The Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\n\nAll exemptions will be handled by course coordinator, Ed Tam, at sta199@duke.edu\nLast minute coding/rendering issues will not be granted extensions.\n\nProcedures for Requesting a Regrade\nEvery effort will be made to mark your work accurately. We are on your side, and want you to receive every point you have worked to earn. However, sometimes grading mistakes happen. If you believe that an error has been made, return the paper to the instructor within four days, stating your claim in writing.\nThe following claims will be considered for re-grading:\n\npoints are not totaled correctly;\nthe grader did not see a correct answer that is on your paper;\nyour answer is the same as the correct answer, but in a different form (e.g., you wrote a correct answer as 1/3 and the grader was looking for .333);\nyour answer to a free response question is essentially correct but stated slightly differently than the grader’s expectation.\n\nThe following claims will not be considered for re-grading:\n\narguments about the number of points lost;\n\n\n\narguments about question wording.\n\nConsidering re-grades consumes time and resources that TAs and the instructor would rather spend helping you understand material. Please bring only claims of type (i), (ii), (iii), or (iv) to our attention.\nCommunication: All lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website. Announcements will be emailed to the class through sakai. Please check your email regularly to ensure you have the latest announcements for the course. For quick communication with your peers and the teaching team, see the course slack for general questions and discussion.\nAccessibility: If there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations. The Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu."
  },
  {
    "objectID": "course-syllabus.html#additional-resources",
    "href": "course-syllabus.html#additional-resources",
    "title": "Syllabus",
    "section": "Additional resources",
    "text": "Additional resources\nStudent mental health and wellness are of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu. The Academic Resource Center (the ARC) offers services to support students academically during their undergraduate careers at Duke. The ARC can provide support with time management, academic skills and strategies, course-specific tutoring, and more. ARC services are available free to any Duke undergraduate student, studying any discipline.\nDuWell: (919) 681-8421, provides Moments of Mindfulness (stress management and resilience building) and meditation programming (Koru workshop) to assist students in developing a daily emotional well-being practice. To see schedules for programs please see https://studentaffairs.duke.edu/duwell. All are welcome and no experience necessary.\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance: https://students.duke.edu/wellness/dukereach/\nCounseling and Psychological Services (CAPS). CAPS services include individual and group counseling services, psychiatric services, and workshops. To initiate services, walk-in/call-in 9-4 M,W,Th,F and 9-6 Tuesdays. CAPS also provides referral to off- campus resources for specialized care.\n\n660-1000 or https://students.duke.edu/wellness/caps/\n\nTimelyCare (formerly known as Blue Devils Care). An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu"
  },
  {
    "objectID": "ae/ae-19.html#notes",
    "href": "ae/ae-19.html#notes",
    "title": "Central limit theorem",
    "section": "Notes",
    "text": "Notes\nHow do we know when to expect a normal distribution to show up?\nLooking at the data from last time, we can see an example where the distribution of sample means looked approximately normal but the distribution of sample medians does not.\nExample:\n\nset.seed(1)\nboot_dist = manhattan %>%\n  specify(response = rent) %>% \n  generate(reps = 1000, type = \"bootstrap\")\n\nboot_dist %>%\n  calculate(stat = \"mean\") %>%\n  visualize() +\n  labs(x = \"Sample mean\", \n       title = \"Simulated distribution of the sample mean\")\n\n\n\nboot_dist %>%\n  calculate(stat = \"median\") %>%\n  visualize() + \n  labs(x = \"Sample median\",\n       title = \"Simulated distribution of the sample median\")\n\n\n\n\nAre there times when the sample mean will not look normal?\n\nDemo\nThe proportion of observed successes for a binary variable is a sample mean.\nScenario: You flip a biased coin numFlips times and compute the sample mean (the proportion of flips that land heads). You repeat this experiment 1000 times and obtain a distribution of sample means.\n\nHow does the shape of the distribution change as you increase the number of coin flips per sample?\n\n\nset.seed(714)\nnumFlips = 1\nnumHeads = rbinom(n = 1000, size = numFlips, prob = 0.9)\ndf = data.frame(numHeads) # new data frame called df\ndf %>%\nmutate(propHeads = numHeads / numFlips) %>%\nggplot(aes(x = propHeads)) +\ngeom_histogram(binwidth = .01)"
  },
  {
    "objectID": "ae/ae-19.html#what-is-the-central-limit-theorem",
    "href": "ae/ae-19.html#what-is-the-central-limit-theorem",
    "title": "Central limit theorem",
    "section": "What is the central limit theorem?",
    "text": "What is the central limit theorem?\nThe central limit theorem is a statement about the distribution of the sample mean, \\(\\bar{x}\\).\nThe central limit theorem guarantees that, when certain criteria are satisfied, the sample mean (\\(\\bar{x}\\)) is normally distributed.\nSpecifically, if\n\nObservations in the sample are independent. Two rules of thumb to check this:\n\ncompletely random sampling\nif sampling without replacement, sample should be less than 10% of the population size\n\n\nand\n\nThe sample is large enough. The required size varies in different contexts, but some good rules of thumb are:\n\nif the population itself is normal, sample size does not matter.\nif numerical require, >30 observations\nif binary outcome, at least 10 successes and 10 failures.\n\n\nthen\n\\[\n\\bar{x} \\sim N(\\mu, \\sigma / \\sqrt{n})\n\\]\ni.e. \\(\\bar{x}\\) is normally distributed (unimodal and symmetric with bell shape) with mean \\(\\mu\\) and standard deviation \\(\\sigma / \\sqrt{n}\\). The standard deviation of the sampling distribution is called the standard error.\n\n\n\n\n\n\nNote\n\n\n\nThe standard deviation of the sample mean depends on the number of samples, \\(n\\)."
  },
  {
    "objectID": "ae/ae-19.html#practice-using-clt-normal-distribution",
    "href": "ae/ae-19.html#practice-using-clt-normal-distribution",
    "title": "Central limit theorem",
    "section": "Practice using CLT & Normal distribution",
    "text": "Practice using CLT & Normal distribution\nSuppose the bone density for 65-year-old women is normally distributed with mean \\(809 mg/cm^3\\) and standard deviation of \\(140 mg/cm^3\\).\nLet \\(x\\) be the bone density of 65-year-old women. We can write this distribution of \\(x\\) in mathematical notation as\n\\[x \\sim N(809, 140)\\]"
  },
  {
    "objectID": "ae/ae-19.html#visualize-the-population-distribution",
    "href": "ae/ae-19.html#visualize-the-population-distribution",
    "title": "Central limit theorem",
    "section": "Visualize the population distribution",
    "text": "Visualize the population distribution\n\nggplot(data = data.frame(x = c(809 - 140*3, 809 + 140*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 809, sd = 140),\n                color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 809, sd = 140/sqrt(10)),\n                color = \"red\",lty = 2) + theme_bw() +\n  labs(title = \"Black solid line = population dist., Red dotted line = sampling dist.\")\n\n\n\n\n\nExercise 1\nBefore typing any code, based on what you know about the normal distribution, what do you expect the median bone density to be?\nWhat bone densities correspond to \\(Q_1\\) (25th percentile), \\(Q_2\\) (50th percentile), and \\(Q_3\\) (the 75th percentile) of this distribution? Use the qnorm() function to calculate these values.\n\n\nExercise 2\nThe densities of three woods are below:\n\nPlywood: 540 mg/cubic centimeter\nPine: 600 mg/cubic centimeter\nMahogany: 710 mg/cubic centimeter\nWhat is the probability that a randomly selected 65-year-old woman has bones less dense than Pine?\nWould you be surprised if a randomly selected 65-year-old woman had bone density less than Mahogany? What if she had bone density less than Plywood? Use the respective probabilities to support your response.\n\n\n\nExercise 3\nSuppose you want to analyze the mean bone density for a group of 10 randomly selected 65-year-old women.\n\nAre the conditions for the Central Limit Theorem met?\n\nIndependence?\nSample size/distribution?\n\nWhat is the shape, center, and spread of the distribution of \\(\\bar{x}\\), the mean bone density for a group of 10 randomly selected 65-year-old women?\nWrite the distribution of \\(\\bar{x}\\) using mathematical notation.\n\n\n\nExercise 4\n\nWhat is the probability that the mean bone density for the group of 10 randomly-selected 65-year-old women is less dense than Pine?\nWould you be surprised if a group of 10 randomly-selected 65-year old women had a mean bone density less than Mahogany? What the group had a mean bone density less than Plywood? Use the respective probabilities to support your response.\n\n\n\nExercise 5\nExplain how your answers differ in Exercises 3 and 5."
  },
  {
    "objectID": "ae/ae-19.html#extra-practice-on-your-own",
    "href": "ae/ae-19.html#extra-practice-on-your-own",
    "title": "Central limit theorem",
    "section": "Extra practice (on your own)",
    "text": "Extra practice (on your own)\nSuppose the distribution of the number of minutes users engage with apps on an iPad has a mean of 8.2 minutes and standard deviation of 1 minute. Let \\(x\\) be the number of minutes users engage with apps on an iPad, \\(\\mu\\) be the population mean and \\(\\sigma\\) the population standard deviation. Then,\n\\[x \\sim N(8.2, 1)\\]\nSuppose you take a sample of 60 randomly selected app users and calculate the mean number of minutes they engage with apps on an iPad, \\(\\bar{x}\\). The conditions (independence & sample size/distribution) to apply the Central Limit Theorem are met. Then by the Central Limit Theorem\n\\[\\bar{x} \\sim N(8.2, 1/\\sqrt{60})\\]\n\nWhat is the probability a randomly selected user engages with iPad apps for more than 8.3 minutes? Use pnorm for calculations.\n\n#add code\n\nWhat is the probability the mean minutes of app engagement for a group of 60 randomly selected iPad users is more than 8.3 minutes? Use pnorm for calculations.\n\n#add code\n\nWhat is the probability the mean minutes of app engagement for a group of 60 randomly selected iPad users is between 8.3 and 8.4 minutes? Use pnorm for calculations.\n\n\n    #add code"
  },
  {
    "objectID": "ae/ae-20.html",
    "href": "ae/ae-20.html",
    "title": "Central limit theorem II",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nhomework 4 due Friday"
  },
  {
    "objectID": "ae/ae-20.html#getting-started",
    "href": "ae/ae-20.html#getting-started",
    "title": "Central limit theorem II",
    "section": "Getting started",
    "text": "Getting started\nClone your ae20-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-20.html#today",
    "href": "ae/ae-20.html#today",
    "title": "Central limit theorem II",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nUse CLT to construct confidence intervals"
  },
  {
    "objectID": "ae/ae-20.html#load-packages",
    "href": "ae/ae-20.html#load-packages",
    "title": "Central limit theorem II",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-20.html#load-data-pokemon",
    "href": "ae/ae-20.html#load-data-pokemon",
    "title": "Central limit theorem II",
    "section": "Load Data: Pokemon",
    "text": "Load Data: Pokemon\nWe will be using the pokemon data set, which contains information about 42 randomly selected Pokemon (from all generations). You may load in the data set with the following code:\n\npokemon = read_csv(\"https://sta101.github.io/static/appex/data/pokemon.csv\")\n\nIn this analysis, we will use CLT-based inference to draw conclusions about the mean height among all Pokemon species.\n\nExercise 1\nLet’s start by looking at the distribution of height_m, the typical height in meters for a Pokemon species, using a visualization and summary statistics.\n\nggplot(data = pokemon, aes(x = height_m)) +\n  geom_histogram(binwidth = 0.25, fill = \"steelblue\", color = \"black\") + \n  labs(x = \"Height (in meters)\", \n       y = \"Distributon of Pokemon heights\")\n\n\n\n\n\npokemon %>%\n  summarise(mean_height = mean(height_m), \n            sd_height = sd(height_m), \n            n_pokemon = n())\n\n# A tibble: 1 × 3\n  mean_height sd_height n_pokemon\n        <dbl>     <dbl>     <int>\n1       0.929     0.497        42\n\n\nIn the previous lecture we were given the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), of the population. That is unrealistic in practice (if we knew \\(\\mu\\) and \\(\\sigma\\), we wouldn’t need to do statistical inference!).\nToday we will start on using the Central Limit Theorem to draw conclusions about the \\(\\mu\\), the mean height in the population of Pokemon.\n\nWhat is the point estimate for \\(\\mu\\), i.e., the “best guess” for the mean height of all Pokemon?\nWhat is the point estimate for \\(\\sigma\\), i.e., the “best guess” for the standard deviation of the distribution of Pokemon heights?\n\n\n\nExercise 2\nBefore moving forward, let’s check the conditions required to apply the Central Limit Theorem. Are the following conditions met:\n\nIndependence?\nSample size/distribution?\n\n\n\nCentral limit theorem\nRemember, when the independence and sample size assumptions are met, the central limit theorem states\n\\[\n\\bar{x} \\sim N(\\mu, \\sigma / \\sqrt{n})\n\\]\nIf we know \\(\\sigma\\), we can construct a symmetric confidence interval for the true mean easily using qnorm().\nFor example, if the true standard deviation in pokemon height is 0.4 meters, then to construct a 95% confidence interval:\n\nxbar = pokemon %>%\n  summarize(xbar = mean(height_m)) %>%\n  pull(xbar)\n\nqnorm(c(0.025, 0.975), mean = xbar, sd = 0.4)\n\n[1] 0.1445858 1.7125570\n\n\nThis can be equivalently expressed\n\nzscore = qnorm(0.025)\nxbar + zscore*0.4\n\n[1] 0.1445858\n\nxbar - zscore*0.4\n\n[1] 1.712557\n\n\nwhere we use the fact that we can write any normal distribution as a linear combination of a standard normal. For example,\nif \\(X \\sim N(0.928, .4)\\), then \\(X = .4Z + 0.928\\) where \\(Z\\) is standard normal, in other words \\(Z \\sim N(0, 1)\\).\nIn general, the confidence interval can be written as\n\\[\n\\bar{x} \\pm z^* \\times \\sigma\n\\]\nwhere \\(z^*\\) is the quantile of a standard normal distribution associated with our level of confidence.\nWhat about when we don’t know \\(\\sigma\\)?\n\n\nPractical confidence intervals\nWe don’t know the true population mean \\(\\mu\\) and standard deviation \\(\\sigma\\), how do we use CLT to construct a confidence interval?\nWe approximate \\(\\mu\\) by \\(\\bar{x}\\) and \\(\\sigma\\) by the same standard deviation \\(s\\). However \\(s\\) may be smaller than \\(\\sigma\\) and our confidence interval could be too narrow, for example, run the code below to compute the standard deviation of three draws from a standard normal.\n\nset.seed(6)\nsamples = rnorm(3, mean = 0, sd = 1)\nsd(samples)\n\n[1] 0.7543284\n\n\nThis was just for 1 random seed. If you remove the seed and repeat the simulation, you will find that \\(s\\) is sometimes above and sometimes below the true standard deviation.\nTo account for this uncertainty, we will use a distribution with thicker tails. This sampling distribution is called a t-distribution.\n\nggplot(data = data.frame(x = c(0 - 1*3, 0 + 1*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                color = \"black\") +\n  stat_function(fun = dt, args = list(df = 3),\n                color = \"red\",lty = 2) + theme_bw() +\n  labs(title = \"Black solid line = normal, Red dotted line = t-distribution\")\n\n\n\n\nThe t-distribution has a bell shape but the extra thick tails help us correct for the variability introduced by using \\(s\\) instead of \\(\\sigma\\).\nThe t-distribution, like the standard normal, is always centered at zero. Therefore, the t-distribution has only a single parameter: degrees of freedom. The degrees of freedom describes the precise form of the bell-shaped t-distribution. In general, we’ll use a t-distribution with \\(df=n−1\\) to model the sample mean when the sample size is \\(n\\).\nWe can use qt and pt to find quantiles and probabilities respectively under the t-distribution.\n\n\nConfidence interval\nTo construct our practical confidence interval (where we don’t know \\(\\sigma\\)) we use the t-distribution:\n\\[\n\\bar{x} \\pm t^*_{n-1} \\times \\frac{s}{\\sqrt{n}}\n\\]\n\nExercise 3\n\nCalculate the 95% confidence interval for pokemon height using the t-distribution.\n\n\n# code here\n\nHow does this compare to a 95% bootstrap confidence interval?\n\n# code here"
  },
  {
    "objectID": "labs/lab-7.html#exercise-1---load-and-visualize-the-datasest",
    "href": "labs/lab-7.html#exercise-1---load-and-visualize-the-datasest",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 1 - Load and visualize the datasest",
    "text": "Exercise 1 - Load and visualize the datasest\n\nLoad the dataset into an environment variable so that tab-autocomplete functions will work more smoothly inside RStudio (i.e., run chickwts <- chickwts). Then, use ggplot to make an appropriate visualization for the distribution of chick weights by feed. Make sure to give the plot an informative title and label the axes.\nBased on the distributions visualized in the plot above, does feed supplementation seem to have an effect on chick weight?\nAnother type of plot that is helpful for visualizing distributions of continuous variables (like weight) across levels of a categorical treatment (like feed) is called a violin plot. A violin plot represents each distribution with a smooth density estimate. Create a new plot to answer part (a) using geom_violin to see how this works."
  },
  {
    "objectID": "labs/lab-7.html#exercise-2---data-wrangling",
    "href": "labs/lab-7.html#exercise-2---data-wrangling",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 2 - Data wrangling",
    "text": "Exercise 2 - Data wrangling\n\nCompute and report the number of chicks in each feed group. We should find this matches our output from part b, below.\nWe plan to compare the weight of the chicks from each feed group. This will be more convenient if, rather than a two column dataframe, we have a list of vectors, each of which contains the weight of chicks in a specific feed group. A convenient way to do this is by using the split function, available in base R. Run the following code to create a list of vectors, each of which contains observations from a specific feed group weight_vecs <- split(chickwts$weight, chickwts$feed). Then, run sapply(weight_vecs, length) to print the length of each vector to the console.\n\n\n\n\n\n\n\nImportant\n\n\n\nNote for later exercises: Now, you should be able to access the vector of weights for chicks in a specific feed group by running, for example, weight_vecs[[\"linseed\"]] or, equivalently, weight_vecs$linseed."
  },
  {
    "objectID": "labs/lab-7.html#exercise-3---single-comparison",
    "href": "labs/lab-7.html#exercise-3---single-comparison",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 3 - Single comparison",
    "text": "Exercise 3 - Single comparison\nIs the average weight of chicks that eat sunflower feed different from the average weight of chicks that eat meatmeal feed? To answer this, you will setup a hypothesis test.\n\nState the null and alternative hypothesis in formal mathematical notation, letting \\(\\mu_s\\) be the true mean weight of sunflower fed chicks and \\(\\mu_m\\) the true mean weight of meatmeal fed chicks.\nCompute and report the observed statistic.\nTo quantify the evidence against the null hypothesis, we estimate the probability that, if the null hypothesis were true, we would have observed a difference in means as large or larger than what we observed in part b. This “tail probability” is called the p-value associated with the difference in means we computed.\n\n\nSimulate under the null distribution using reps = 5000 and set.seed(3). Save your simulation as null_dist. Next, compute the p-value and state your conclusion."
  },
  {
    "objectID": "labs/lab-7.html#exercise-4---evaluating-evidence-and-multiple-testing",
    "href": "labs/lab-7.html#exercise-4---evaluating-evidence-and-multiple-testing",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 4 - Evaluating evidence and multiple testing",
    "text": "Exercise 4 - Evaluating evidence and multiple testing\nWe rarely want to “reject” the null hypothesis when it is in fact true - that is called a “false positive” or type 1 error. In this case, we don’t want to claim sunflower feed leads to higher expected chick weight if in fact it does not. So we want to set a bar high enough that we should rarely incur type 1 error.\nAn experimenter will often decide in advance what is an acceptable false positive rate. In many situations, by convention, the threshold of requiring p-values below 0.05 is adopted.\n\nSuppose a scientist repeats the experiment 100 times in a row. If the true difference in weight between group is in fact 0, in roughly how many of the experiments would the scientist expect to find a p-value of less than 0.05?\n\nThe phenomenon described in part (a) is called p-hacking. The idea is that, by design, a “significance threshold” of level \\(\\alpha\\) will mean that, even if we only conduct experiments for which the null hypothesis is always true, we will expect to reject the null with probability \\(\\alpha\\). Thus, if you simply conduct enough experiments then eventually you will have “significant” findings even if the true effect is nonexistent.\n\nSuppose we would like to compare all pairs of feed groups to test for differences in expected weight. There are 6 types of feed, so there are \\({6 \\choose{2}} = 6*5/2 = 15\\) pairs we could compare. If we reject the null hypothesis for any p-value less than or equal to 0.05, and we assume the p-values are independent for each pair, and we also assume the null is true for every comparison, what is the probability that we don’t reject the null for any of the pairs?\n\n\n\n\n\n\n\nHint\n\n\n\nP(A, B) = P(A)P(B) when events A and B are independent.\n\n\n\nBased on your answer in part b, if we compared all 15 pairs of feeds for differences in expected weight, assuming the p-values are independent for each pair, what is the probability that we will make a type 1 error (i.e., the probability that we do reject the null for at least one pair)?\n\n\n\n\n\n\n\nHint\n\n\n\nP(\\(A\\)) + P(\\(A^C\\)) = 1 where \\(A^C\\) denotes “not \\(A\\)”. In other words, probabilities of complementary events add to 1."
  },
  {
    "objectID": "labs/lab-7.html#exercise-5---bonferroni-correction",
    "href": "labs/lab-7.html#exercise-5---bonferroni-correction",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 5 - Bonferroni Correction",
    "text": "Exercise 5 - Bonferroni Correction\nIf the probability of event \\(A\\) is \\(p_1\\) and the probability of event \\(B\\) is \\(p_2\\), the largest possible probability for the event of either \\(A\\) or \\(B\\) happening is \\(p_1 + p_2\\). Hence, in the worst possible case, if we perform a series of tests \\(T_1, \\dots, T_k\\) calibrated to have type 1 error probabilities \\(p_1, \\dots, p_k\\), the largest possible probability that at least one of the tests results in a type 1 error is \\(p_1 + \\dots + p_k\\). If all of the tests have a common type 1 error probability \\(p'\\), then this value will equal \\(kp'\\).\n\nIf we were to perform all pairwise tests for the chickwts dataset, what p-value threshold must we use if we want to ensure the probability that any of the tests leads to a type 1 error is less than 0.05?\n\nThis “higher bar” for p-values in multiple testing situations is called the Bonferroni correction.\n\nI will spare you from performing all possible pairwise comparisons. However, you can see from the plots in exercise 1 that the difference in sample means between the sunflower and horsebean groups will likely be the greatest of any pair in the dataset. If we began our experiment with no particular hypothesis and choose to formally compare these two groups based on a visualization, we are implicitly performing multiple testing by pre-screening out all pairs which do not have an obvious visual difference, thus we should impose a higher bar on the evidence for rejecting our null if we wish to maintain a low type 1 error rate overall. Is that pair significantly different if we use the Bonferroni-corrected threshold from part (a) above? Hint: feel free to re-use the example code from exercise 3(c), with the necessary modifications."
  },
  {
    "objectID": "labs/lab-7.html#submission",
    "href": "labs/lab-7.html#submission",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to render and push changes.\nYou must turn in a PDF file to the Gradescope page by the submission deadline to be considered “on time”.\nMake sure your data are tidy! That is, your code should not be running off the pages and spaced properly. See: https://style.tidyverse.org/ggplot2.html.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials \\(\\rightarrow\\) Duke NetID and log in using your NetID credentials.\nClick on your STA 199 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark all the pages associated with exercise. All the pages of your lab should be associated with at least one question (i.e., should be “checked”). If you do not do this, you will be subject to lose points on the assignment.\nSelect all pages of your .pdf submission to be associated with the “Workflow & formatting” question."
  },
  {
    "objectID": "labs/lab-7.html#exercise-1",
    "href": "labs/lab-7.html#exercise-1",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 1",
    "text": "Exercise 1\nWhich of the following affect the p-value and why?\n\nNull hypothesis\nAlternative hypothesis\nObserved statistic\nSignificance level (\\(\\alpha\\))"
  },
  {
    "objectID": "labs/lab-7.html#exercise-2",
    "href": "labs/lab-7.html#exercise-2",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 2",
    "text": "Exercise 2\nYou are interested in testing whether or not average chick weight, regardless of the feed the chick eats, is different from 200 grams. What is wrong with the following null hypothesis?\n\\[\nH_0: \\bar{x} = 200\n\\]"
  },
  {
    "objectID": "labs/lab-7.html#exercise-3---load-and-visualize-the-datasest",
    "href": "labs/lab-7.html#exercise-3---load-and-visualize-the-datasest",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 3 - Load and visualize the datasest",
    "text": "Exercise 3 - Load and visualize the datasest\n\nLoad the data set into an environment variable so that tab-autocomplete functions will work more smoothly inside RStudio (i.e., run chickwts <- chickwts). Then, use ggplot to make an appropriate visualization for the distribution of chick weights by feed. Make sure to give the plot an informative title and label the axes.\nBased on the distributions visualized in the plot above, does feed supplementation seem to have an effect on chick weight?\nAnother type of plot that is helpful for visualizing distributions of continuous variables (like weight) across levels of a categorical treatment (like feed) is called a violin plot. A violin plot represents each distribution with a smooth density estimate. Create a new plot to answer part (a) using geom_violin to see how this works."
  },
  {
    "objectID": "labs/lab-7.html#exercise-4---single-comparison",
    "href": "labs/lab-7.html#exercise-4---single-comparison",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 4 - Single comparison",
    "text": "Exercise 4 - Single comparison\nIs the average weight of chicks that eat sunflower feed different from the average weight of chicks that eat meatmeal feed? To answer this, you will setup a hypothesis test.\n\nState the null and alternative hypothesis in formal mathematical notation, letting \\(\\mu_s\\) be the true mean weight of sunflower fed chicks and \\(\\mu_m\\) the true mean weight of meatmeal fed chicks.\nCompute and report the observed statistic.\nTo quantify the evidence against the null hypothesis, we estimate the probability that, if the null hypothesis were true, we would have observed a difference in means as large or larger than what we observed in part b. This “tail probability” is called the p-value associated with the difference in means we computed.\n\n\nSimulate under the null distribution using reps = 5000 and set.seed(3). Save your simulation as null_dist. Next, compute the p-value and state your conclusion."
  },
  {
    "objectID": "labs/lab-7.html#exercise-5---evaluating-evidence-and-multiple-testing",
    "href": "labs/lab-7.html#exercise-5---evaluating-evidence-and-multiple-testing",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 5 - Evaluating evidence and multiple testing",
    "text": "Exercise 5 - Evaluating evidence and multiple testing\nWe rarely want to “reject” the null hypothesis when it is in fact true - that is called a “false positive” or type 1 error. In this case, we don’t want to claim sunflower feed leads to higher expected chick weight if in fact it does not. So we want to set a bar high enough that we should rarely incur type 1 error.\nAn experimenter will often decide in advance what is an acceptable false positive rate. In many situations, by convention, the threshold of requiring p-values below 0.05 is adopted.\n\nSuppose a scientist repeats the experiment 100 times in a row. If the true difference in weight between group is in fact 0, in roughly how many of the experiments would the scientist expect to find a p-value of less than 0.05?\n\nThe phenomenon described in part (a) is called p-hacking. The idea is that, by design, a “significance threshold” of level \\(\\alpha\\) will mean that, even if we only conduct experiments for which the null hypothesis is always true, we will expect to reject the null with probability \\(\\alpha\\). Thus, if you simply conduct enough experiments then eventually you will have “significant” findings even if the true effect is nonexistent.\n\nSuppose we would like to compare all pairs of feed groups to test for differences in expected weight. There are 6 types of feed, so there are \\({6 \\choose{2}} = 6*5/2 = 15\\) pairs we could compare. If we reject the null hypothesis for any p-value less than or equal to 0.05, and we assume the p-values are independent for each pair, and we also assume the null is true for every comparison, what is the probability that we don’t reject the null for any of the pairs?\n\n\n\n\n\n\n\nHint\n\n\n\nP(A, B) = P(A)P(B) when events A and B are independent.\n\n\n\nBased on your answer in part b, if we compared all 15 pairs of feeds for differences in expected weight, assuming the p-values are independent for each pair, what is the probability that we will make a type 1 error (i.e., the probability that we do reject the null for at least one pair)?\n\n\n\n\n\n\n\nHint\n\n\n\nP(\\(A\\)) + P(\\(A^C\\)) = 1 where \\(A^C\\) denotes “not \\(A\\)”. In other words, probabilities of complementary events add to 1."
  },
  {
    "objectID": "labs/lab-7.html#exercise-6---bonferroni-correction",
    "href": "labs/lab-7.html#exercise-6---bonferroni-correction",
    "title": "Lab 7 - Confidence Intervals and Hypothesis Testing",
    "section": "Exercise 6 - Bonferroni Correction",
    "text": "Exercise 6 - Bonferroni Correction\nIf the probability of event \\(A\\) is \\(p_1\\) and the probability of event \\(B\\) is \\(p_2\\), the largest possible probability for the either the event \\(A\\) or \\(B\\) occuring is \\(p_1 + p_2\\). Hence, in the worst possible case, if we perform a series of tests \\(T_1, \\dots, T_k\\) calibrated to have type 1 error probabilities \\(p_1, \\dots, p_k\\), the largest possible probability that at least one of the tests results in a type 1 error is \\(p_1 + \\dots + p_k\\).\nIf all of the tests have a common type 1 error probability \\(p^*\\), then this value will equal \\(kp^*\\).\n\nIf we were to perform all pairwise tests for the chickwts data set, what p-value threshold must we use if we want to guarantee that the probability of one or more type 1 errors is less than 0.05?\n\nThis “higher bar” for p-values in multiple testing situations is called the Bonferroni correction.\n\nI will spare you from performing all possible pairwise comparisons. However, you can see from the plots in exercise 3 that the difference in sample means between the sunflower and horsebean groups will likely be the greatest of any pair in the data set. If we began our experiment with no particular hypothesis and choose to formally compare these two groups based on a visualization, we are implicitly performing multiple testing by pre-screening out all pairs which do not have an obvious visual difference, thus we should impose a higher bar on the evidence for rejecting our null if we wish to maintain a low type 1 error rate overall. Is that pair significantly different if we use the Bonferroni-corrected threshold from part (a) above?\n\n\n\n\n\n\n\nHint\n\n\n\nRe-use your code from exercise 4(c) with the necessary modifications."
  },
  {
    "objectID": "ae/ae-21.html",
    "href": "ae/ae-21.html",
    "title": "Intro to hypothesis tests",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nhomework 4 due today\nproject draft report due April 7th to your GitHub repo."
  },
  {
    "objectID": "ae/ae-21.html#getting-started",
    "href": "ae/ae-21.html#getting-started",
    "title": "Intro to hypothesis tests",
    "section": "Getting started",
    "text": "Getting started\nClone your ae21-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-21.html#today",
    "href": "ae/ae-21.html#today",
    "title": "Intro to hypothesis tests",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nBe familiar with the terms “p-value”, “null-distribution”, “null hypothesis”, “alternative hypothesis”\nCompute a p-value"
  },
  {
    "objectID": "ae/ae-21.html#load-packages",
    "href": "ae/ae-21.html#load-packages",
    "title": "Intro to hypothesis tests",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-21.html#load-data-pokemon",
    "href": "ae/ae-21.html#load-data-pokemon",
    "title": "Intro to hypothesis tests",
    "section": "Load Data: Pokemon",
    "text": "Load Data: Pokemon\nWe will be using the pokemon data set, which contains information about 42 randomly selected Pokemon (from all generations). You may load in the data set with the following code:\n\npokemon = read_csv(\"https://sta101.github.io/static/appex/data/pokemon.csv\")\n\nIn this analysis, we will use CLT-based inference to draw conclusions about the mean height among all Pokemon species.\n\nExercise 1\nLet’s start by looking at the distribution of height_m, the typical height in meters for a Pokemon species, using a visualization and summary statistics.\n\nggplot(data = pokemon, aes(x = height_m)) +\n  geom_histogram(binwidth = 0.25, fill = \"steelblue\", color = \"black\") + \n  labs(x = \"Height (in meters)\", \n       y = \"Distributon of Pokemon heights\")\n\n\n\n\n\npokemon %>%\n  summarise(mean_height = mean(height_m), \n            sd_height = sd(height_m), \n            n_pokemon = n())\n\n# A tibble: 1 × 3\n  mean_height sd_height n_pokemon\n        <dbl>     <dbl>     <int>\n1       0.929     0.497        42\n\n\nIn the previous lecture we were given the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), of the population. That is unrealistic in practice (if we knew \\(\\mu\\) and \\(\\sigma\\), we wouldn’t need to do statistical inference!).\nToday we will start on using the Central Limit Theorem to draw conclusions about the \\(\\mu\\), the mean height in the population of Pokemon.\n\nWhat is the point estimate for \\(\\mu\\), i.e., the “best guess” for the mean height of all Pokemon?\nWhat is the point estimate for \\(\\sigma\\), i.e., the “best guess” for the standard deviation of the distribution of Pokemon heights?\n\n\n\nExercise 2\nBefore moving forward, let’s check the conditions required to apply the Central Limit Theorem. Are the following conditions met:\n\nIndependence?\nSample size/distribution?\n\n\n\nCentral limit theorem\nRemember, when the independence and sample size assumptions are met, the central limit theorem states\n\\[\n\\bar{x} \\sim N(\\mu, \\sigma / \\sqrt{n})\n\\]\nIf we know \\(\\sigma\\), we can construct a symmetric confidence interval for the true mean easily using qnorm().\nFor example, if the true standard deviation in pokemon height is 0.4 meters, then to construct a 95% confidence interval:\n\nxbar = pokemon %>%\n  summarize(xbar = mean(height_m)) %>%\n  pull(xbar)\n\nqnorm(c(0.025, 0.975), mean = xbar, sd = 0.4)\n\n[1] 0.1445858 1.7125570\n\n\nThis can be equivalently expressed\n\nzscore = qnorm(0.025)\nxbar + zscore*0.4\n\n[1] 0.1445858\n\nxbar - zscore*0.4\n\n[1] 1.712557\n\n\nwhere we use the fact that we can write any normal distribution as a linear combination of a standard normal. For example,\nif \\(X \\sim N(0.928, .4)\\), then \\(X = .4Z + 0.928\\) where \\(Z\\) is standard normal, in other words \\(Z \\sim N(0, 1)\\).\nIn general, the confidence interval can be written as\n\\[\n\\bar{x} \\pm z^* \\times \\sigma\n\\]\nwhere \\(z^*\\) is the quantile of a standard normal distribution associated with our level of confidence.\nWhat about when we don’t know \\(\\sigma\\)?\n\n\nPractical confidence intervals\nWe don’t know the true population mean \\(\\mu\\) and standard deviation \\(\\sigma\\), how do we use CLT to construct a confidence interval?\nWe approximate \\(\\mu\\) by \\(\\bar{x}\\) and \\(\\sigma\\) by the same standard deviation \\(s\\). However \\(s\\) may be smaller than \\(\\sigma\\) and our confidence interval could be too narrow, for example, run the code below to compute the standard deviation of three draws from a standard normal.\n\nset.seed(6)\nsamples = rnorm(3, mean = 0, sd = 1)\nsd(samples)\n\n[1] 0.7543284\n\n\nThis was just for 1 random seed. If you remove the seed and repeat the simulation, you will find that \\(s\\) is sometimes above and sometimes below the true standard deviation.\nTo account for this uncertainty, we will use a distribution with thicker tails. This sampling distribution is called a t-distribution.\n\nggplot(data = data.frame(x = c(0 - 1*3, 0 + 1*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                color = \"black\") +\n  stat_function(fun = dt, args = list(df = 3),\n                color = \"red\",lty = 2) + theme_bw() +\n  labs(title = \"Black solid line = normal, Red dotted line = t-distribution\")\n\n\n\n\nThe t-distribution has a bell shape but the extra thick tails help us correct for the variability introduced by using \\(s\\) instead of \\(\\sigma\\).\nThe t-distribution, like the standard normal, is always centered at zero. Therefore, the t-distribution has only a single parameter: degrees of freedom. The degrees of freedom describes the precise form of the bell-shaped t-distribution. In general, we’ll use a t-distribution with \\(df=n−1\\) to model the sample mean when the sample size is \\(n\\).\nWe can use qt and pt to find quantiles and probabilities respectively under the t-distribution.\n\n\nConfidence interval\nTo construct our practical confidence interval (where we don’t know \\(\\sigma\\)) we use the t-distribution:\n\\[\n\\bar{x} \\pm t^*_{n-1} \\times \\frac{s}{\\sqrt{n}}\n\\]\n\nExercise 3\n\nCalculate the 95% confidence interval for pokemon height using the t-distribution.\n\n\n# code here\n\nHow does this compare to a 95% bootstrap confidence interval?\n\n# code here"
  },
  {
    "objectID": "ae/ae-21.html#introduction-to-hypothesis-testing",
    "href": "ae/ae-21.html#introduction-to-hypothesis-testing",
    "title": "Intro to hypothesis tests",
    "section": "Introduction to hypothesis testing",
    "text": "Introduction to hypothesis testing\n\nIs this a fair coin?\nWe’ll record 1 if the coin is “Heads” and 0 if the coin lands “Tails”.\n\n# coin_flips = c()\n\nIf the coin is fair, what is the probability of seeing the outcome we saw? To answer this question we’ll setup a statistical model:\n\\[\n\\text{\\# heads} \\sim Binomial(n, p)\n\\]\nwhere \\(p\\) is the probability of a heads and \\(n\\) is the total number of coin flips.\n\nExercise 2\n\nIf the coin is fair, what would \\(p\\) be?\nUsing R, flip a fair coin 6 times and count the number of heads. Next, repeat this experiment 1000 times and count the proportion of times you observe 6 heads.\n\n\n# code here\n\n\nWhat is the probability of observing 6 heads in 6 coin flips?"
  },
  {
    "objectID": "ae/ae-21.html#hypothesis-testing-framework",
    "href": "ae/ae-21.html#hypothesis-testing-framework",
    "title": "Intro to hypothesis tests",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\nYou may not have realized it but you just performed a hypothesis test!\nYou setup a null hypothesis: \\(H_0\\). The null hypothesis is a hypothesis you set up and then try and knock down. Conceptually, it’s the “nothing special is going on” hypothesis. Formally, the null hypothesis makes a claim or assumption about a population parameter.\nIn this case, it was the assumption that the coin is fair. Mathematically, we write this:\n\\[\nH_0: p = 0.5\n\\]\nConceptually, \\(p\\) is the probability of flipping a head if we flipped the coin infinitely many times.\nSince we computed the probability of observing all heads, we were fundamentally interested in if \\(p > 0.5\\). This is our alternative hypothesis, \\(H_A\\). In mathematical notation, we write\n\\[\nH_A: p > 0.5\n\\]\nNext, we simulated under the null. This means that we simulated what the coin flips would have looked like if the null was true. In this context, this means we simulated as if the coin was fair.\nFinally, we check to see where our actual observed data places under the null distribution. If it’s way out in the tail, we reject the null. If its not way out in the tail, we fail to reject the null.\nHow can we make “way out in the tail” more precise? Well, it’s arbitrary and context-dependent. In some contexts it is popular to use a cut-off of \\(0.05\\). This cutoff is called “the significance level” and is also known as \\(\\alpha\\).\n\nExercise 3\nAssume we continue flipping our coin for a total of 30 coin flips and observe 23 heads and 7 tails. What is the probability of seeing 23 or more heads if the coin is fair?\n\n# code here"
  },
  {
    "objectID": "ae/ae-21.html#p-values",
    "href": "ae/ae-21.html#p-values",
    "title": "Intro to hypothesis tests",
    "section": "p-values",
    "text": "p-values\nYou might not realize it, but you just computed a p-value… again!\nA p-value is a probability. It’s the tail probability associated with your alternative hypothesis.\nThe alternative hypothesis must always relate to the null. Here we had three options:\n\n\\(H_A: p < 0.5\\), the coin is biased to land tails\n\\(H_A: p > 0.5\\), the coin is biased to land heads\n\\(H_A: p \\neq 0.5\\), the coin is biased\n\nLet’s look offline at what each one would like here.\n\nExercise 4\nCompute the p-value associated with each of the alternative hypotheses above.\n\n# code here\n\nMake a conclusion based on a significance level of 0.05. In other words,\n\nif p < 0.05, reject the null.\nif p > 0.05, we fail to reject the null.\n\nAs you can see our conclusion depends on our alternative hypothesis. For this reason, it is important to set up an alternative hypothesis before looking at the data.\n\n\nRecap\nWe were interested in whether or not a coin was fair. We let \\(p\\) be the probability of landing heads. Fundamentally, we were interested in whether or not \\(p = 0.5\\). This was our null hypothesis:\n\\[\nH_0: p = .5\n\\]\nand our alternative, was that the coin was biased heads:\n\\[\nH_A: p > 0.5\n\\]\nIn one example our data consisted of 30 coin flips and 23 heads. The proportion of heads that we observed, \\(\\hat{p} = 23/30 = .77\\).\nDo these 30 coin flips give us enough evidence to reject the null in favor of the alternative?\nTo answer this question, we computed the p-value: \\(Pr(\\hat{p} \\geq .77 | H_0 \\text{ true})\\). In words, the probability that our statistic of interest, (\\(\\hat{p}\\)), is greater than or equal to what we saw given that the null is true.\nNotice that the p-value is defined by three things:\n\nour observed statistic (0.77)\nthe null hypothesis (\\(H_0\\))\nthe alternative hypothesis (\\(H_A\\)), this tells us the direction (\\(>=\\)) to shade.\n\nWe compared the p-value to some pre-defined cutoff, \\(\\alpha\\). In our example we set our cutoff at \\(\\alpha = 0.05\\). If p-value \\(< \\alpha\\), we reject the null. If p-value \\(> \\alpha\\), we fail to reject the null.\n\n\nThe tidy way\n\ncoin_flips = data.frame(one_flip = sample(c(rep(\"H\",23), rep(\"T\", 7)), size = 30))\n\nglimpse(coin_flips)\n\nRows: 30\nColumns: 1\n$ one_flip <chr> \"H\", \"H\", \"H\", \"H\", \"T\", \"H\", \"T\", \"H\", \"H\", \"H\", \"T\", \"H\", \"…\n\n\n\nset.seed(2022)\nnull_dist = \n  coin_flips %>% \n  specify(response = one_flip, success = \"H\") %>%\n  hypothesize(null = \"point\", p = 0.5) %>%\n  generate(reps = 10000, type = \"draw\") %>%\n  calculate(stat = \"prop\")\n\nobs_stat = 23/30\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat, direction = \"right\")\n\n\n\nnull_dist %>%\nget_p_value(obs_stat, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1  0.0027\n\n\np-value of \\(0.0027 < 0.05\\). We reject the null hypothesis that the coin is fair."
  },
  {
    "objectID": "prepare/prep24.html",
    "href": "prepare/prep24.html",
    "title": "Prepare",
    "section": "",
    "text": "Read: chapter 16.1, 17.1"
  },
  {
    "objectID": "prepare/prep27.html",
    "href": "prepare/prep27.html",
    "title": "Prepare",
    "section": "",
    "text": "Data privacy\nMisrepresentation\nAlgorithmic bias"
  },
  {
    "objectID": "prepare/prep26.html",
    "href": "prepare/prep26.html",
    "title": "Prepare",
    "section": "",
    "text": "No prepare."
  },
  {
    "objectID": "prepare/prep25.html",
    "href": "prepare/prep25.html",
    "title": "Prepare",
    "section": "",
    "text": "Read: chapter 20"
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "HW 5: Sleep case study and beyond",
    "section": "",
    "text": "Important\n\n\n\nThis homework is due Thursday, April 13 at 5:00pm."
  },
  {
    "objectID": "hw/hw-5.html#getting-started",
    "href": "hw/hw-5.html#getting-started",
    "title": "HW 5: Sleep case study and beyond",
    "section": "Getting Started",
    "text": "Getting Started\n\nGo to the Github Organization page and open your hw5-username repo\nClone the repository, open a new project in RStudio. It contains the starter documents you need to complete the homework assignment."
  },
  {
    "objectID": "hw/hw-5.html#exercises",
    "href": "hw/hw-5.html#exercises",
    "title": "HW 5: Sleep case study and beyond",
    "section": "Exercises",
    "text": "Exercises\n\nInvestigating sleep\n\nFind the mean time it takes the participant to fall asleep. Check whether or not you can use central limit theorem to construct a 95% confidence interval to report with your estimate. For purposes of this exercise, you may consider samples of time to sleep as random. If you can use CLT, please do so to construct and report the confidence interval. Otherwise, construct a bootstrap confidence interval with set.seed(5) and 10000 reps. Interpret your interval in context.\nDoes the participant take more than 35 minutes on average to fall asleep? Perform a hypothesis test to investigate. What is the null? What is the alternative? Use set.seed(2) and reps=5000 to simulate under the null.\n\nVisualize the null distribution and shade the p-value. Be sure to label the x-axis. Make a conclusion based on significance level. Given your response from the previous exercise, does this result surprise you? Why or why not? What is the probability of rejecting the null if it’s actually true? What type of error is this (type 1 or type 2)?\n\nDo calcium-magnesium supplements reduce median time to fall asleep? Construct a hypothesis test to investigate. To begin, state the null and alternative hypothesis in words and in statistical notation. Use simulation-based inference to generate 5000 samples from the null distribution with set.seed(5). Compute and state your observed statistic. Finally, compute the p-value and report it as well as significance at the alpha = 0.05 level and state your conclusion in context.\n\n\n\n\n\n\n\nHint\n\n\n\nYou will need to mutate to ensure that one of the variables is non-numeric.\n\n\n\nThe more time the individual spent lying in bed, awake, and trying to sleep, the worse their overall quality of sleep. To begin investigating quality of sleep, create a new column timeAwake that shows the total number of minutes the individual spent in bed but not asleep. When timeAwake is large, sleep is awful and interrupted. Save your data frame with the new column as sleep2.\n\nExample: TTS is the time to fall asleep, TBT is total bed time, i.e. total time spent, lying in bed and trying to sleep.\nSo if the participant gets into bed at 9:00pm and then falls asleep at 9:30pm and wakes up at 1:00am and lies awake for an hour only to finally sleep until 6:00am and gets out of bed then\n\nTTS is 30 minutes,\nTBT is 9 hours,\nTotal Sleep Time (TST) is 3.5h + 4h = 7.5 hours and the total time awake is 1.5 hours.\n\n\nHypothesis: Most nights, the participant spends over fifty minutes lying in bed awake. Construct a hypothesis test to investigate. To begin, state the null and alternative hypothesis in words and in statistical notation. Use simulation-based inference to generate 5000 samples from the null distribution with set.seed(8). Compute and state your observed statistic. Finally, compute the p-value and report it as well as significance at the alpha = 0.01 level and state your conclusion in context.\n\n\n\n\n\n\n\nHint\n\n\n\nStart with sleep2 from the previous exercise.\n\n\n\n\nBeyond this class: pick a package\n\nIn this exercise, you will find a new package that you haven’t used in class and learn how to use it. Being able to find new packages, install them, read the documentation and finally use the package is an essential skill beyond the classroom.\n\nTo begin,\n\nPick a package. You can choose one from the list below, or venture into the great unknown and find another online. If you have trouble getting a package to work, ask for help on slack or come to office hours.\nInstall the package. Be sure to do this in the Console, not in your quarto document because you do not want to keep re-installing every time you render the document.\n\nDepending on where the package comes from, how you install the package differs:\n\nIf the package is on CRAN (Comprehensive R Archive Network), you can install it with install.packages.\nIf the package is only on Github (most likely because it is still under development), you need to use the install_github function, click here for more details.\nLoad the package. Regardless of how you installed the package you can load it with the library function.\nDo something with the package. Typically, simpler is better. The goal is for you to read and understand the package documentation to carry out a simple task.\nFinally, write a short 3-4 sentence statement (at the beginning of your solution to this exercise) and include:\n\n\nThe name of the package you use and whether it is from CRAN or GitHub\nA short description of what the package does (in your own words)\nA short description of what you do with the package.\n\nSample list of packages on CRAN:\n\n\n\npackage name\ndescription\n\n\n\n\nape\nManipulate, plot and interact with phylogenetic trees and models. Comes with sample data\n\n\nastrodatR\nAstronomy datasets\n\n\ncowsay\nAllows printing of character strings as messages/warnings/etc. with ASCII animals, including cats, cows, frogs, chickens, ghosts, and more\n\n\nbabynames\nUS Baby Names 1880-2015\n\n\ndragracer\nThese are data sets for the hit TV show, RuPaul’s Drag Race. Data right now include episode-level data, contestant-level data, and episode-contestant-level data\n\n\ndatapasta\nRStudio addins and R functions that make copy-pasting vectors and tables to text painless\n\n\nDiagrammeR\nGraph/Network Visualization\n\n\njaneaustenr\nFull texts for Jane Austen’s 6 completed novels, ready for text analysis. These novels are “Sense and Sensibility”, “Pride and Prejudice”, “Mansfield Park”, “Emma”, “Northanger Abbey”, and “Persuasion”\n\n\nggimage\nSupports image files and graphic objects to be visualized in ‘ggplot2’ graphic system\n\n\ngganimate\nCreate easy animations with ggplot2\n\n\ngt\nEasily Create Presentation-Ready Display Tables\n\n\nleaflet\nCreate Interactive Web Maps with the JavaScript ‘Leaflet’ Library\n\n\npraise\nBuild friendly R packages that praise their users if they have done something good, or they just need it to feel better\n\n\nplotly\nCreate interactive web graphics from ggplot2 graphs and/or a custom interface to the JavaScript library plotly.js inspired by the grammar of graphics\n\n\nsuncalc\nR interface to suncalc.js library, part of the SunCalc.net project, for calculating sun position, sunlight phases (times for sunrise, sunset, dusk, etc.), moon position and lunar phase for the given location and time\n\n\nschrute\nThe complete scripts from the American version of the Office television show in tibble format\n\n\nstatebins\nThe cartogram heatmaps generated by the included methods are an alternative to choropleth maps for the United States and are based on work by the Washington Post graphics department in their report on “The states most threatened by trade”\n\n\nttbbeer\nAn R data package of beer statistics from U.S. Department of the Treasury, Alcohol and Tobacco Tax and Trade Bureau (TTB)\n\n\nttbbeer\nAn R data package of beer statistics from U.S. Department of the Treasury, Alcohol and Tobacco Tax and Trade Bureau (TTB)\n\n\nukbabynames\nFull listing of UK baby names occurring more than three times per year between 1996 and 2015, and rankings of baby name popularity by decade from 1904 to 1994\n\n\nwesanderson\nColor palettes from Wes Anderson films\n\n\nscatterplot3d\nCreate 3D plots\n\n\n\n\n\nAssessment\nA research team at Duke University is creating an intro data science assessment. They are hopeful to have students in an intro data science course take the assessment to better pin down how students think about and reason through data science assessment questions. If you are able and willing, please consider participating in this research.\nParticipation is completely optional, and has no impact on your academic standing in STA199.\nClick here to take the assessment"
  },
  {
    "objectID": "hw/hw-5.html#rubric",
    "href": "hw/hw-5.html#rubric",
    "title": "HW 5: Sleep case study and beyond",
    "section": "Rubric",
    "text": "Rubric\n\nEx 1: 5 pts.\nEx 2: 10 pts.\nEx 3: 8 pts.\nEx 4: 2 pts.\nEx 5: 8 pts.\nEx 6: 12 pts.\nWorkflow and formatting - 5 pts"
  },
  {
    "objectID": "hw/hw-5.html#submission",
    "href": "hw/hw-5.html#submission",
    "title": "HW 5: Sleep case study and beyond",
    "section": "Submission",
    "text": "Submission\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials Duke Net ID and log in using your Net ID credentials.\nClick on your STA 199 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark all the pages associated with exercise. All the pages of your homework should be associated with at least one question (i.e., should be “checked”). If you do not do this, you will be subject to lose points on the assignment.\nSelect all pages of your PDF submission to be associated with the “Workflow & formatting” question.\n\n\n\n\n\n\n\nNote\n\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes:\n\nlinking all pages appropriately on Gradescope\nputting your name in the YAML at the top of the document\ncommitting the submitted version of your .qmd to GitHub\nAre you under the 80 character code limit? (You shouldn’t have to scroll to see all your code).\nPipes %>%, |> and ggplot layers + should be followed by a new line\nYou should be consistent with stylistic choices, e.g. only use 1 of = vs <- and %>% vs |>\nAll binary operators should be surrounded by space. For example x + y is appropriate. x+y is not."
  },
  {
    "objectID": "ae/ae-22.html",
    "href": "ae/ae-22.html",
    "title": "Tidy hypotheses",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nhomework 5 released today\nproject draft report due April 7th to your GitHub repo."
  },
  {
    "objectID": "ae/ae-22.html#getting-started",
    "href": "ae/ae-22.html#getting-started",
    "title": "Tidy hypotheses",
    "section": "Getting started",
    "text": "Getting started\nClone your ae22-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-22.html#today",
    "href": "ae/ae-22.html#today",
    "title": "Tidy hypotheses",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nhypothesis test the tidy way\npractice more than testing proportions"
  },
  {
    "objectID": "ae/ae-22.html#load-packages",
    "href": "ae/ae-22.html#load-packages",
    "title": "Tidy hypotheses",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-22.html#notes",
    "href": "ae/ae-22.html#notes",
    "title": "Tidy hypotheses",
    "section": "Notes",
    "text": "Notes\n\nNot just a coin flip\n\nHere’s an example of testing a proportion outside the context of coins.\n\n\npush_pull = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/push_pull.csv\")\n\n\npush_pull %>%\n  slice(1:3, 24:26)\n\n# A tibble: 6 × 7\n  participant_id   age push1 push2 pull1 pull2 training\n           <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <chr>   \n1              1    41    41    45    16    17 density \n2              2    32    35    44     9    11 density \n3              3    44    33    38    10    11 density \n4             24    36    31    60     9    15 gtg     \n5             25    50    35    42     9    12 gtg     \n6             26    34    23    39     9    13 gtg     \n\n\nThe push_pull dataset comes from a “mini study” by mountain tactical institute.\n26 individuals completed 1 of 2 exercise regiments for 3.5 weeks to increase their pushups and pullups. Codebook below:\n\nparticipant_id: unique identifier for each participant\nage: age of participant\npush1/2: push-ups at beginning and end of program respectively\npull1/2: pull-ups at beginning and end of program respectively\ntraining: which training protocol the individual participated in\n\n\npush_pull = push_pull %>%\n  mutate(\n    pct_push_inc = (push2 / push1 ) - 1,\n    pct_pull_inc = (pull2 / pull1) - 1)\n\nHypothesis: “Most people who train consistently will see at least a 15% increase in push-ups over a 3.5 week training period.”\nBreaking it down:\n\n“Most” i.e. “greater than 50%” indicates we should examine a proportion.\n\n\nExercise 1\nWhat’s the null?\n\n“will see at least a 15% increase”. Each person either increases by 15% over 3.5 weeks or does not. This is our binary outcome.\ncreate a new column called over_15pct that tells you whether or not an individual achieved at least a 15% increase in push-ups\n\n\n# code here\n\n\nWhat would be a default theory (null hypothesis)?\n\n\n\nExercise 2\nWrite the null and alternative in mathematical notation.\n\n\nExercise 3\nWhat is the observed statistic? Compute and write it in mathematical notation.\n\n\nExercise 4\nNext, simulate under the null and compute the p-value. State your conclusion with \\(\\alpha = 0.05\\). As a bonus, visualize the null distribution and shade in the p-value.\n\n\n\nMore than a proportion\n\nWhat if we want to make a claim about a different population parameter than a proportion? Maybe a mean, or median? We can’t necessarily flip a coin. The answer, is once again, bootstrap sampling.\n\nHypothesis: “The mean age of push-up/pull-up training participants is greater than 30”.\nLet’s investigate this hypothesis with a significance level \\(\\alpha = 0.01\\).\n\nExercise 5\nWrite down the null and alternative hypotheses in words and mathematical notation\n\n\nExercise 6\nWhat is the observed statistic? Write it in mathematical notation.\nBootstrapping does the following…\n\n# find observed statistic\nobs_mean_age = push_pull %>%\n  drop_na(age) %>%\n  summarize(meanAge = mean(age)) %>%\n  pull()\n# subtract observed_mean - desired_mean from age\nage_and_null = push_pull %>%\n  select(age) %>%\n  drop_na(age) %>%\n  mutate(nullAge = age - (obs_mean_age - 30))\n# show data frame\nage_and_null\n\n# A tibble: 25 × 2\n     age nullAge\n   <dbl>   <dbl>\n 1    41    35.8\n 2    32    26.8\n 3    44    38.8\n 4    37    31.8\n 5    37    31.8\n 6    21    15.8\n 7    33    27.8\n 8    38    32.8\n 9    49    43.8\n10    33    27.8\n# … with 15 more rows\n\n# show the means of each column\nage_and_null %>%\n  summarize(meanAge = mean(age),\n  mean_nullAge = mean(nullAge))\n\n# A tibble: 1 × 2\n  meanAge mean_nullAge\n    <dbl>        <dbl>\n1    35.2           30\n\n\nIf we take bootstrap samples from this new nullAge column, we are sampling from data with the same variability as our original data, but a different mean. This is a nice way to explore the null!\n\nset.seed(3)\n\n# simulate null\nnull_dist = push_pull %>%\n  specify(response = age) %>%\n  hypothesize(null = \"point\", mu = 30) %>%\n  generate(reps = 10000, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n\n# get observed statistic\nobs_stat = obs_mean_age\n\np_value = null_dist %>%\n  get_p_value(obs_stat, direction = \"right\")\n\np_value\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1  0.0001\n\n\n\nThe p-value 1e-04 is less than \\(\\alpha = 0.01\\). I reject the null hypothesis. In context, there is evidence to suggest that average push/pull trainee age is older than 30 years old.\n\n\n\nExercise 7\nSay we are interested in the performance of trainees at this particular facility and the sample is representative of the population.\nHypothesis: The median number of pull-ups trainees can perform is less than 20 even after training for 3.5 weeks.\nWrite down the null and alternative hypothesis in mathematical notation.\n\n\nExercise 8\nWrite down the observed statistic. Simulate under the null and compute the p-value. Finally, visualize and interpret the p-value in context."
  },
  {
    "objectID": "ae/ae-22.html#summary",
    "href": "ae/ae-22.html#summary",
    "title": "Tidy hypotheses",
    "section": "Summary",
    "text": "Summary\n\nHypothesis testing procedure\n\nSpecify the null and alternative hypothesis. Choose or know \\(\\alpha\\).\nCollect/examine the data. Compute the observed statistic.\nSimulate under the null and compute the p-value using the observed statistic and the alternative hypothesis.\nCompare the p-value to your significance level \\(\\alpha\\) and reject or fail to reject the null. Interpret your result in context."
  },
  {
    "objectID": "ae/ae-22.html#which-training-method-is-better",
    "href": "ae/ae-22.html#which-training-method-is-better",
    "title": "Tidy hypotheses",
    "section": "Which training method is better?",
    "text": "Which training method is better?\nTwo exercise regimes:\n\n“density” training\n“grease-the-groove” (gtg)\n\nWe want to know, is the average pull-up percent increase of a gtg trainee significantly different than a density trainee?\nFundamentally, does the categorical variable training affect the average percentage increase in pull-ups?\nState the null hypothesis:\n\\[\n\\mu_d = \\mu_{gtg}\n\\]\n\\[\nH_0: \\mu_d - \\mu_{gtg} = 0\n\\]\nWhat we want to do to simulate data under this null:\n\nrandom_training = sample(push_pull$training, replace = FALSE)\n\npush_pull %>%\n  select(pct_pull_inc) %>%\n  mutate(random_training = random_training)\n\n\nExercise 9:\n\nComplete the hypothesis specification above by stating the alternative. Check the observed statistic reported below.\n\n\n# code here\n\nSimulating under the null and computing the p-value:\n\nsim_num = 10000\nset.seed(1)\n# simulate null\nnull_dist = push_pull %>%\n  specify(response = pct_pull_inc, explanatory = training) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = sim_num, type = \"permute\") %>%\n  calculate(stat = \"diff in means\", order = c(\"density\", \"gtg\"))\n# observed statistic\nobs_stat = .196 - .489\n# visualize / get p\nvisualize(null_dist) +\n  shade_p_value(obs_stat, direction = \"both\")\n\n\n\n\n\n\nExercise 10\nCompute the p-value and state your conclusion with \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "ae/ae-22.html#summary-of-generate-options",
    "href": "ae/ae-22.html#summary-of-generate-options",
    "title": "Tidy hypotheses",
    "section": "Summary of generate() options",
    "text": "Summary of generate() options\n\n1. draw\n\ndescription: flip a coin with probability p, or roll a die with probabilities associated with each side. See here for reference to the multinomial setting.\ntypical case: test the proportion of a binary outcome\nnull: proportion \\(p\\) is some fixed number\nExample (hypothesis test for a single proportion)\n\n\n\n2. bootstrap\n\ndescription: re-sample your data with replacement\ntypical case (in hypothesis testing): does the mean equal a specific value? Does the median equal a specific value?\nnull: what would the data have looked like if nothing but the point estimate changed?\n\n\n\n3. permute\n\ndescription: permutes variables\ntypical case: is there a difference in the outcome between groups?\nassociated null: group membership does not matter i.e. group A and group B have the same outcome\nExample (test for independence)"
  },
  {
    "objectID": "ae/ae-23.html",
    "href": "ae/ae-23.html",
    "title": "Hypothesis tests and confidence intervals",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nproject draft report due to GitHub today\nhomework 5 due next Thursday\nExam 2 next Friday"
  },
  {
    "objectID": "ae/ae-23.html#getting-started",
    "href": "ae/ae-23.html#getting-started",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Getting started",
    "text": "Getting started\nClone your ae23-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-23.html#today",
    "href": "ae/ae-23.html#today",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ndefine type I and type II error\ncompare hypothesis tests with confidence intervals\npractice conducting hypothesis tests"
  },
  {
    "objectID": "ae/ae-23.html#load-packages",
    "href": "ae/ae-23.html#load-packages",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "ae/ae-23.html#practice",
    "href": "ae/ae-23.html#practice",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Practice",
    "text": "Practice\n\nLoad data\nThe stent30 data set comes from the openintro package and is from a study conducted in 2011 on the effects of arterial stents as a therapy for stroke patients. See the original publication:\nChimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Med- ical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993- 1003. doi: 10.1056/NEJMoa1105335.\nor check ?stent30 for more information.\n\ndata(stent30)\n\n\nglimpse(stent30)\n\nRows: 451\nColumns: 2\n$ group   <fct> treatment, treatment, treatment, treatment, treatment, treatme…\n$ outcome <fct> stroke, stroke, stroke, stroke, stroke, stroke, stroke, stroke…\n\n\n\nExercise 1\nDo stents affect stroke outcome in patients?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context"
  },
  {
    "objectID": "ae/ae-23.html#confidence-intervals-and-hypothesis-tests",
    "href": "ae/ae-23.html#confidence-intervals-and-hypothesis-tests",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Confidence intervals and hypothesis tests",
    "text": "Confidence intervals and hypothesis tests\nHere we revisit the data from the first three seasons of NC Courage games (2017-2019).\n\ncourage = read_csv(\"https://sta101-fa22.netlify.app/static/labs/data/courage.csv\")\n\n\nglimpse(courage)\n\nRows: 78\nColumns: 10\n$ game_id     <chr> \"washington-spirit-vs-north-carolina-courage-2017-04-15\", …\n$ game_date   <chr> \"4/15/2017\", \"4/22/2017\", \"4/29/2017\", \"5/7/2017\", \"5/14/2…\n$ game_number <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ home_team   <chr> \"WAS\", \"NC\", \"NC\", \"BOS\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NC\", …\n$ away_team   <chr> \"NC\", \"POR\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NJ\", \"NC\", \"KC\", \"…\n$ opponent    <chr> \"WAS\", \"POR\", \"ORL\", \"BOS\", \"ORL\", \"CHI\", \"NJ\", \"CHI\", \"KC…\n$ home_pts    <dbl> 0, 1, 3, 0, 3, 1, 2, 3, 2, 3, 0, 0, 2, 1, 1, 0, 1, 2, 2, 2…\n$ away_pts    <dbl> 1, 0, 1, 1, 1, 3, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 2, 0, 3, 1…\n$ result      <chr> \"win\", \"win\", \"win\", \"win\", \"loss\", \"loss\", \"win\", \"loss\",…\n$ season      <dbl> 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017…\n\n\nDo National Women’s Soccer League (NWSL) teams have a home-field advantage? We’ll answer this question in a few separate ways.\nHypothesis testing framework: does NC Courage score a significantly different number of points (on average) away than at home?\n\nExercise 2\n\nCreate a new column location that tells you whether the courage are “home” or “away”\nCreate a new column pts that always reports the Courage points scored in a game.\nSave your result as a new data frame titled courage2.\n\n\n# code here\n\n\n\nExercise 3\nTo answer the question does NC Courage score a significantly different number of points (on average) away than at home?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context\n\n\n# code here\n\n\n\nExercise 4\n\nReport the mean difference between away and home games and report a 95% bootstrap confidence interval. Use set.seed(3) and reps=5000 Interpret your interval in context.\n\n\n# code here\n\n\n\nExercise 5\nIs there a better way we could investigate whether or not the Courage have a home-field advantage? Why?"
  },
  {
    "objectID": "ae/ae-23.html#notes",
    "href": "ae/ae-23.html#notes",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Notes",
    "text": "Notes\n\nType 1 and Type 2 Errors\n\n\n\nTruth\nReject the null\nFail to reject the null\n\n\n\n\n\\(H_0\\) is true\nType 1 error\n✔️\n\n\n\\(H_A\\) is true\n✔️\nType 2 error\n\n\n\nThe significance level, \\(\\alpha\\), is the probability of a type 1 error. In some contexts, a type 1 error may be referred to as a “false positive” and a type 2 error as a “false negative”.\nIntuitively, by considering extremes, one can see a trade-off exists between type 1 and type 2 error.\n\nIf \\(\\alpha = 0\\), then the p-value stands no chance of being smaller than \\(\\alpha\\) and we always fail to reject the null. This makes type 1 errors impossible.\n\nSimilarly, if \\(\\alpha = 1\\), then all p-values will be smaller than \\(\\alpha\\) and type 2 errors will become impossible, because we will always reject the null.\n\\(\\beta\\) is used to denote the probability of a type 2 error.\nThe power of a test is \\(1 - \\beta\\), which is the probability that your test rejects the null hypothesis when the null hypothesis is false."
  },
  {
    "objectID": "ae/ae-23.html#why-its-important-to-be-careful-with-interpretation",
    "href": "ae/ae-23.html#why-its-important-to-be-careful-with-interpretation",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Why it’s important to be careful with interpretation",
    "text": "Why it’s important to be careful with interpretation\n\n(And why hypothesis tests don’t tell the whole story)\nThe data for this example comes from Confounding and Simpson’s paradox1 by Julious and Mullee.\nThe data examines 901 individuals with diabetes and includes the following variables\n\ninsulin_dep: whether or not the patient has insulin dependent or non-insulin dependent diabetes\nage: whether or not the individual is less than 40 years old\nsurvival: whether or not the individual survived the length of the study\n\n\ndiabetes = read_csv(\"https://sta101.github.io/static/appex/data/diabetes.csv\")\n\nFlex Aisher thinks people with insulin dependent diabetes actually survive longer than those without insulin dependence. Flex wants to formally test his hypothesis.\nLet \\(p_{d}\\) be the probability of insulin dependent survival and \\(p_{i}\\) be the probability of insulin independent survival.\n\\[\nH_0: p_{d} - p_{i} = 0\n\\]\n\\[\nH_A: p_{d} - p_{i} > 0\n\\]\nAt first glance the data seem to back up his claim…\n\n\nExercise 6\nCompute the probability of survival and death for diabetic individuals with and without insulin dependence.\n\n#  code here\n\n\n\nExercise 7\nIs Flex’s claim significant at the \\(\\alpha = 0.05\\) level? Perform a hypothesis test and report your results.\n\n# code here\n\n\n\nExercise 8\nIs the aggregate data misleading? Use the code chunk below to investigate further.\n\n# code here"
  },
  {
    "objectID": "ae/ae-23.html#project-logistics",
    "href": "ae/ae-23.html#project-logistics",
    "title": "Project tips",
    "section": "Project logistics",
    "text": "Project logistics\n\nPresentations are a required component of the final project and it’s expected all group members will be there.\nThe final project is a website.\n\nYou can keep or delete the “Proposal” page.\nYou can add additional pages if you wish, e.g. an “Abstract” page\nThe Presentation, Report and About pages are required."
  },
  {
    "objectID": "ae/ae-23.html#code-chunk-settings",
    "href": "ae/ae-23.html#code-chunk-settings",
    "title": "Project tips",
    "section": "Code chunk settings",
    "text": "Code chunk settings\nSome options available for customizing output (see quarto documenation for more detail).\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code chunk (if false, just echos the code into the output)\n\n\necho\nInclude the source code in output\n\n\nwarning\nInclude warnings in the output\n\n\nmessage\nWhether to preserve messages emitted by message() (similar to the option warning)\n\n\ninclude\nCatch all for preventing any output (code or results) from being included (e.g. include: false suppresses all output from the code block)\n\n\n\nThese options can be applied globally (the whole document) or locally (a specific code chunk). Global settings are controlled in the YAML (see the top of the document) while local code chunk options can be applied with #| (see example below).\n\nExercise 1\nIn the code chunk below:\n\nset warning to false\nset echo to false\n\nand re-render.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.5      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.1.4      ✔ stringr 1.4.0 \n✔ readr   2.1.1      ✔ forcats 0.5.1 \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(knitr)\n\nIn addition to code chunks, figures have settings as well.\nWe can set captions and an alt attributes using #| fig-cap: and #| fig-alt: respectively. alt captions specify “alternate text” for an image. Alternative text appears if an image cannot be displayed and is also read by screen-readers.\nAdditional figure options include\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nfig-width\nfigure width in inches\n\n\nfig-height\nfigure height in inches\n\n\nfig.align\ne.g. fig.align: center centers figure alignment\n\n\nfig.asp\nchanges figure height based on aspect ratio with width\n\n\nout.width\nsets figure width relative to text (1000 = 100% text width), e.g. out.width: 1000\n\n\n\nIn all cases above, we can again set options locally or globally. Note: local options override global options.\n\n\nExercise 2\nAdd a figure caption to the figure below. Next, change the output width to be 50% of the text. Finally, align the figure with the center of the page.\n\nstarwars %>%\n  ggplot(aes(x = height)) +\n  geom_density() +\n  labs(x = \"Height (cm)\", y = \"Density\") +\n  theme_bw()\n\nWarning: Removed 6 rows containing non-finite values (stat_density).\n\n\n\n\n\n\n\nProject specific notes\nFor the project, you will set the option echo: FALSE and warning: FALSE to hide all code and warnings in your final report.\nSuggestion: make your figures consistently themed, e.g. use similar figure size/aspect ratio and color scheme throughout your report. Change the default gray background, see themes.\n\n\nExercise 3\nChange the global code chunk settings so the document is formatted as your final project will be. Render and take a look at the updated PDF."
  },
  {
    "objectID": "ae/ae-23.html#citations",
    "href": "ae/ae-23.html#citations",
    "title": "Project tips",
    "section": "Citations",
    "text": "Citations\nYour report will include citations, e.g. the data source, previous research, and other sources as needed. At a minimum, you should have a citation for the data source.\nAll of your bibliography entries will be stored in a .bib file. The entries of the bibliography are stored using BibTex, i.e., a format to store citations in LaTeX. Let’s take a look at references.bib.\nIn addition to the .bib file:\n\nInclude bibliography: references.bib in the YAML.\nAt the end of the report, include ## References. This will list all of the references at the end of the document.\n\n\nCitation examples\n\nIn (wickham2016package?), the authors focus present the grammar of graphics package ggplot2 for R.\nWithin the grammar of graphics, ggplot() is the first layer of any plot (wickham2016package?).\n\n\nExercise 4\n\nAdd a citation for tidytuesday to this document. Hint: check out the tidytuesday GitHub page."
  },
  {
    "objectID": "ae/ae-23.html#links",
    "href": "ae/ae-23.html#links",
    "title": "Project tips",
    "section": "Links",
    "text": "Links\nAdd URLs to your document using the following syntax:\nDISPLAYED TEXT"
  },
  {
    "objectID": "ae/ae-23.html#neat-kable-table",
    "href": "ae/ae-23.html#neat-kable-table",
    "title": "Project tips",
    "section": "Neat kable table",
    "text": "Neat kable table\n\nCalculate the mean, median, and standard deviation of mass. Display the results.\n\n\nExercise 5\n\n# code here\n\n\nLet’s neatly display the results using the kable function from the knitr package. We will\n\nDisplay results to 2 decimal places\nCustomize column names\nAdd a caption\n\n\n\n## add code"
  },
  {
    "objectID": "ae/ae-23.html#presentations-demo",
    "href": "ae/ae-23.html#presentations-demo",
    "title": "Project tips",
    "section": "Presentations (demo)",
    "text": "Presentations (demo)"
  },
  {
    "objectID": "ae/ae-24.html",
    "href": "ae/ae-24.html",
    "title": "Project tips",
    "section": "",
    "text": "this ae is not due for grade.\nexam 2 released today"
  },
  {
    "objectID": "ae/ae-24.html#getting-started",
    "href": "ae/ae-24.html#getting-started",
    "title": "Project tips",
    "section": "Getting started",
    "text": "Getting started\nClone your ae24-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-24.html#today",
    "href": "ae/ae-24.html#today",
    "title": "Project tips",
    "section": "Today",
    "text": "Today\nBy the end of today you will practice a few quarto/markdown tricks to polish your report and simplify your presentation. Specifically we will discuss:\n\ncode chunk settings\ncitations\nkable() tables\nquarto presentations"
  },
  {
    "objectID": "ae/ae-24.html#load-packages",
    "href": "ae/ae-24.html#load-packages",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "ae/ae-24.html#practice",
    "href": "ae/ae-24.html#practice",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Practice",
    "text": "Practice\n\nLoad data\nThe stent30 data set comes from the openintro package and is from a study conducted in 2011 on the effects of arterial stents as a therapy for stroke patients. See the original publication:\nChimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Med- ical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993- 1003. doi: 10.1056/NEJMoa1105335.\nor check ?stent30 for more information.\n\ndata(stent30)\n\n\nglimpse(stent30)\n\nRows: 451\nColumns: 2\n$ group   <fct> treatment, treatment, treatment, treatment, treatment, treatme…\n$ outcome <fct> stroke, stroke, stroke, stroke, stroke, stroke, stroke, stroke…\n\n\n\nExercise 1\nDo stents affect stroke outcome in patients?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context"
  },
  {
    "objectID": "ae/ae-24.html#confidence-intervals-and-hypothesis-tests",
    "href": "ae/ae-24.html#confidence-intervals-and-hypothesis-tests",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Confidence intervals and hypothesis tests",
    "text": "Confidence intervals and hypothesis tests\nHere we revisit the data from the first three seasons of NC Courage games (2017-2019).\n\ncourage = read_csv(\"https://sta101-fa22.netlify.app/static/labs/data/courage.csv\")\n\n\nglimpse(courage)\n\nRows: 78\nColumns: 10\n$ game_id     <chr> \"washington-spirit-vs-north-carolina-courage-2017-04-15\", …\n$ game_date   <chr> \"4/15/2017\", \"4/22/2017\", \"4/29/2017\", \"5/7/2017\", \"5/14/2…\n$ game_number <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ home_team   <chr> \"WAS\", \"NC\", \"NC\", \"BOS\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NC\", …\n$ away_team   <chr> \"NC\", \"POR\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NJ\", \"NC\", \"KC\", \"…\n$ opponent    <chr> \"WAS\", \"POR\", \"ORL\", \"BOS\", \"ORL\", \"CHI\", \"NJ\", \"CHI\", \"KC…\n$ home_pts    <dbl> 0, 1, 3, 0, 3, 1, 2, 3, 2, 3, 0, 0, 2, 1, 1, 0, 1, 2, 2, 2…\n$ away_pts    <dbl> 1, 0, 1, 1, 1, 3, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 2, 0, 3, 1…\n$ result      <chr> \"win\", \"win\", \"win\", \"win\", \"loss\", \"loss\", \"win\", \"loss\",…\n$ season      <dbl> 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017…\n\n\nDo National Women’s Soccer League (NWSL) teams have a home-field advantage? We’ll answer this question in a few separate ways.\nHypothesis testing framework: does NC Courage score a significantly different number of points (on average) away than at home?\n\nExercise 2\n\nCreate a new column location that tells you whether the courage are “home” or “away”\nCreate a new column pts that always reports the Courage points scored in a game.\nSave your result as a new data frame titled courage2.\n\n\n# code here\n\n\n\nExercise 3\nTo answer the question does NC Courage score a significantly different number of points (on average) away than at home?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context\n\n\n# code here\n\n\n\nExercise 4\n\nReport the mean difference between away and home games and report a 95% bootstrap confidence interval. Use set.seed(3) and reps=5000 Interpret your interval in context.\n\n\n# code here\n\n\n\nExercise 5\nIs there a better way we could investigate whether or not the Courage have a home-field advantage? Why?"
  },
  {
    "objectID": "ae/ae-24.html#notes",
    "href": "ae/ae-24.html#notes",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Notes",
    "text": "Notes\n\nType 1 and Type 2 Errors\n\n\n\nTruth\nReject the null\nFail to reject the null\n\n\n\n\n\\(H_0\\) is true\nType 1 error\n✔️\n\n\n\\(H_A\\) is true\n✔️\nType 2 error\n\n\n\nThe significance level, \\(\\alpha\\), is the probability of a type 1 error. In some contexts, a type 1 error may be referred to as a “false positive” and a type 2 error as a “false negative”.\nIntuitively, by considering extremes, one can see a trade-off exists between type 1 and type 2 error.\n\nIf \\(\\alpha = 0\\), then the p-value stands no chance of being smaller than \\(\\alpha\\) and we always fail to reject the null. This makes type 1 errors impossible.\n\nSimilarly, if \\(\\alpha = 1\\), then all p-values will be smaller than \\(\\alpha\\) and type 2 errors will become impossible, because we will always reject the null.\n\\(\\beta\\) is used to denote the probability of a type 2 error.\nThe power of a test is \\(1 - \\beta\\), which is the probability that your test rejects the null hypothesis when the null hypothesis is false."
  },
  {
    "objectID": "ae/ae-24.html#why-its-important-to-be-careful-with-interpretation",
    "href": "ae/ae-24.html#why-its-important-to-be-careful-with-interpretation",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Why it’s important to be careful with interpretation",
    "text": "Why it’s important to be careful with interpretation\n\n(And why hypothesis tests don’t tell the whole story)\nThe data for this example comes from Confounding and Simpson’s paradox1 by Julious and Mullee.\nThe data examines 901 individuals with diabetes and includes the following variables\n\ninsulin_dep: whether or not the patient has insulin dependent or non-insulin dependent diabetes\nage: whether or not the individual is less than 40 years old\nsurvival: whether or not the individual survived the length of the study\n\n\ndiabetes = read_csv(\"https://sta101.github.io/static/appex/data/diabetes.csv\")\n\nFlex Aisher thinks people with insulin dependent diabetes actually survive longer than those without insulin dependence. Flex wants to formally test his hypothesis.\nLet \\(p_{d}\\) be the probability of insulin dependent survival and \\(p_{i}\\) be the probability of insulin independent survival.\n\\[\nH_0: p_{d} - p_{i} = 0\n\\]\n\\[\nH_A: p_{d} - p_{i} > 0\n\\]\nAt first glance the data seem to back up his claim…\n\n\nExercise 6\nCompute the probability of survival and death for diabetic individuals with and without insulin dependence.\n\n#  code here\n\n\n\nExercise 7\nIs Flex’s claim significant at the \\(\\alpha = 0.05\\) level? Perform a hypothesis test and report your results.\n\n# code here\n\n\n\nExercise 8\nIs the aggregate data misleading? Use the code chunk below to investigate further.\n\n# code here"
  },
  {
    "objectID": "ae/ae-24.html#project-logistics",
    "href": "ae/ae-24.html#project-logistics",
    "title": "Project tips",
    "section": "Project logistics",
    "text": "Project logistics\n\nPresentations are a required component of the final project and it’s expected all group members will be there.\nThe final project is a website.\n\nYou can keep or delete the “Proposal” page.\nYou can add additional pages if you wish, e.g. an “Abstract” page\nThe Presentation, Report and About pages are required.\nYou will turn in your slides, report (whole website) by pushing to GitHub\nYou will additionally turn in your report via Gradescope."
  },
  {
    "objectID": "ae/ae-24.html#code-chunk-settings",
    "href": "ae/ae-24.html#code-chunk-settings",
    "title": "Project tips",
    "section": "Code chunk settings",
    "text": "Code chunk settings\nSome options available for customizing output (see quarto documenation for more detail).\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code chunk (if false, just echos the code into the output)\n\n\necho\nInclude the source code in output\n\n\nwarning\nInclude warnings in the output\n\n\nmessage\nWhether to preserve messages emitted by message() (similar to the option warning)\n\n\ninclude\nCatch all for preventing any output (code or results) from being included (e.g. include: false suppresses all output from the code block)\n\n\n\nThese options can be applied globally (the whole document) or locally (a specific code chunk). Global settings are controlled in the YAML (see the top of the document) while local code chunk options can be applied with #| (see example below).\n\nExercise 1\nIn the code chunk below:\n\nset warning to false\nset echo to false\n\nand re-render.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.5      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.1.4      ✔ stringr 1.4.0 \n✔ readr   2.1.1      ✔ forcats 0.5.1 \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(knitr)\n\nIn addition to code chunks, figures have settings as well.\nWe can set captions and an alt attributes using #| fig-cap: and #| fig-alt: respectively. alt captions specify “alternate text” for an image. Alternative text appears if an image cannot be displayed and is also read by screen-readers.\nAdditional figure options include\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nfig-width\nfigure width in inches\n\n\nfig-height\nfigure height in inches\n\n\nfig.align\ne.g. fig.align: center centers figure alignment\n\n\nfig.asp\nchanges figure height based on aspect ratio with width\n\n\nout.width\nsets figure width relative to text (1000 = 100% text width), e.g. out.width: 1000\n\n\n\nIn all cases above, we can again set options locally or globally. Note: local options override global options.\n\n\nExercise 2\nAdd a figure caption to the figure below. Next, change the output width to be 50% of the text. Finally, align the figure with the center of the page.\n\nstarwars %>%\n  ggplot(aes(x = height)) +\n  geom_density() +\n  labs(x = \"Height (cm)\", y = \"Density\") +\n  theme_bw()\n\nWarning: Removed 6 rows containing non-finite values (stat_density).\n\n\n\n\n\n\n\nProject specific notes\nFor the project, you will set the option echo: FALSE and warning: FALSE to hide all code and warnings in your final report.\nSuggestion: make your figures consistently themed, e.g. use similar figure size/aspect ratio and color scheme throughout your report. Change the default gray background, see themes.\n\n\nExercise 3\nChange the global code chunk settings so the document is formatted as your final project will be. Render and take a look at the updated PDF."
  },
  {
    "objectID": "ae/ae-24.html#citations",
    "href": "ae/ae-24.html#citations",
    "title": "Project tips",
    "section": "Citations",
    "text": "Citations\nYour report will include citations, e.g. the data source, previous research, and other sources as needed. At a minimum, you should have a citation for the data source.\nAll of your bibliography entries will be stored in a .bib file. The entries of the bibliography are stored using BibTex, i.e., a format to store citations in LaTeX. Let’s take a look at references.bib.\nIn addition to the .bib file:\n\nInclude bibliography: references.bib in the YAML.\nAt the end of the report, include ## References. This will list all of the references at the end of the document.\n\n\nCitation examples\n\nIn Wickham, Chang, and Wickham (2016), the authors focus present the grammar of graphics package ggplot2 for R.\nWithin the grammar of graphics, ggplot() is the first layer of any plot (Wickham, Chang, and Wickham 2016).\n\n\nExercise 4\n\nAdd a citation for tidytuesday to this document. Hint: check out the tidytuesday GitHub page."
  },
  {
    "objectID": "ae/ae-24.html#links",
    "href": "ae/ae-24.html#links",
    "title": "Project tips",
    "section": "Links",
    "text": "Links\nAdd URLs to your document using the following syntax:\nDISPLAYED TEXT"
  },
  {
    "objectID": "ae/ae-24.html#neat-kable-table",
    "href": "ae/ae-24.html#neat-kable-table",
    "title": "Project tips",
    "section": "Neat kable table",
    "text": "Neat kable table\n\nCalculate the mean, median, and standard deviation of mass. Display the results.\n\n\nExercise 5\n\n# code here\n\n\nLet’s neatly display the results using the kable function from the knitr package. We will\n\nDisplay results to 2 decimal places\nCustomize column names\nAdd a caption\n\n\n\n## add code"
  },
  {
    "objectID": "ae/ae-24.html#presentations-demo",
    "href": "ae/ae-24.html#presentations-demo",
    "title": "Project tips",
    "section": "Presentations (demo)",
    "text": "Presentations (demo)"
  },
  {
    "objectID": "exams/exam2.html",
    "href": "exams/exam2.html",
    "title": "Exam 2",
    "section": "",
    "text": "Important\n\n\n\nFind your exam 2 instructions in the README of your exam2-username repository.\nThe exam is due Tuesday April 18th at 5:00pm!"
  },
  {
    "objectID": "exams/exam2.html#rules",
    "href": "exams/exam2.html#rules",
    "title": "Exam 2",
    "section": "Rules",
    "text": "Rules\n\nThis is an individual assignment.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor or teaching team via a public channel in slack. For example, you may not communicate with other students or post/solicit help on the internet, email or via any other method of communication.\nYou may use R, as well as any notes, books, or existing internet resources to answer exam questions. If you use a resource outside of those we used for class, you should cite it by providing the URL in your rendered PDF.\nYou must cite any code you use as inspiration. A failure to cite is plagiarism. Cite any sources by providing a link to the original source in your exam write-up.\nIf you have questions email the instructor or message in slack. Questions should only be about understanding the data or the exam’s instructions. You may not ask questions on any topics from past assignments or material related to the exam.\nThe instructor will provide code debugging if needed, but this will result in a grade penalty. Note: ask questions early. Questions asked the day the exam is due may not be answered.\n\nWith the exception of major emergencies, late submissions will not be accepted. In the case of a major emergency, you should have your Dean send the instructor or course coordinator sta199@duke.edu an excuse. Start and submit the exam early in order to avoid any last-minute technical issues."
  },
  {
    "objectID": "exams/exam2.html#render-frequently",
    "href": "exams/exam2.html#render-frequently",
    "title": "Exam 2",
    "section": "Render frequently",
    "text": "Render frequently\nRender, commit and push your PDF to GitHub frequently. At least after every exercise.\n\nIf a PDF is not submitted on Gradescope (-10 points). If a PDF is not uploaded to Gradescope by the submission deadline, the PDF at your latest commit prior to the deadline will be used as your submission.\nIf there is no PDF in your repo, i.e., you’ve never rendered your .qmd file, your work will not be graded and you will receive a 0 on the exam."
  },
  {
    "objectID": "exams/exam2.html#academic-integrity",
    "href": "exams/exam2.html#academic-integrity",
    "title": "Exam 2",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Proposal due Friday, March 10th\nDraft due Friday April 7th\nPeer review in lab Monday April 10th\nPresentation + slides and final GitHub repo due Monday April 24th to GitHub\nProject report due Wednesday April 26th to Gradescope\nIn addition to the above, a component of the grade will be comprised of evaluating group members via survey by exam period.\n\n\nFind a data set, develop a question you can answer with the data, and do it."
  },
  {
    "objectID": "project-description.html#where-to-find-data",
    "href": "project-description.html#where-to-find-data",
    "title": "Project description",
    "section": "Where to find data?",
    "text": "Where to find data?\n\nAwesome public datasets\nBikeshare data portal\nCDC\nData.gov\nData is Plural\nDurham Open Data Portal\nEdinburgh Open Data\nElection Studies\nEuropean Statistics\nCORGIS: The Collection of Really Great, Interesting, Situated Datasets\nGeneral Social Survey\nGoogle Dataset Search\nHarvard Dataverse\nInternational Monetary Fund\nIPUMS survey data from around the world\nLos Angeles Open Data\nNHS Scotland Open Data\nNYC OpenData\nOpen access to Scotland’s official statistics\nPew Research\nPRISM Data Archive Project\nStatistics Canada\nThe National Bureau of Economic Research\nTidyTuesday\nUCI Machine Learning Repository\nUK Government Data\nUNICEF Data\nUnited Nations Data\nUnited Nations Statistics Division\nUS Census Data\nUS Government Data\nWorld Bank Data\nYouth Risk Behavior Surveillance System (YRBSS)"
  },
  {
    "objectID": "project-description.html#presentation-30pts",
    "href": "project-description.html#presentation-30pts",
    "title": "Project description",
    "section": "Presentation 30pts",
    "text": "Presentation 30pts\nPresentations will take place in class during the last lab of the semester.\nThe presentation must be no longer than 5 minutes. This will be strictly enforced."
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo.\nThe repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#teamwork",
    "href": "project-description.html#teamwork",
    "title": "Project description",
    "section": "Teamwork",
    "text": "Teamwork\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member.\nFilling out the survey is a prerequisite for getting credit on the teamwork portion of the grade.\nThe teamwork survey together with GitHub commits will be used to measure individual contribution to the assignment. All group members are expected to participate equally. In the event of team concerns and low effort commits, individual grades may differ from the rest of the group.\nIf you have concerns with the teamwork and/or contribution from any team members, please reach out to me or the head TA as early as possible."
  },
  {
    "objectID": "ae/ae-25.html",
    "href": "ae/ae-25.html",
    "title": "Ethics in Statistics and Data Science",
    "section": "",
    "text": "this ae is not due for grade.\nProject presentations and repo due Monday lab (by pushing to GitHub)\nExtended deadline: project reports now due Wednesday\nDon’t forget about the statistics experience"
  },
  {
    "objectID": "ae/ae-25.html#getting-started",
    "href": "ae/ae-25.html#getting-started",
    "title": "Ethics in Statistics and Data Science",
    "section": "Getting started",
    "text": "Getting started\nClone your ae25-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-25.html#today",
    "href": "ae/ae-25.html#today",
    "title": "Ethics in Statistics and Data Science",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ncritically examine graphics, models and results\ndiscuss data privacy and redundancy\nanalyze a real data example of Simpson’s paradox"
  },
  {
    "objectID": "ae/ae-25.html#load-packages",
    "href": "ae/ae-25.html#load-packages",
    "title": "Ethics in Statistics and Data Science",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nGuidelines for Discussion\n\nRespect others. Be charitable towards each other. Speak calmly."
  },
  {
    "objectID": "ae/ae-25.html#practice",
    "href": "ae/ae-25.html#practice",
    "title": "Ethics",
    "section": "Practice",
    "text": "Practice\n\nLoad data\nThe stent30 data set comes from the openintro package and is from a study conducted in 2011 on the effects of arterial stents as a therapy for stroke patients. See the original publication:\nChimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Med- ical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993- 1003. doi: 10.1056/NEJMoa1105335.\nor check ?stent30 for more information.\n\ndata(stent30)\n\n\nglimpse(stent30)\n\nRows: 451\nColumns: 2\n$ group   <fct> treatment, treatment, treatment, treatment, treatment, treatme…\n$ outcome <fct> stroke, stroke, stroke, stroke, stroke, stroke, stroke, stroke…\n\n\n\nExercise 1\nDo stents affect stroke outcome in patients?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context"
  },
  {
    "objectID": "ae/ae-25.html#confidence-intervals-and-hypothesis-tests",
    "href": "ae/ae-25.html#confidence-intervals-and-hypothesis-tests",
    "title": "Ethics",
    "section": "Confidence intervals and hypothesis tests",
    "text": "Confidence intervals and hypothesis tests\nHere we revisit the data from the first three seasons of NC Courage games (2017-2019).\n\ncourage = read_csv(\"https://sta101-fa22.netlify.app/static/labs/data/courage.csv\")\n\n\nglimpse(courage)\n\nRows: 78\nColumns: 10\n$ game_id     <chr> \"washington-spirit-vs-north-carolina-courage-2017-04-15\", …\n$ game_date   <chr> \"4/15/2017\", \"4/22/2017\", \"4/29/2017\", \"5/7/2017\", \"5/14/2…\n$ game_number <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ home_team   <chr> \"WAS\", \"NC\", \"NC\", \"BOS\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NC\", …\n$ away_team   <chr> \"NC\", \"POR\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NJ\", \"NC\", \"KC\", \"…\n$ opponent    <chr> \"WAS\", \"POR\", \"ORL\", \"BOS\", \"ORL\", \"CHI\", \"NJ\", \"CHI\", \"KC…\n$ home_pts    <dbl> 0, 1, 3, 0, 3, 1, 2, 3, 2, 3, 0, 0, 2, 1, 1, 0, 1, 2, 2, 2…\n$ away_pts    <dbl> 1, 0, 1, 1, 1, 3, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 2, 0, 3, 1…\n$ result      <chr> \"win\", \"win\", \"win\", \"win\", \"loss\", \"loss\", \"win\", \"loss\",…\n$ season      <dbl> 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017…\n\n\nDo National Women’s Soccer League (NWSL) teams have a home-field advantage? We’ll answer this question in a few separate ways.\nHypothesis testing framework: does NC Courage score a significantly different number of points (on average) away than at home?\n\nExercise 2\n\nCreate a new column location that tells you whether the courage are “home” or “away”\nCreate a new column pts that always reports the Courage points scored in a game.\nSave your result as a new data frame titled courage2.\n\n\n# code here\n\n\n\nExercise 3\nTo answer the question does NC Courage score a significantly different number of points (on average) away than at home?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context\n\n\n# code here\n\n\n\nExercise 4\n\nReport the mean difference between away and home games and report a 95% bootstrap confidence interval. Use set.seed(3) and reps=5000 Interpret your interval in context.\n\n\n# code here\n\n\n\nExercise 5\nIs there a better way we could investigate whether or not the Courage have a home-field advantage? Why?"
  },
  {
    "objectID": "ae/ae-25.html#notes",
    "href": "ae/ae-25.html#notes",
    "title": "Ethics",
    "section": "Notes",
    "text": "Notes\n\nType 1 and Type 2 Errors\n\n\n\nTruth\nReject the null\nFail to reject the null\n\n\n\n\n\\(H_0\\) is true\nType 1 error\n✔️\n\n\n\\(H_A\\) is true\n✔️\nType 2 error\n\n\n\nThe significance level, \\(\\alpha\\), is the probability of a type 1 error. In some contexts, a type 1 error may be referred to as a “false positive” and a type 2 error as a “false negative”.\nIntuitively, by considering extremes, one can see a trade-off exists between type 1 and type 2 error.\n\nIf \\(\\alpha = 0\\), then the p-value stands no chance of being smaller than \\(\\alpha\\) and we always fail to reject the null. This makes type 1 errors impossible.\n\nSimilarly, if \\(\\alpha = 1\\), then all p-values will be smaller than \\(\\alpha\\) and type 2 errors will become impossible, because we will always reject the null.\n\\(\\beta\\) is used to denote the probability of a type 2 error.\nThe power of a test is \\(1 - \\beta\\), which is the probability that your test rejects the null hypothesis when the null hypothesis is false."
  },
  {
    "objectID": "ae/ae-25.html#why-its-important-to-be-careful-with-interpretation",
    "href": "ae/ae-25.html#why-its-important-to-be-careful-with-interpretation",
    "title": "Ethics",
    "section": "Why it’s important to be careful with interpretation",
    "text": "Why it’s important to be careful with interpretation\n\n(And why hypothesis tests don’t tell the whole story)\nThe data for this example comes from Confounding and Simpson’s paradox1 by Julious and Mullee.\nThe data examines 901 individuals with diabetes and includes the following variables\n\ninsulin_dep: whether or not the patient has insulin dependent or non-insulin dependent diabetes\nage: whether or not the individual is less than 40 years old\nsurvival: whether or not the individual survived the length of the study\n\n\ndiabetes = read_csv(\"https://sta101.github.io/static/appex/data/diabetes.csv\")\n\nFlex Aisher thinks people with insulin dependent diabetes actually survive longer than those without insulin dependence. Flex wants to formally test his hypothesis.\nLet \\(p_{d}\\) be the probability of insulin dependent survival and \\(p_{i}\\) be the probability of insulin independent survival.\n\\[\nH_0: p_{d} - p_{i} = 0\n\\]\n\\[\nH_A: p_{d} - p_{i} > 0\n\\]\nAt first glance the data seem to back up his claim…\n\n\nExercise 6\nCompute the probability of survival and death for diabetic individuals with and without insulin dependence.\n\n#  code here\n\n\n\nExercise 7\nIs Flex’s claim significant at the \\(\\alpha = 0.05\\) level? Perform a hypothesis test and report your results.\n\n# code here\n\n\n\nExercise 8\nIs the aggregate data misleading? Use the code chunk below to investigate further.\n\n# code here"
  },
  {
    "objectID": "ae/ae-25.html#data-representation",
    "href": "ae/ae-25.html#data-representation",
    "title": "Ethics in Statistics and Data Science",
    "section": "Data Representation",
    "text": "Data Representation\n\nMisleading Data Visualizations1\nBrexit\n\n\n\nBrexit\n\n\n\nWhat is the graph trying to show?\nWhy is this graph misleading?\nHow can you improve this graph?\n\nSpurious Correlations2\n\n\n\nA Spurious Correlation\n\n\n\nWhat is the graph trying to show?\nWhy is this graph misleading?"
  },
  {
    "objectID": "ae/ae-25.html#statistical-modeling",
    "href": "ae/ae-25.html#statistical-modeling",
    "title": "Ethics in Statistics and Data Science",
    "section": "Statistical modeling",
    "text": "Statistical modeling\nRead, with a critical eye, page 2 and table 1 from Physician–patient racial concordance and disparities in birthing mortality for newborns and chat with your neighbor."
  },
  {
    "objectID": "ae/ae-25.html#data-privacy",
    "href": "ae/ae-25.html#data-privacy",
    "title": "Ethics in Statistics and Data Science",
    "section": "Data privacy",
    "text": "Data privacy\n\nWeb scraping3\nA data analyst received permission to post a data set that was scraped from a social media site. The full data set included name, screen name, email address, geographic location, IP (Internet protocol) address, demographic profiles, and preferences for relationships. The analyst removes name and email address from the data set in effort to deidentify it.\n\nWhy might it be problematic to post this data set publicly?\nHow can you store the full dataset in a safe and ethical way?\nYou want to make the data available so your analysis is transparent and reproducible. How can you modify the full data set to make the data available in an ethical way?\n\n\n\nRedundancy\n\nslides\n\n\n\nAdditional readings\n\nWhy pokemon go’s plan to 3d scan the world is dangerous\nHow companies learn your secrets\n\n\n\nDiscussion questions\n\n“Simpson’s paradox”, where conclusions drawn from analyzing subgroups differ from conclusions drawn when the groups are combined. Can you demonstrate Simpson’s Paradox with the data below? 4\nFor further reading see Bickel, Peter J., Eugene A. Hammel, and J. William O’Connell. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation.” Science 187.4175 (1975): 398-404.\n\n\nberk = read_csv(\"https://sta101.github.io/static/appex/data/BerkeleyAdmissionsData.csv\")\n\nRows: 7 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Dept\ndbl (4): MaleYes, MaleNo, FemaleYes, FemaleNo\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nberk\n\n# A tibble: 7 × 5\n  Dept  MaleYes MaleNo FemaleYes FemaleNo\n  <chr>   <dbl>  <dbl>     <dbl>    <dbl>\n1 A         512    313        89       19\n2 B         313    207        17        8\n3 C         120    205       202      391\n4 D         138    279       131      244\n5 E          53    138        94      299\n6 F          22    351        24      317\n7 All      1158   1493       557     1278"
  },
  {
    "objectID": "prepare/prep28.html",
    "href": "prepare/prep28.html",
    "title": "Prepare",
    "section": "",
    "text": "Watch the Caeser cipher\nWatch the Enigma"
  },
  {
    "objectID": "ae/ae-26.html",
    "href": "ae/ae-26.html",
    "title": "Cryptanalysis",
    "section": "",
    "text": "this ae is due for grade. Push your completed ae to GitHub within 48 hours to receive credit\nProject presentations and repo due Monday lab (by pushing to GitHub)\nExtended deadline: project reports now due Wednesday\nDon’t forget about the statistics experience\ncourse evaluations open. \\(>80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae-26.html#getting-started",
    "href": "ae/ae-26.html#getting-started",
    "title": "Cryptanalysis",
    "section": "Getting started",
    "text": "Getting started\nClone your ae26-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-26.html#today",
    "href": "ae/ae-26.html#today",
    "title": "Ethics in Statistics and Data Science",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ncritically examine graphics, models and results\ndiscuss data privacy and redundancy\nanalyze a real data example of Simpson’s paradox"
  },
  {
    "objectID": "ae/ae-26.html#load-packages",
    "href": "ae/ae-26.html#load-packages",
    "title": "Cryptanalysis",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(reshape2)\n\nOur data includes\n\nComplete text of “War and Peace” by Leo Tolstoy\nFreq. analysis data frame of “War and Peace”\nA secret message"
  },
  {
    "objectID": "ae/ae-26.html#data-representation",
    "href": "ae/ae-26.html#data-representation",
    "title": "Ethics in Statistics and Data Science",
    "section": "Data Representation",
    "text": "Data Representation\n\nMisleading Data Visualizations1\nBrexit\n\n\n\nBrexit\n\n\n\nWhat is the graph trying to show?\nWhy is this graph misleading?\nHow can you improve this graph?\n\nSpurious Correlations2\n\n\n\nA Spurious Correlation\n\n\n\nWhat is the graph trying to show?\nWhy is this graph misleading?"
  },
  {
    "objectID": "ae/ae-26.html#statistical-modeling",
    "href": "ae/ae-26.html#statistical-modeling",
    "title": "Ethics in Statistics and Data Science",
    "section": "Statistical modeling",
    "text": "Statistical modeling\nRead, with a critical eye, page 2 and table 1 from Physician–patient racial concordance and disparities in birthing mortality for newborns and chat with your neighbor."
  },
  {
    "objectID": "ae/ae-26.html#data-privacy",
    "href": "ae/ae-26.html#data-privacy",
    "title": "Ethics in Statistics and Data Science",
    "section": "Data privacy",
    "text": "Data privacy\n\nWeb scraping3\nA data analyst received permission to post a data set that was scraped from a social media site. The full data set included name, screen name, email address, geographic location, IP (Internet protocol) address, demographic profiles, and preferences for relationships. The analyst removes name and email address from the data set in effort to deidentify it.\n\nWhy might it be problematic to post this data set publicly?\nHow can you store the full dataset in a safe and ethical way?\nYou want to make the data available so your analysis is transparent and reproducible. How can you modify the full data set to make the data available in an ethical way?\n\n\n\nRedundancy\n\nslides\n\n\n\nAdditional readings\n\nWhy pokemon go’s plan to 3d scan the world is dangerous\nHow companies learn your secrets\n\n\n\nDiscussion questions\n\n“Simpson’s paradox”, where conclusions drawn from analyzing subgroups differ from conclusions drawn when the groups are combined. Can you demonstrate Simpson’s Paradox with the data below? 4\nFor further reading see Bickel, Peter J., Eugene A. Hammel, and J. William O’Connell. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation.” Science 187.4175 (1975): 398-404.\n\n\nberk = read_csv(\"https://sta101.github.io/static/appex/data/BerkeleyAdmissionsData.csv\")\n\nRows: 7 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Dept\ndbl (4): MaleYes, MaleNo, FemaleYes, FemaleNo\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nberk\n\n# A tibble: 7 × 5\n  Dept  MaleYes MaleNo FemaleYes FemaleNo\n  <chr>   <dbl>  <dbl>     <dbl>    <dbl>\n1 A         512    313        89       19\n2 B         313    207        17        8\n3 C         120    205       202      391\n4 D         138    279       131      244\n5 E          53    138        94      299\n6 F          22    351        24      317\n7 All      1158   1493       557     1278"
  },
  {
    "objectID": "ae/ae-26.html#background",
    "href": "ae/ae-26.html#background",
    "title": "Cryptanalysis",
    "section": "Background",
    "text": "Background\nThis application exercise builds on the R code found here and The Markov Chain Monte Carlo Revolution by Persi Diaconis.\n\nLet’s load the data\n\nwarandpeace = readLines(\"https://sta101.github.io/static/appex/data/warandpeace.txt\")\nfrequency = read.table(\"https://sta101.github.io/static/appex/data/frequencies.txt\")\ncolnames(frequency) = c(toupper(letters), \"\") # edit column names\nsecret_message = readLines(\"https://sta101.github.io/static/appex/data/secret-message.txt\")"
  },
  {
    "objectID": "ae/ae-26.html#exercise-1",
    "href": "ae/ae-26.html#exercise-1",
    "title": "Cryptanalysis",
    "section": "Exercise 1",
    "text": "Exercise 1\nTake a look at secret-message. Each letter is a stand in for exactly one other letter. This sort of cipher is known as a “substitution cipher”.\n‘A’ could be encoded as one of 26 characters (A, B, C, …). Once the encoding for ‘A’ is chosen, ‘B’ has 25 possibilities and so on so there are, in total \\(26 \\times 25 \\times 24 \\times \\ldots \\times 3 \\times 2 \\times 1\\) possibilities.\n\nn = 26\ninput = n + 1\nkeys = gamma(input)\nkeys\n\n[1] 4.032915e+26\n\n\nThat’s over \\(4 \\times 10^{26}\\) possible keys! If you could check 10M keys per second, it would take approximately \\(1 \\times 10^{12}\\) (trillion) years to check every possible key. Trying every possible key is known as a “brute force” approach.\n\nChat with your neighbor and develop a strategy better than the brute force approach. Detail your strategy below."
  },
  {
    "objectID": "ae/ae-26.html#exercise-2",
    "href": "ae/ae-26.html#exercise-2",
    "title": "Cryptanalysis",
    "section": "Exercise 2",
    "text": "Exercise 2\nHere we determine how often one character follows another using the text from the very long book, War and Peace. We include a whitespace character as a 27th character in our alphabet.\nTo reduce computational demand, we will load the object created by this analysis but leave the code below for reference.\nThe result of the below analysis is in the object frequency.\nThe letter row denotes the first character in a 2 character sequence while columns determine the second character in the character sequence.\n\nWhat should be the sum of a row? Check this for the first row of the data frame.\n\n\n# code here\n\n\n\n\nCreate a heatmap of character frequency, where the y-axis is the first letter and the x-axis is the second letter in a two letter chain.\n\nUncomment and complete the code below.\n\nHint: We’ll use melt from the reshape2 package. Click here for an example.\n\nmelt_freq = melt(as.matrix(frequency))\n\n# melt_freq %>%\n#   ggplot(aes(x = __, y = __, fill = __))"
  },
  {
    "objectID": "ae/ae-26.html#mcmc",
    "href": "ae/ae-26.html#mcmc",
    "title": "Cryptanalysis",
    "section": "MCMC",
    "text": "MCMC\nWe will use a famous statistical algorithm, Markov chain Monte Carlo (MCMC) to break the substitution cipher.\nMCMC is composed of three essential components:\n\nAbility. A way to propose any possible key.\nFeedback. A way to evaluate how good a given key is.\nCuriosity. A way to leave a good key for a worse key.\n\n\nThe analogy of the blind monkey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s a monkey on an island with many ponds. The monkey has been tasked with finding the largest body of water on the island. The only trouble is, he is blind. In order to find the largest body of water, he throws rocks randomly and listens for a splash. If he hears a splash, he knows there’s a body of water where he last threw. He continues to throw more rocks in that direction to find out how big the pool is. Occasionally he gets bored and wanders off to look for another pond. In this way, he uses the MCMC algorithm to find the largest body of water.\n\n\nMaking the connection.\n\nAbility (throw a rock). The Monkey can walk around and throw the rock anywhere on the island, ensuring that given enough time, he will cover every inch of the island.\nFeedback (test for waters). Every time the Monkey throws the rock, he receives feedback by listening for a splash. A large splash means a deep pond and encourages him to continue throwing in that direction to figure out the perimeter of the pond.\nCuriosity (boredom). If the monkey finds the second largest pond on the island, he might get stuck throwing rocks in it for a long time. By occasionally walking away from a large pond, he will reach the largest pond quicker."
  },
  {
    "objectID": "ae/ae-26.html#exercise-3",
    "href": "ae/ae-26.html#exercise-3",
    "title": "Cryptanalysis",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet’s write out together what MCMC looks like when decrpyting a secret message.\n\nAbility\nFeedback\nCuriosity"
  },
  {
    "objectID": "ae/ae-26.html#exercise-4",
    "href": "ae/ae-26.html#exercise-4",
    "title": "Cryptanalysis",
    "section": "Exercise 4",
    "text": "Exercise 4\nHere we load some functions that will help us decode the message.\n\n\n\nRun the code below to break the secret message. If the message is unintelligible after several iterations, you may try re-starting with a new seed. What is this equivalent to in the monkey analogy above?"
  },
  {
    "objectID": "ae/ae-26.html#exercise-5",
    "href": "ae/ae-26.html#exercise-5",
    "title": "Cryptanalysis",
    "section": "Exercise 5",
    "text": "Exercise 5\nTry your own message!\nCreate your own message in the code below and call mcmcAttack(coded) on your message to decode it!\n\n\n\n\nYou might think about what makes the message easy or difficult to attack, e.g. does length of the message affect its susceptibility to attack? What else might?"
  },
  {
    "objectID": "ae/ae-27.html",
    "href": "ae/ae-27.html",
    "title": "Forensic genetic analysis",
    "section": "",
    "text": "this ae is not due for grade.\nProject reports due today to Gradescope\nTeam evaluation survey\nStatistics experience due today\ncourse evaluations open. \\(>80\\%\\) response \\(\\rightarrow\\) +1pt final project\nmy final office hours today\npublic final project portfolio"
  },
  {
    "objectID": "ae/ae-27.html#getting-started",
    "href": "ae/ae-27.html#getting-started",
    "title": "Forensic genetic analysis",
    "section": "Getting started",
    "text": "Getting started\nClone your ae27-username repo from the GitHub organization."
  },
  {
    "objectID": "ae/ae-27.html#load-packages",
    "href": "ae/ae-27.html#load-packages",
    "title": "Forensic genetic analysis",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "ae/ae-27.html#background",
    "href": "ae/ae-27.html#background",
    "title": "Forensic genetic analysis",
    "section": "Background",
    "text": "Background\nDNA evidence is sometimes used in court. In this AE, you will learn about the case of Dr. Schmidt from Lafayette, Louisiana, who was accused of infecting his former lover with HIV through a contaminated blood sample of one of his patients. Read more about this court case here and here."
  },
  {
    "objectID": "ae/ae-27.html#today",
    "href": "ae/ae-27.html#today",
    "title": "Forensic genetic analysis",
    "section": "Today",
    "text": "Today\nBy the end of today you will\n\nbe able to critical think about the use of DNA evidence and statistics in court\nanalyze non-numerical data rigorously"
  },
  {
    "objectID": "ae/ae-27.html#explore-the-data",
    "href": "ae/ae-27.html#explore-the-data",
    "title": "Forensic genetic analysis",
    "section": "Explore the data",
    "text": "Explore the data\n\ndf_HIV = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/HIV.csv\")\n\ndf_HIV contains several observations of HIV genomes. Within this data set are two samples with special ids: sample1 and sample2.\nFor the purpose of this exercise, you might imagine sample1 is associated with the HIV sampled from the plaintiff while sample2 belongs to that of the defendant’s patient.\nFundamentally, we are interested in whether or not sample1 and sample2 are closely related.\n\nExercise 1\n\nhow many observations are present in the data set?\nwhat are the observational units?\nhow many bases does the first DNA sequence contain? Hint: use the R function nchar."
  },
  {
    "objectID": "ae/ae-27.html#extracting-a-sub-sequence-of-dna",
    "href": "ae/ae-27.html#extracting-a-sub-sequence-of-dna",
    "title": "Forensic genetic analysis",
    "section": "Extracting a sub-sequence of DNA",
    "text": "Extracting a sub-sequence of DNA\nFor computational speed, we will have to work with shorter sub-sequences of DNA.\nThe function str_sub from the package stringr in the tidyverse can be used to extract a sub-string from a character vector.\n\nExercise 2\n\nhow many arguments does the function str_sub take? What does each argument do?\nuse str_sub to extract (i) the first four letters of the words statistics, (ii) the sub-string between the third and the seventh letters, and (iii) the last four letters\n\n\nterm = \"statistics\"\n\nLet’s use str_sub to extract the first 500 bases from the DNA sequences using the template below.\n\nHIV = df_HIV %>% \n  mutate(dna_short = ___) %>%\n  select(-dna)\n\n\nuse nchar to verify that each sub-sequence dna_short contains 500 bases. Hint: nchar is vectorized, meaning if given a vector input, it will return a vector output."
  },
  {
    "objectID": "ae/ae-27.html#computing-the-pairwise-distances-between-the-dna-sequence",
    "href": "ae/ae-27.html#computing-the-pairwise-distances-between-the-dna-sequence",
    "title": "Forensic genetic analysis",
    "section": "Computing the pairwise distances between the DNA sequence",
    "text": "Computing the pairwise distances between the DNA sequence\nNow that the data have been prepared, we will establish how similar/different each DNA sequence is to the others. To accomplish this, given two DNA sequences we will count the number of bases for which they differ. The rationale for this step is the following. If two DNA differ on many bases, it means that they have evolved separately for a while and had the time to undergo numerous mutations. On the other hand, if they only differ on a few bases, it means that the two sequences have only recently began to evolve separately.\nThe following code creates a data frame where each row corresponds to a pair of DNA sequences.\n\nd_pairs <- combn(HIV$person_id, 2) %>%\n  t() %>% # go from wide matrix (2 rows) to long matrix (2 columns)\n  as_tibble() %>%\n  rename(id1 = V1, id2 = V2) %>%\n  left_join(HIV, by = c(\"id1\" = \"person_id\")) %>% # add dna for person 1\n  rename(dna1 = dna_short) %>%\n  left_join(HIV, by = c(\"id2\" = \"person_id\")) %>% # add dna for person 1\n  rename(dna2 = dna_short)\n\nCheck the dimensions of d_pairs\n\nHamming distance\nTo measure the distance between two sequences, we first consider the Hamming distance\n\\[\nd(i,j) = \\sum_{k=1}^n 1\\{\\text{dna}_{ik} \\neq \\text{dna}_{jk}\\}\n\\]\nwhich counts the number of elements that are different between two sequences. Here \\(d(i,j)\\) denotes the Hamming distance between sequences \\(i\\) and \\(j\\), and \\(1\\{\\text{dna}_{ik} \\neq \\text{dna}_{jk}\\}\\) is equal to \\(1\\) is the \\(k\\)-th element of sequence \\(i\\) is different from that of sequence \\(j\\).\nThe following code computes the Hamming distance between each pair of sequences. We first construct function compute_hamming which computes the Hamming distance between two DNA sequences. We then apply this function to each row of the d_pairs data frame.\n\ncompute_hamming <- function(dna1, dna2) {\n  \n  dna1_split <- str_split(dna1, pattern = \"\", simplify = TRUE)\n  dna2_split <- str_split(dna2, pattern = \"\", simplify = TRUE)\n  \n  hamming_distance <- sum(dna1_split != dna2_split)\n  return(hamming_distance)\n}\n\nd_hamming <- d_pairs %>%\n  mutate(\n    distance_ham = list(dna1, dna2) %>% pmap_dbl(compute_hamming)\n  )\n\n\nExercise 3\n\nMake a histogram of the Hamming distances and describe the distribution.\n\n\n\n\n\nFind the 10 pairs of DNA sequences that are the closest to the plaintiff’s sequence in terms of Hamming distance.\n\n\n\n\n\nWhich sequence is most closely related to the plaintiff’s sequence?\nHow many differences are there between the DNA sequence of the plaintiff and that of the defendant’s patient?\nCan you identify a shortcoming of the Hamming distance? Does a large Hamming distance necessarily imply that the two DNA sequences are very different?\n\n\n\n\nAn alternative measure of distance for DNA sequences\nLet us now consider an alternative measure of distance for DNA sequences.\n\nd_biology <- d_pairs %>%\n  mutate(\n    distance_bio = list(dna1, dna2) %>% pmap_dbl(adist)\n    )\n\n\nExercise 4\n\nAgain, make a histogram of the new distance and find the 10 DNA sequences closest to the plaintiff’s.\n\n\n\n\n\nWhich of the two measures do you find more adequate for these data?\n\n\n\nExercise 5\n\nImagine that you are a juror in this court case, would you find this piece of evidence convincing? Would you like to have more information? If so, what additional information would you need? Now, imagine that you are a judge; do you think that this piece of evidence should be admitted to court? Why?"
  }
]
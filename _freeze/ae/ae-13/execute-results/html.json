{
  "hash": "2d5eee35fd5c39e0b6e249bb837461ec",
  "result": {
    "markdown": "---\ntitle: \"Multiple regression I\"\nsubtitle: \"STA 199\"\neditor: source\nformat: html\nexecute:\n  error: true\n---\n\n\n## Bulletin\n\n-   this `ae` is **due for grade**. Push your completed ae to GitHub within 48 hours to receive credit\n-   homework 02 due today\n\n## Getting started\n\nClone *your* `ae13-username` repo from the [GitHub organization](https://github.com/sta199-sp23-1/).\n\n\n## Recap (warmup)\n\nFrom last time...\n\n- What is $\\hat{y}$? How is it different than $y$?\n\n- What is $\\hat{\\beta}$? How is it different than $\\beta$?\n\n- What is a residual? How is it different than error?\n\n\n## Today\n\nBy the end of today you will...\n\n- compute $R^2$ and use it to select between models\n- understand the geometric picture of multiple linear regression\n- be able to build, fit and interpret linear models with $>1$ predictor\n\n## Load packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\n```\n:::\n\n\nToday's data is a collection of tech stock prices from January 1st 2020 to December 31st 2021. I pulled this data off Yahoo finance using their API via the tidyquant package July 2022.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstocks = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/stocks2.csv\")\n```\n:::\n\n\n## Notes\n\n## $R^2$ and checking model fit\n\n### Conceptual introduction\n\n$R^2$, aka \"the coefficient of determination\" or \"correlation squared\" is a way to see how well a given model fits the data. Formally,\n\n\n$$\nR^2 = 1 - \\frac{\\sum_i r_i^2}{\\sum_i (y_i - \\bar{y})^2}\n$$\n\n\nwhere $\\bar{y}$ is the **mean** of all *y* values.\n\nIn words,\n\n\n$$\nR^2 = 1 - \\frac{\\text{sum of squared residuals}}{\\text{sum of outcome squared distance from the mean}}\n$$\n\n\nLet's focus on the word version to build intuition.\n\n-   The sum of squared residuals is a measure of how wrong our model is (how much our model **doesn't** explain)\n\n-   The denominator is proportional to the average square distance from the mean, i.e. the variance, i.e. the amount of variability in the data.\n\n-   Together, the fraction represents the proportion of variability that is not explained by the model.\n\nIf the sum of squared residuals is 0, then the model explains all variability and $R^2 = 1 - 0 = 1$.\n\nSimilarly if the sum of squared residuals is the same as all the variability in the data, then model does not explain any variability and $R^2 = 1 - 1 = 0$.\n\nFinal take-away: $R^2$ is a measure of the proportion of variability the model explains. An $R^2$ of 0 is a poor fit and $R^2$ of 1 is a perfect fit.\n\n### How to find $R^2$\n\nTo find $R^2$ simply call the function `glance()` on your `modelFit`, e.g.\n\n```\nmodelFit = linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(outcome ~ predictor, data = data_set)\n  \nglance(modelFit)\n```\n\n## Two predictor main effects model and notation\n\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n$$\n\n\n- $y$: the **outcome** variable. Also called the \"response\" or \"dependent variable\". In prediction problems, this is what we are interested in predicting.\n\n- $x_i$: the $i^{th}$ **predictor**. Also commonly referred to as \"regressor\", \"independent variable\", \"covariate\", \"feature\", \"the data\".\n\n- $\\beta_i$: \"constants\" or **coefficients** i.e. fixed numbers. These are **population parameters**. $\\beta_0$ has another special name, \"the intercept\".\n\n- $\\epsilon$: the **error**. This quantity represents observational error, i.e. the difference between our observation and the true population-level expected value: $\\beta_0 + \\beta_1 x$.\n\nEffectively this model says our data $y$ is linearly related to the $x_1$ and $x_2$ but is not perfectly observed due to some error.\n\n### A simple example\n\nLet's examine the first quarter of 2020 high prices of Microsoft, IBM and Apple stocks to illustrate some ideas.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ae-13_files/figure-html/simple-example-1.png){width=672}\n:::\n:::\n\n\n- If we have three measurements (variables) then each observation is a point in three-dimensional space. In this example, we can choose one of our measurements to be the outcome variable (e.g. Apple stock price) and use our other two measurements (MSFT and IBM price) as predictors.\n\n- In general, the total number of measurements, i.e. variables (columns) in our linear model represents the spatial dimension of our model.\n \n- Our fitted linear model no longer looks like a line, but instead looks like a **plane**.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ae-13_files/figure-html/fit-plane-example-1.png){width=672}\n:::\n:::\n\n\n- This plane shows our prediction of AAPL price ($y$) given both MSFT price ($x_1$) and IBM price ($x_2$)\n\n- Demo: building intuition for higher dimensional linear models\n\n#### Exercise 1\n\nIn $n$-dimensional space, a linear equation creates a $\\text{insert number here}$-dimensional object.\n\n\n## Fitting a multiple regression model in R\n\nFind the **equation of the plane** above with this one simple trick!\n\n```\nmyModelFit = linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(outcome ~ predictor1 + predictor2 + predictor3 + ..., data = data-set-here)\n```\n\nwe can simply 'add' in new predictors! This code template will fit the model according to the ordinary least squares (OLS) objective function, i.e. we are finding the equation of the hyperplane that minimizes the sum of squared residuals.\n\nYou can subsequently print the coefficients ($\\beta$s) to the screen by simply typing the model name, e.g. `myModelFit` or calling the `tidy()` function on your fitted model, e.g. `tidy(myModelFit)`.\n\n#### Exercise 2\n\nIn the code chunk below, fit the multiple regression model described above where \n\n$y$: AAPL high price,\n$x_1$: MSFT high price,\n$x_2$: IBM high price.\n\nThen write the equation of your **fitted model** below.\n\n- Note: you should change the name of \"myModelFit\" to be something more meaningful, e.g. `apple_high_fit`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here \n```\n:::\n\n\nThe equation of the plane above:\n\n\n$$\n\\text{your equation here}\n$$\n\n\n#### Exercise 3\n\nInterpret the coefficients in your equation above.\n\n[your interpretation here]\n\n\n\n## A better model\n\n\n### Log return\n\nApplying a model to values outside of the original data is called **extrapolation**. Extrapolation can be very unreliable. \n\nThat being noted, it would be nice if our model was only able to predict realistic outcomes. If we consider extrapolating our forecast, we will see that our linear model can easily predict unrealistic values. For example, with a negative slope, we can imagine that a very high Microsoft price drives our Apple prediction down to a negative value. \n\nHowever, stock prices cannot be negative. A more useful modeling framework used by investors is to predict the \"log return\" of a stock. Over the course of day, the log return is defined:\n\n\n$$\n\\log(\\text{close price}) - \\log(\\text{open price}) = \\log \\left( \\frac{\\text{close price}}{\\text{open price}} \\right)\n$$\n\n\n#### Exercise 4 \n\nStarting with your `stocks` data frame, create new columns  `AAPL.LogReturn`, `MSFT.LogReturn`, `IBM.LogReturn` that shows the daily log return of each stock. Continue this for the remaining stocks in the data frame. Save your new data frame as `stock_returns`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n```\n:::\n\n\n\n#### Exercise 5\n\nFit the following model:\n\n\n$$\ny = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + \\epsilon\n$$\n\nwhere \n\n- $y$: AAPL daily log return\n- $x_1$: MSFT daily log return\n- $x_2$: IBM daily log return\n\nand report $R^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here \n```\n:::\n\n\n\n### Predicting the future\n\nSo far we've only used the present to predict the present. i.e. we've used January 1st IBM prices to predict January 1st AAPL prices. While the resulting models are quite good, they are not particularly useful.\n\nIt would be much more useful if we could predict the return of AAPL tomorrow so that we could make an informed decision about buying or selling it.\n\nTo begin such an endeavor, let's build a model that uses yesterday's log-return of IBM and MSFT to predict today's log return of AAPL. \n\n#### Exercise 6 \n\nWhat should our data frame look like?\n\n[ your answer here ]\n\nLet's make that data frame! Adapt the example below to create new columns for yesterday's IBM and MSFT returns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstock_returns2 = stock_returns %>%\n  mutate(AAPL.LogReturnYesterday = lag(AAPL.LogReturn, 1)) %>%\n  filter(!is.na(AAPL.LogReturnYesterday))\n\nstock_returns2\n```\n:::\n\n\n#### Exercise 7 \n\nFit the following model:\n\n\n$$\ny = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + \\epsilon\n$$\n\n\nwhere \n\n$y$: AAPL daily log return\n$x_1$: MSFT log return yesterday\n$x_2$: IBM log return yesterday\n\nand report $R^2$. What do you notice?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n```\n:::\n",
    "supporting": [
      "ae-13_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
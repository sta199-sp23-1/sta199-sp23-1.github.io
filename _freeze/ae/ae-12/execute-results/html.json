{
  "hash": "f1a4118955e0c535e35665838a4d672c",
  "result": {
    "markdown": "---\ntitle: \"Simple regression\"\nsubtitle: \"STA 199\"\neditor: source\nformat: html\nexecute:\n  error: true\n---\n\n\n## Bulletin\n\n-   this `ae` is **due for grade**. Push your completed ae to GitHub within 48 hours to receive credit\n-   homework 02 due Friday\n\n## Getting started\n\nClone *your* `ae12-username` repo from the [GitHub organization](https://github.com/sta199-sp23-1/).\n\n# Today\n\nBy the end of today you will\n\n- understand the grammar of linear modeling, including $y$, $x$, $\\beta$, $\\epsilon$, fitted estimates and residuals\n- add linear regression plots to your 2D graphs\n- be able to write a simple linear regression model mathematically and \n- fit the model to data in R in a `tidy` way\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n\n## Data\n\nToday's data is Apple and Microsoft stock prices from January 1st 2020 to December 31st 2021. I pulled this data off the Yahoo finance using their API via the `tidyquant` package July 2022.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstocks = read_csv(\"https://sta101.github.io/static/appex/data/stocks1.csv\")\n```\n:::\n\n\n## Notes\n\n### The simple regression model and notation\n\n\n$$\ny = \\beta_0 + \\beta_1 x + \\epsilon\n$$\n\n\n- $y$: the **outcome** variable. Also called the \"response\" or \"dependent variable\". In prediction problems, this is what we are interested in predicting.\n\n- $x$: the **predictor**. Also commonly referred to as \"regressor\", \"independent variable\", \"covariate\", \"feature\", \"the data\".\n\n- $\\beta_0$, $\\beta_1$ are called \"constants\" or **coefficients**. They are fixed numbers. These are **population parameters**. $\\beta_0$ has another special name, \"the intercept\".\n\n- $\\epsilon$: the **error**. This quantity represents observational error, i.e. the difference between our observation and the true population-level expected value: $\\beta_0 + \\beta_1 x$.\n\nEffectively this model says our data $y$ is linearly related to $x$ but is not perfectly observed due to some error.\n\n### A simple example\n\nLet's examine January 2020 open prices of Microsoft and Apple stocks to illustrate some ideas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstocks_subset = stocks %>%\n  slice(1:21)\n\nstocks_subset %>%\n  ggplot(aes(x = MSFT.Open, y = AAPL.Open)) +\n  geom_point() + \n  labs(x = \"MSFT Open\", y = \"AAPL Open\", title = \"Open prices of MSFT and AAPL January 2020\") +\n  theme_bw() \n```\n\n::: {.cell-output-display}\n![](ae-12_files/figure-html/simple-example-1.png){width=672}\n:::\n\n```{.r .cell-code}\n  # more code here\n```\n:::\n\n\n#### Exercise 1\n\nAdd `geom_abline()` to the above plot and try different slopes and intercepts until you find a trendline you are satisfied with. The equation below describes your **fitted model**. Re-write the equation below, filling in $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ with your estimates.\n\n\n$$\n\\hat{y} = \\hat{\\beta_0} +  \\hat{\\beta_1} x\n$$\n\n\n- $\\hat{y}$ is the **expected outcome**.\n- $\\hat{\\beta}$ is the **estimated** or **fitted** coefficient\n- there is no error term here because we do not predict error\n\nThe equation of my line above:\n\n\n$$\n\\text{[your equation here]}\n$$\n\n\nThe central idea is that if we measure every $x$ and every $y$ in existence, (\"the entire population\") there is some true \"best\" $\\beta_0$ and $\\beta_1$ that describe the relationship between $x$ and $y$. Since we only have a **sample** of the data, we estimate $\\beta_0$ and $\\beta_1$. We call our estimates $\\hat{\\beta_0}$, $\\hat{\\beta_1}$ \"beta hat\". We never have all the data, thus we never can really know what the true $\\beta$s are.\n\n## Ordinary least squares (OLS) regression\n\n### The residuals\n\nFor any linear equation we write down, there will be some difference between the predicted outcome of our linear model ($\\hat{y}$) and what we observe ($y$)... (But of course! Otherwise everything would fall on a perfect straight line!)\n\nThis difference between what we observe and what we predict $y - \\hat{y}$ is known as a residual $r$.\n\nMore concisely,\n\n\n$$\nr = y - \\hat{y}\n$$\n\n\nResiduals are dependent on the line we draw. Visually, here is a model of the data, $y = -5 + \\frac{1}{2}x$ and 1 of the residuals is outlined in red.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ae-12_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThere is, in fact, a residual associated with every single point in the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictAAPL = function(x) {\n  return(-5 + (0.5*x))\n}\n\nxPoints = stocks$MSFT.Open[1:21]\nyPoints = stocks$AAPL.Open[1:21]\nyHat = predictAAPL(xPoints)\n\nstocks_subset %>%\n  ggplot(aes(x = MSFT.Open, y = AAPL.Open)) +\n  geom_point() + \n  labs(x = \"MSFT Open\", y = \"AAPL Open\", title = \"Open prices of MSFT and AAPL January 2020\") +\n  theme_bw() +\n  geom_abline(slope = 0.5, intercept = -5) +\n  geom_segment(x = xPoints, xend = xPoints, y  = yPoints, yend = yHat, color = 'red')\n```\n\n::: {.cell-output-display}\n![](ae-12_files/figure-html/all-residuals-1.png){width=672}\n:::\n:::\n\n\nWe often wish to find a line that fits the data \"really well\", but what does this mean? Well, we want small residuals! So we pick an **objective function**. That is, a function we wish to minimize or maximize.\n\n### The objective function\n\n#### Exercise 2\n\nAt first, you might be tempted to minimize $\\sum_i r_i$, but this is problematic. Why? Can you come up with a better solution (other than the one listed below)?\n\n[answer here]\n\nIn practice, we minimize the **sum of squared residuals**: \n\n\n$$\n\\sum_i r_i^2\n$$\n\n\nNote, this is the same as \n\n\n$$\n\\sum_i (y_i - \\hat{y})^2\n$$\n\n\n#### Exercise 3\n\nCheck out an interactive visualization of \"least squares regression\" [here](https://seeing-theory.brown.edu/regression-analysis/index.html#section1). Click on `I` and drag the points around to get started. Describe what you see.\n\n[response here]\n\n#### Exercise 4\n\n- Check for understanding\n\nHow far off is your model (from exercise 1) from the actual observed data on January 11 2020? The observed value is  MSFT: \\$164.35 and AAPL: \\$78.4. Compute the single square residual using your **fitted model** from exercise 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n```\n:::\n\n\n## Plotting the OLS regression line\n\nPlotting the OLS regression line, that is, the line that minimizes the sum of square residuals is very easy with ggplot. Simply add \n\n```\ngeom_smooth(method = 'lm', se = F)\n```\n\nto your plot.\n\n`method = lm` says to draw a line according to a \"linear model\" and `se = F` turns off standard error bars. You can try without these options for comparison.\n\nOptionally, you can change the color of the line, e.g.\n\n```\ngeom_smooth(method = 'lm', se = F, color = 'red')\n```\n\n\n#### Exercise 5 \n\nCopy your code from exercise 1 below. Add `geom_smooth()` as described above with `color = 'steelblue'` to see how close your line is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n```\n:::\n\n\n## Finding $\\hat{\\beta}$\n\nTo **fit the model** in R, i.e. to \"find $\\hat{\\beta}$\", use the code below as a template:\n\n```\nmodelFit = linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y-variable-here ~ x-variable-here, data = data-frame-here)\n```\n\n- `linear_reg` tells `R` we will perform linear regression\n- `set_engine` tells `R` to use the standard linear modeling (lm) machinery\n- `fit` defines the outcome $y$, predictor $x$ and the data set\n\nRunning the code above, but replacing the arguments of the `fit` command appropriately will create a new object called \"modelFit\" (defined on the first line) that stores all information about your fitted model.\n\nTo access the information, you can run, e.g.\n\n```\ntidy(modelFit)\n```\n\nLet's try it out.\n\n#### Exercise 6\n\nFind the OLS fitted linear model $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x$ for January 2020, where $x$ is Microsoft's opening price and $y$ is Apple's opening price. Print your results to the screen\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n```\n:::\n\n\n\n#### Exercise 7\n\nRe-write the fitted equation replacing $\\beta_0$ and $\\beta_1$ with the OLS fitted values.\n\n\n$$\n\\text{[your equation here]}\n$$",
    "supporting": [
      "ae-12_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
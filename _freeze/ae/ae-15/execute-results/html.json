{
  "hash": "e2d390e5a409a324802774838157d167",
  "result": {
    "markdown": "---\ntitle: \"Model selection\"\nsubtitle: \"STA 199\"\neditor: source\nformat: html\nexecute:\n  error: true\n---\n\n\n## Bulletin\n\n-   this `ae` is **due for grade**. Push your completed ae to GitHub within 48 hours to receive credit\n-   homework 03 due next Wednesday\n-   final project instructions\n\n## Getting started\n\nClone *your* `ae15-username` repo from the [GitHub organization](https://github.com/sta199-sp23-1/).\n\n## Today\n\nBy the end of today you will...\n\n- select between linear models with different numbers of predictors\n\n## Load packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n\n## Notes\n\n### The problem with $R^2$\n\n$R^2$ tell us the proportion of variability in the data our model explains. If we add predictors to our model, we will *always* improve $R^2$ (regardless of whether the predictor is good or not). \n\nTo see this...\n\n- offline example\n\n- take away: a line can go through any two points, a plane can go through any three points, etc. In general an $n$ dimensional object can go through $n$ points.\n\nFor this reason, $R^2$ is not a good way to select between two models that have a different number of predictors. Instead, we prefer to use Akaike Information Criterion (AIC). \n\n### AIC \n\n\n$$\n\\text{AIC} = 2k - 2 \\log (\\text{likelihood})\n$$\n\n\nwhere $k$ is the number of estimated parameters ($\\beta$s) in the model. Notice this will be 1 + the number of predictors. and $\\hat{L}$ is \"likelihood\" of the data given the fitted model. \n\nThe likelihood is a measure of how well a given model fits the data. Specifically, higher likelihoods imply better fits. Since the AIC score has a negative in front of the log likelihood, lower scores are better fits. However, $k$ penalizes adding new predictors to the model.\n\nTake-away: lower AIC is better fit.\n\nYou can find AIC using `glance(fitted-model)`. (Assuming you named your fitted model `fitted-model`)\n\n## Building a model\n\nScenario: you have an outcome $y$ you want to predict. You have several variables you've measured that you *could* use as predictors in your linear model. Each predictor is expensive to collect future measurements of. You want your model to only include the most useful predictors. \n\n### Backward elimination \n\nBackward elimination starts with the full model (the model that includes all potential predictor variables). Variables are eliminated one-at-a-time from the model until we cannot improve the model any further.[^1]\n\nProcedure:\n\n1. Start with a model that has all predictors under study and compute the AIC.\n2. Next fit every possible model with 1 less predictor.\n3. Compare AIC scores to select the best model with 1 less predictor.\n4. Repeat steps 2 and 3 until you can no longer improve the model.\n\n\n### Forward selection\n\nForward selection is the reverse of the backward elimination technique. Instead, of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model any further.\n\n[^1]: see [Introduction to Modern Statistics](https://openintro-ims.netlify.app/model-mlr.html?q=selection#stepwise-selection) for further reference.\n\nProcedure:\n\n1. Start with a model that has no predictors.\n2. Next fit every possible model with 1 additional predictor and score each model.\n3. Compare AIC scores to select the best model with 1 additional predictor.\n4. Repeat steps 2 and 3 until you can no longer improve the model.\n\n\n## Example\n\n### Exercise \n\n- Will forward selection and backward elimination always yield the same model? Type your answer below before running any code.\n\n- Next, see if you are right using the data set below.\n\n**Solution below**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_df = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/test_df.csv\")\n```\n:::\n\n\nIn the following two examples, we will use stepwise selection to build a **main effects** model.\n\nPerform 1 step of forward selection. What variable will be **in** the final *forward selection* model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 93.08637\n```\n:::\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x2, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 131.8917\n```\n:::\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 152.9043\n```\n:::\n:::\n\n\nNext, perform 1 step of backward elimination. Which variable will **not** be in the final *backward elimination* model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x2 + x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 35.25949\n```\n:::\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1 + x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 94.2536\n```\n:::\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1 + x2, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 87.1686\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
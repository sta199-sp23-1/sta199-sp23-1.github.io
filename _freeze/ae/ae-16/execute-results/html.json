{
  "hash": "f24641b3f82382cb33def224ac4a3852",
  "result": {
    "markdown": "---\ntitle: \"Logistic regression\"\nsubtitle: \"STA 199\"\neditor: source\nformat: html\nexecute:\n  error: true\n---\n\n\n## Bulletin\n\n-   this `ae` is **due for grade**. Push your completed ae to GitHub within 48 hours to receive credit\n-   homework 03 due today\n-   final project proposal due Friday\n- statistics experience homework\n    - Friday is last day to register for [datafest](https://dukestatsci.github.io/datafest/). \n\n## Getting started\n\nClone *your* `ae16-username` repo from the [GitHub organization](https://github.com/sta199-sp23-1/).\n\n# Today\n\nBy the end of today you will...\n\n- understand logistic regression as a linear model of binary outcomes\n- be able to fit logistic regression in R\n\n\n## Load packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(boot) # contains inv.logit() function\n```\n:::\n\n\nTo illustrate logistic regression, we will build a spam filter from email data. Today's data consists of 4601 emails that are classified as `spam` or `non-spam`. The data was collected at Hewlett-Packard labs and contains 58 variables. The first 48 variables are specific keywords and each observation is the percentage of appearance **(frequency)** of that word in the message. Click [here](https://rdrr.io/cran/kernlab/man/spam.html) to read more. \n\n- `type` $= 1$ is spam\n- `type` $= 0$ is non-spam\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspam = read_csv(\"https://sta101.github.io/static/appex/data/spam.csv\")\nglimpse(spam)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 4,601\nColumns: 58\n$ make              <dbl> 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ address           <dbl> 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ all               <dbl> 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.46…\n$ num3d             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ our               <dbl> 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1.88, 0.61…\n$ over              <dbl> 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ remove            <dbl> 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0.00, 0.30…\n$ internet          <dbl> 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1.88, 0.00…\n$ order             <dbl> 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0.00, 0.92…\n$ mail              <dbl> 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0.00, 0.76…\n$ receive           <dbl> 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0.00, 0.76…\n$ will              <dbl> 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0.00, 0.92…\n$ people            <dbl> 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0.00, 0.00…\n$ report            <dbl> 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ addresses         <dbl> 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ free              <dbl> 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0.00, 0.00…\n$ business          <dbl> 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ email             <dbl> 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0.00, 0.15…\n$ you               <dbl> 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0.00, 1.23…\n$ credit            <dbl> 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0.00, 3.53…\n$ your              <dbl> 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0.00, 2.00…\n$ font              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num000            <dbl> 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ money             <dbl> 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ hp                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hpl               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ george            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num650            <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ lab               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ labs              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ telnet            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num857            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ data              <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ num415            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num85             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ technology        <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ num1999           <dbl> 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ parts             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pm                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ direct            <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ cs                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ meeting           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ original          <dbl> 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30…\n$ project           <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ re                <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ edu               <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ table             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ conference        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ charSemicolon     <dbl> 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charRoundbracket  <dbl> 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0.054, 0.2…\n$ charSquarebracket <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charExclamation   <dbl> 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0.164, 0.0…\n$ charDollar        <dbl> 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0.054, 0.0…\n$ charHash          <dbl> 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ capitalAve        <dbl> 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1.671, 2.4…\n$ capitalLong       <dbl> 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6, 11, 61,…\n$ capitalTotal      <dbl> 278, 1028, 2259, 191, 191, 54, 112, 49, 1257, 749, 2…\n$ type              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n:::\n:::\n\n\nThe basic logic of our model is that the frequency of certain words can help us determine whether or not an email is spam.\n\nFor example, these emails came from George's inbox. If the word \"george\" is not present in the message and the dollar symbol (`charDollar`) is, you might expect the email to be spam.\n\nUsing this data, we want to build a model that **predicts** whether a new email is spam or not. How do we build a model that can do this?\n\n#### Exercise 1 \n\nStart by examining 1 predictor.\n\n- Visualize a **linear model** where the outcome is `type` (spam or not) and `george` is the predictor.\n\n- Discuss your visualization with your neighbor. Is this a good model? Why or why not?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n```\n:::\n\n\n\n# Logistic regression\n\n*How do you build a model to fit a binary outcome?*\n\nLinear logistic regression (also simply called \"logistic regression\") takes in a number of predictors and outputs the probability of a \"success\" (an outcome of 1) in a binary outcome variable. The probability is related to the predictors via a **sigmoid link function**,\n\n$$\np(y_i = 1) = \\frac{1}{1+\\text{exp}({- \\sum \\beta_i x_i })},\n$$\n\nwhose output is in $(0,1)$ (a probability). In this modeling scheme, one typically finds $\\hat{\\beta}$ by maximizing the **likelihood function**, (another objective function, different than our previous \"least squares\" objective).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigmoid = function(x) 1 / (1 + exp(-x + 10))\nplot.function(sigmoid, from = 0, to = 20, n = 101, ylab=\"p(yi = 1)\", xlab=\"input\", main=\"Sigmoid link function\", lwd = 3)\nbox()\n```\n\n::: {.cell-output-display}\n![](ae-16_files/figure-html/sigmoid-function-1.png){width=672}\n:::\n:::\n\n\nTo proceed with building our email classifier, we will, as usual, use our data (outcome $y_i$ and predictor $x_i$ pairs), to estimate $\\beta$ (find $\\hat{\\beta}$) and obtain the model:\n\n$$\np(y_i = 1) = \\frac{1}{1+\\text{exp}({- \\sum  \\hat{\\beta}_i x_i})},\n$$\n\n\n## Example\n\nLet's build a model centered around just two predictor variables. \n\nThe first will be the word `you` and the second will be `capitalTotal` (the total number of capital letters in the message).\n\n#### Exercise 2 \n\nCreate a visualization with `you` on the x-axis and `capitalTotal` on the y-axis. Color data points by whether or not they are spam.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n```\n:::\n\n\nLet's fit the model!\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_1 = logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(as.factor(type) ~ you + capitalTotal, data = spam, family = \"binomial\")\n  \nfit_1 %>%\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.50     0.0554       -27.1 2.97e-162\n2 you           0.361    0.0198        18.3 1.84e- 74\n3 capitalTotal  0.00173  0.000104      16.6 5.66e- 62\n```\n:::\n:::\n\n#### Exercise 3 \n\n- What is different in the code above from previous linear models we fit? \n\n#### Exercise 4 \n\n- What is the probability the email is spam if the frequency of `you` is 5% in the email and there are 2500 capital letters. Use the model equation above.\n\n- What is the log-odds? (Recall from the prep that log-odds $= \\log \\frac{p}{1-p}$). Use the code below to check your work.\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata = data.frame(you = 5, capitalTotal = 2500)\n\n# code here\n\n# check work\ncheckLogOdds = predict(fit_1$fit, newdata)\ncheckLogOdds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n4.633134 \n```\n:::\n\n```{.r .cell-code}\ncheckP = inv.logit(checkLogOdds)\ncheckP\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1 \n0.9903694 \n```\n:::\n:::\n\n\n## Visualize logistic regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta = fit_1$fit$coefficients\nhyperplane = function(x){\n    decisionBoundary = 0.5\n    c = logit(decisionBoundary)\n    const = c - beta[1]\n    return((-beta[2]*x + const) / beta[3])\n}\n\nspam %>%\n  ggplot(aes(x = you, y = capitalTotal, color = as.factor(type))) + \n  geom_point(alpha = 0.3) +\n  geom_function(fun = hyperplane) +\n  scale_colour_manual(values = c(\"orange\", \"steelblue\")) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple drawing groups in `geom_function()`. Did you use the correct\n`group`, `colour`, or `fill` aesthetics?\n```\n:::\n\n::: {.cell-output-display}\n![](ae-16_files/figure-html/find-hyperplane-1.png){width=672}\n:::\n:::\n\n\n- Just because there's greater than 50% probability an email is spam doesn't mean we have to label it as such. We can adjust our **threshold** or **critical probability**, a.k.a. **decision boundary** to be more or less sensitive to spam emails.\n\nIn other words we get to select a number $p^*$ such that \n\nif $p > p^*$, then label the email as spam.\n\n#### Exercise 5 \n\n- What would you set your decision boundary to and why?\n\n- Change `decisionBoundary` in the code above to 0.01 and 0.999999. Do the results surprise you? Why or why not?\n\n- lower boundary means that we label more emails as spam, high boundary means fewer emails as spam. We can adjust the boundary depending on how much we value receiving important emails vs how much we dislike spam.\n\n- 0 means all emails are spam, 1 means no emails are spam. Note you cannot set decision boundary to 0 or 1 because of logit function (would evaluate to inf or negative inf)\n\n## Classify a new email\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail = readLines(\"https://sta101.github.io/static/appex/data/test-email.txt\")\nemail\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"You Have Been Selected To Win A Free Trip To Disney World! \"\n[2] \"\"                                                           \n[3] \"YOU HAVE 30 SECONDS TO CLICK HERE TO CLAIM YOUR REWARD!\"    \n[4] \"\"                                                           \n[5] \"WHAT ARE YOU WAITING FOR? ACT NOW!\"                         \n[6] \"\"                                                           \n[7] \"SINCERELY,\"                                                 \n[8] \"\"                                                           \n[9] \"WALT DISNEY\"                                                \n```\n:::\n\n```{.r .cell-code}\ntotalWord = sum(str_count(email, \" \"))\ntotalYou = sum(str_count(tolower(email), \"you\"))\ncapitalTotal = sum(str_count(email, \"[A-Z]\"))\n\nyouFreq = 100 * totalYou / totalWord\nnewemail = data.frame(you = youFreq, capitalTotal = capitalTotal)\n\nlogOdds = predict(fit_1$fit, newemail)\nlogOdds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n3.648776 \n```\n:::\n\n```{.r .cell-code}\ninv.logit(logOdds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1 \n0.9746371 \n```\n:::\n:::\n\n\n\n#### Exercise 6\n\n- Does the code above count the correct number of \"you\"? Why or why not?\n\n- Do you believe the predicted odds of the email being spam? Why or why not?\n\n- What is the **probability** the test email is spam?\n\n## Assessing predictive ability\n\nWe will divide the data into a training set and testing set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(6)\nsampleIndices = sample.int(n = nrow(spam), size = 2000, replace = F)\ntrain = spam[sampleIndices, ]\ntest  = spam[-sampleIndices, ] %>%\n  slice_sample(n = 2000)\n```\n:::\n\n\n#### Exercise 7\n\nNext, let's train your model on the training set.\nBuild a predictive model using any combination of predictors from `spam`. Save your fitted model as `myModel`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n\n#example (delete this):\nmyModel = logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(as.factor(type) ~ you + address, data = train, family = \"binomial\")\n```\n:::\n\n\nand test it on the testing set,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction = test %>%\n  mutate(myModelPrediction = predict(myModel, test)$.pred_class) \n\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,000 × 59\n    make address   all num3d   our  over remove internet order  mail receive\n   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl> <dbl> <dbl>   <dbl>\n 1  0       0     0.7      0  0     0.14   0        0     0.28  0       0   \n 2  0       0     0        0  0     0      0        0     0     0       0   \n 3  0       0     2.5      0  0     0      0        0     0     0       0   \n 4  0       2.08  0        0  3.12  0      1.04     0     0     0       0   \n 5  0       0     1.04     0  1.04  0      0        1.39  0.34  0       0   \n 6  0       0.35  0.7      0  0.35  0      0        0     0     0       0   \n 7  0.39    0     0        0  1.17  0      0        0     0     0.39    0   \n 8  0       0.25  0.75     0  1     0.25   0        0     0     0       0.25\n 9  0       0     0        0  0.87  0      0        0     0     0       1.31\n10  0       0     0        0  1.11  0      0        0.55  0     3.91    0   \n# … with 1,990 more rows, and 48 more variables: will <dbl>, people <dbl>,\n#   report <dbl>, addresses <dbl>, free <dbl>, business <dbl>, email <dbl>,\n#   you <dbl>, credit <dbl>, your <dbl>, font <dbl>, num000 <dbl>, money <dbl>,\n#   hp <dbl>, hpl <dbl>, george <dbl>, num650 <dbl>, lab <dbl>, labs <dbl>,\n#   telnet <dbl>, num857 <dbl>, data <dbl>, num415 <dbl>, num85 <dbl>,\n#   technology <dbl>, num1999 <dbl>, parts <dbl>, pm <dbl>, direct <dbl>,\n#   cs <dbl>, meeting <dbl>, original <dbl>, project <dbl>, re <dbl>, …\n```\n:::\n:::\n\n\n\n#### Exercise 8\nWhat is the proportion of false positives (i.e. classified as spam but was not)? False negatives?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# code here\n```\n:::\n",
    "supporting": [
      "ae-16_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}